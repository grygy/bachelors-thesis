{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from os import path\n",
    "import pathlib\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the images to [0, 1]\n",
    "def normalize(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        image, size=[256, 256, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_flowers_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    return image, label\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess_flowers(image, label):\n",
    "    image = tf.image.resize(image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label\n",
    "\n",
    "def load_flowers_dataset():  \n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(name=\"tf_flowers\", \n",
    "                                             with_info=True,\n",
    "                                             split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],  #70/15/15 split\n",
    "                                             as_supervised=True)\n",
    "\n",
    "    ds_train = ds_train.map(normalize)    \n",
    "    ds_train = ds_train.map(preprocess_flowers)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    ds_validation = ds_validation.map(preprocess_flowers)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    ds_test = ds_test.map(preprocess_flowers)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test\n",
    "\n",
    "def load_beans_datasets():\n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
    "        'beans',\n",
    "        split=['train', 'validation', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    \n",
    "    ds_train = ds_train.map(normalize)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization after training\n",
    "\n",
    "1. Dynamic range quantization\n",
    "2. Full integer quantization\n",
    "3. Float16 quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dynamic range quantization\n",
    "\n",
    "Only the weights are converted from float to 8 bit int. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Full integer quantization\n",
    "\n",
    "Weights and activation outputs are quantizated. Good for microcontrolers and TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_quantization(model_path, ds):\n",
    "    # check if model was already in optimization folder\n",
    "    if (len(model_path.split('/')[0].split('_')) == 3):\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '/')\n",
    "    else:\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '_optimized/')\n",
    "    model_name = model_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # save converted tflite model\n",
    "    tf_model_path = optimized_dir/(model_name + '.tflite')\n",
    "    size = tf_model_path.write_bytes(tflite_model)\n",
    "    print('Converted TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_model_path))\n",
    "    \n",
    "    # 1. optimize model using dynamic range quantization\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model_quant = converter.convert()\n",
    "    \n",
    "    tf_quant_model_path = optimized_dir/(model_name + '_dynamic_rage_quantization.tflite')\n",
    "    size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "    print('Dynamic range quantizatized TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n",
    "    \n",
    "    # 2. Full integer quantization\n",
    "    def representative_data_gen():\n",
    "        for input_value, _ in ds.batch(1).take(100):\n",
    "            # Model has only one input so each data point has one element.\n",
    "            yield [input_value]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "\n",
    "    tflite_model_quant = converter.convert()\n",
    "    tf_quant_model_path = optimized_dir/(model_name + '_full_integer_quantization.tflite')\n",
    "    size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "    print('Full integer quantizatized TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n",
    "    \n",
    "    # 2.1 Full integer quantization with input and output in integer too\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_data_gen\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "\n",
    "        tflite_model_quant = converter.convert()\n",
    "        tf_quant_model_path = optimized_dir/(model_name + '_full_integer_quantization_integer_io.tflite')\n",
    "        size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "        print('Full integer quantizatized with integer io TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n",
    "    except:\n",
    "        print('ERROR: Failed Full integer quantizatized with integer io TFLite model')\n",
    "        \n",
    "    # 3. float16 quantization\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    tflite_model_quant = converter.convert()\n",
    "    tf_quant_model_path = optimized_dir/(model_name + '_float16_quantization.tflite')\n",
    "    size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "    print('float16 quantizatized TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning and fine-tuning\n",
    "\n",
    "1. Prune model to different sparsity  ( tf uses magnitude-based pruning )\n",
    "    1. ConstantSparsity - sparsity is kept constant during training.\n",
    "    2. PolynomialDecay - the degree of sparsity is changed during training.\n",
    "2. Fine-tune model\n",
    "\n",
    "https://www.machinecurve.com/index.php/2020/09/29/tensorflow-pruning-schedules-constantsparsity-and-polynomialdecay/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# todo change this\n",
    "PRUNING_EPOCHS = 1\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some layers cannot be pruned\n",
    "\n",
    "def prune_prunable_layers(model, pruning_params):\n",
    "    \"\"\"returns model for pruning with avoided non prunable layers\"\"\"\n",
    "    \n",
    "    # Rescaling layer cannot be pruned\n",
    "    def apply_pruning_to_prunable(layer):\n",
    "        if isinstance(layer, tf.keras.layers.experimental.preprocessing.Rescaling) or isinstance(layer, tf.keras.layers.experimental.preprocessing.Normalization):\n",
    "            return layer\n",
    "        return prune_low_magnitude(layer, **pruning_params)\n",
    "    model_for_pruning = tf.keras.models.clone_model(\n",
    "                                model,\n",
    "                                clone_function=apply_pruning_to_prunable,\n",
    "                            )\n",
    "    return model_for_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model_path, batch_size, pruning_epochs, ds_train, ds_validation):\n",
    "     # check if model was already in optimization folder\n",
    "    if (len(model_path.split('/')[0].split('_')) == 3):\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '/')\n",
    "    else:\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '_optimized/')\n",
    "    model_name = model_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    sparsities = [0.5, 0.75, 0.9]\n",
    "    \n",
    "    ds_train = ds_train.batch(batch_size)\n",
    "    ds_validation = ds_validation.batch(batch_size)\n",
    "    \n",
    "    ds_train.cache()\n",
    "    ds_validation.cache()\n",
    "    \n",
    "    # get number of images\n",
    "    num_images = 0\n",
    "    for i in ds_train.as_numpy_iterator():\n",
    "        num_images+=1\n",
    "\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * pruning_epochs\n",
    "        \n",
    "    for sparsity in sparsities:\n",
    "        # Define pruning configuration\n",
    "        pruning_params_constant = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(target_sparsity=sparsity,\n",
    "                                                                        begin_step=0,\n",
    "                                                                        end_step=end_step)\n",
    "        }\n",
    "        \n",
    "        pruning_params_polynomial = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0,\n",
    "                                                                    final_sparsity=sparsity,\n",
    "                                                                    begin_step=0,\n",
    "                                                                    end_step=end_step)\n",
    "        }\n",
    "        \n",
    "        # Rescaling layer cannot be wrapped in prune low magnitude\n",
    "        try:\n",
    "            model_for_pruning_constant = prune_low_magnitude(model, **pruning_params_constant)\n",
    "            model_for_pruning_polynomial = prune_low_magnitude(model, **pruning_params_polynomial)\n",
    "        except:\n",
    "            model_for_pruning_constant = prune_prunable_layers(model, pruning_params_constant) \n",
    "            model_for_pruning_polynomial = prune_prunable_layers(model, pruning_params_polynomial)\n",
    "            \n",
    "\n",
    "        # Compile models for pruning\n",
    "        model_for_pruning_constant.compile(optimizer='adam',\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])\n",
    "        model_for_pruning_polynomial.compile(optimizer='adam',\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        constant_log_dir = pathlib.Path(\"./tmp/\" + model_name + '_ConstantSparsity' + str(int(sparsity*100)))\n",
    "        constant_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        polynomial_log_dir = pathlib.Path(\"./tmp/\" + model_name + '_PolynomialDecay' + str(int(sparsity*100)))\n",
    "        polynomial_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model callbacks\n",
    "        constant_callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "            tfmot.sparsity.keras.PruningSummaries(log_dir=str(constant_log_dir))\n",
    "        ]\n",
    "        polynomial_callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "            tfmot.sparsity.keras.PruningSummaries(log_dir=str(polynomial_log_dir))\n",
    "        ]\n",
    "\n",
    "        # Fitting data\n",
    "        model_for_pruning_constant.fit(ds_train,\n",
    "                                  validation_data=ds_validation,\n",
    "                                  epochs=pruning_epochs,\n",
    "                                  callbacks=constant_callbacks)\n",
    "        \n",
    "        model_for_pruning_polynomial.fit(ds_train,\n",
    "                                  validation_data=ds_validation,\n",
    "                                  epochs=pruning_epochs,\n",
    "                                  callbacks=polynomial_callbacks)\n",
    "        \n",
    "        # Save pruned models\n",
    "        model_constant_path = optimized_dir/(model_name + '_ConstantSparsity' + str(int(sparsity*100)) + '.h5')\n",
    "        model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning_constant)\n",
    "        model_for_export.save(str(model_constant_path))\n",
    "        print('saved ' + str(model_constant_path))\n",
    "        \n",
    "        model_polynomial_path = optimized_dir/(model_name + '_PolynomialDecay' + str(int(sparsity*100)) + '.h5')\n",
    "        model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning_polynomial)\n",
    "        model_for_export.save(str(model_polynomial_path))\n",
    "        print('saved ' + str(model_polynomial_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some layers cannot be weight clustered\n",
    "\n",
    "def cluster_clustred_layers(model, cluster_params):    \n",
    "    # Rescaling layer cannot be pruned\n",
    "    def apply_clustering_to_clusterable(layer):\n",
    "        if model.layers[0] == layer or  model.layers[-1] == layer:\n",
    "            return layer\n",
    "        try:\n",
    "            x = cluster_weights(layer, **cluster_params)\n",
    "            return x\n",
    "        except:\n",
    "            return layer\n",
    "\n",
    "    model_for_clustering = tf.keras.models.clone_model(\n",
    "                                model,\n",
    "                                clone_function=apply_clustering_to_clusterable,\n",
    "                            )\n",
    "    return model_for_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_cluster_model(model_path, batch_size, epochs, ds_train, ds_validation, number_of_clusters):\n",
    "    \"\"\" Weight clustering on given moodel \n",
    "    note: cannot use for cycle in this function to do different number of clusters because of compatibility issues\"\"\"\n",
    "     # check if model was already in optimization folder\n",
    "    if (len(model_path.split('/')[0].split('_')) == 3):\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '/')\n",
    "    else:\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '_optimized/')\n",
    "    model_name = model_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    ds_train = ds_train.batch(batch_size)\n",
    "    ds_validation = ds_validation.batch(batch_size)\n",
    "    \n",
    "    ds_train.cache()\n",
    "    ds_validation.cache()\n",
    "    \n",
    "    # get number of images\n",
    "    num_images = 0\n",
    "    for i in ds_train.as_numpy_iterator():\n",
    "        num_images+=1\n",
    "\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "    # Define weight clustering configuration\n",
    "    cluster_params_kmeans = {\n",
    "                                  'number_of_clusters': number_of_clusters,\n",
    "                                  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS\n",
    "                                }\n",
    "\n",
    "    # Rescaling layer cannot be wrapped in prune low magnitude\n",
    "    try:\n",
    "        model_for_clustering_kmeans = cluster_weights(model, **cluster_params_kmeans)\n",
    "    except:\n",
    "        model_for_clustering_kmeans = cluster_clustred_layers(model, cluster_params_kmeans)\n",
    "\n",
    "    # Compile models for clustering\n",
    "    model_for_clustering_kmeans.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    # Fitting data        \n",
    "    model_for_clustering_kmeans.fit(ds_train,\n",
    "                              validation_data=ds_validation,\n",
    "                              epochs=epochs)\n",
    "\n",
    "    # Save pruned models\n",
    "    model_kmeans_path = optimized_dir/(model_name + '_KMeansPlusPlus' + str(number_of_clusters) + '.h5')\n",
    "    model_for_export = tfmot.clustering.keras.strip_clustering(model_for_clustering_kmeans)\n",
    "    model_for_export.save(str(model_kmeans_path))\n",
    "    print('saved ' + str(model_kmeans_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimze models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "flowers_datasets = load_flowers_dataset()\n",
    "beans_datasets = load_beans_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Due to bad tensorflow optimization of calling fit function in a loop, calling prune_model in a loop is unusable.\\nSolution: don't use for loops\\nIssue: https://github.com/tensorflow/tensorflow/issues/34025\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Due to bad tensorflow optimization of calling fit function in a loop, calling prune_model in a loop is unusable.\n",
    "Solution: don't use for loops\n",
    "Issue: https://github.com/tensorflow/tensorflow/issues/34025\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prune base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 533s 12s/step - loss: 0.3792 - accuracy: 0.8859 - val_loss: 4.9261 - val_accuracy: 0.4065\n",
      "41/41 [==============================] - 549s 12s/step - loss: 0.2094 - accuracy: 0.9421 - val_loss: 2.5561 - val_accuracy: 0.5644\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity50.h5\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay50.h5\n",
      "41/41 [==============================] - 553s 13s/step - loss: 0.9043 - accuracy: 0.6854 - val_loss: 3.8644 - val_accuracy: 0.2668\n",
      "41/41 [==============================] - 560s 13s/step - loss: 0.5485 - accuracy: 0.8018 - val_loss: 4.0769 - val_accuracy: 0.2668\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity75.h5\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay75.h5\n",
      "41/41 [==============================] - 550s 12s/step - loss: 1.4384 - accuracy: 0.3937 - val_loss: 1.6750 - val_accuracy: 0.2668\n",
      "10/41 [======>.......................] - ETA: 6:40 - loss: 1.1175 - accuracy: 0.6006"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-06eda28b7316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m prune_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mds_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mds_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             pruning_epochs=PRUNING_EPOCHS)\n",
      "\u001b[0;32m<ipython-input-6-9e1beb3d4655>\u001b[0m in \u001b[0;36mprune_model\u001b[0;34m(model_path, batch_size, pruning_epochs, ds_train, ds_validation)\u001b[0m\n\u001b[1;32m     78\u001b[0m                                   callbacks=constant_callbacks)\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         model_for_pruning_polynomial.fit(ds_train,\n\u001b[0m\u001b[1;32m     81\u001b[0m                                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruning_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prune_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "prune_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "prune_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight cluster base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 436s 10s/step - loss: 0.3022 - accuracy: 0.9128 - val_loss: 1.2240 - val_accuracy: 0.6860\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5\n",
      "41/41 [==============================] - 471s 11s/step - loss: 0.5874 - accuracy: 0.8928 - val_loss: 1.1913 - val_accuracy: 0.7223\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus128.h5\n",
      "41/41 [==============================] - 651s 15s/step - loss: 0.0272 - accuracy: 0.9928 - val_loss: 1.9860 - val_accuracy: 0.6479\n",
      "saved flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-de10ff10b411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRUNING_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                      number_of_clusters=32)\n\u001b[0;32m---> 20\u001b[0;31m weight_cluster_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n\u001b[0m\u001b[1;32m     21\u001b[0m                      \u001b[0mds_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                      \u001b[0mds_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5c99d32a386c>\u001b[0m in \u001b[0;36mweight_cluster_model\u001b[0;34m(model_path, batch_size, epochs, ds_train, ds_validation, number_of_clusters)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Fitting data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     model_for_clustering_kmeans.fit(ds_train,\n\u001b[0m\u001b[1;32m     45\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                               epochs=epochs)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weight_cluster_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. prune weight clustered models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. weight cluster prunned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beans models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beans_models/MobileNetV2_beans_model.h5',\n",
       " 'beans_models/EfficentNetB0_beans_model.h5',\n",
       " 'beans_models/MobileNetV2_beans_model_PolynomialDecay75.h5',\n",
       " 'beans_models/MobileNetV2_beans_model_PolynomialDecay87.h5',\n",
       " 'beans_models/MobileNetV2_beans_model_PolynomialDecay50.h5',\n",
       " 'beans_models/MobileNetV2_beans_model_ConstantSparsity87.h5',\n",
       " 'beans_models/MobileNetV2_beans_model_ConstantSparsity50.h5',\n",
       " 'beans_models/MobileNetV2_beans_model_ConstantSparsity75.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model.h5',\n",
       " 'beans_models/EfficentNetB0_flowers_model.h5',\n",
       " 'beans_models/EfficentNetB0_flowers_model_KMeansPlusPlus8.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_KMeansPlusPlus8.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_PolynomialDecay50.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_KMeansPlusPlus128.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_KMeansPlusPlus16.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_KMeansPlusPlus32.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_PolynomialDecay75.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_ConstantSparsity75.h5',\n",
       " 'beans_models/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5',\n",
       " 'beans_models/EfficentNetB0_flowers_model_KMeansPlusPlus16.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_ConstantSparsity50.h5',\n",
       " 'beans_models/EfficentNetB0_flowers_model_KMeansPlusPlus4.h5',\n",
       " 'beans_models/MobileNetV2_flowers_model_KMeansPlusPlus4.h5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all models paths\n",
    "model_paths = []\n",
    "import os\n",
    "for file in os.listdir(\"beans_models/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))\n",
    "for file in os.listdir(\"beans_models_optimized/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))\n",
    "for file in os.listdir(\"flowers_models/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))\n",
    "for file in os.listdir(\"flowers_models_optimized/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/_1/7lg8klcj1d55272nzrt0k5740000gn/T/tmp7e8_zb3g/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/_1/7lg8klcj1d55272nzrt0k5740000gn/T/tmp7e8_zb3g/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted TFLite model (8873924 Bytes) saved to: beans_models_optimized/MobileNetV2_beans_model.tflite\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7097d41c5c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"beans\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel_quantization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeans_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8b0f13698ff5>\u001b[0m in \u001b[0;36mmodel_quantization\u001b[0;34m(model_path, ds)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# 1. optimize model using dynamic range quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtflite_model_quant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtf_quant_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimized_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_dynamic_rage_quantization.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;31m# When storing the given keras model to a saved model is failed, let's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \"\"\"\n\u001b[1;32m   2000\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   2002\u001b[0m                     signatures, options, save_traces)\n\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    154\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    155\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0m\u001b[1;32m    157\u001b[0m                           signatures, options, save_traces)\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_traces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0msave_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m   _, exported_graph, object_saver, asset_info = _build_meta_graph(\n\u001b[0m\u001b[1;32m   1033\u001b[0m       obj, signatures, options, meta_graph_def)\n\u001b[1;32m   1034\u001b[0m   \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_schema_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_SCHEMA_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0msave_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_build_meta_graph_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1130\u001b[0m   \u001b[0mcheckpoint_graph_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_AugmentedGraphView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msignatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     signatures = signature_serialization.find_function_to_export(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         checkpoint_graph_view)\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_serialization.py\u001b[0m in \u001b[0;36mfind_function_to_export\u001b[0;34m(saveable_view)\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;31m# If the user did not specify signatures, check the root object for a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;31m# that can be made into a signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m   \u001b[0mfunctions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_SIGNATURE_ATTR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36mlist_functions\u001b[0;34m(self, obj, extra_functions)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mobj_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj_functions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    151\u001b[0m           self._serialization_cache)\n\u001b[1;32m    152\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_list_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m   2610\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2611\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2612\u001b[0;31m     functions = super(\n\u001b[0m\u001b[1;32m   2613\u001b[0m         Model, self)._list_functions_for_serialization(serialization_cache)\n\u001b[1;32m   2614\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_list_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_list_functions_for_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     return (self._trackable_saved_model_saver\n\u001b[0m\u001b[1;32m   3087\u001b[0m             .list_functions_for_serialization(serialization_cache))\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\u001b[0m in \u001b[0;36mlist_functions_for_serialization\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mfns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions_to_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# The parent AutoTrackable class saves all user-defined tf.functions, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36mfunctions_to_serialize\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfunctions_to_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     return (self._get_serialized_attributes(\n\u001b[0m\u001b[1;32m     79\u001b[0m         serialization_cache).functions_to_serialize)\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mserialized_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     object_dict, function_dict = self._get_serialized_attributes_internal(\n\u001b[0m\u001b[1;32m     95\u001b[0m         serialization_cache)\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# the ones serialized by Layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     objects, functions = (\n\u001b[0;32m---> 56\u001b[0;31m         super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(\n\u001b[0m\u001b[1;32m     57\u001b[0m             serialization_cache))\n\u001b[1;32m     58\u001b[0m     \u001b[0mfunctions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_default_save_signature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\u001b[0m in \u001b[0;36m_get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"\"\"Returns dictionary of serialized attributes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_layer_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mfunctions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_layer_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Attribute validator requires that the default save signature is added to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# function dict, even if the value is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36mwrap_layer_functions\u001b[0;34m(layer, serialization_cache)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0;31m# call with losses) are traced with the same inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0mcall_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerCallCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m   call_fn_with_losses = call_collection.add_function(\n\u001b[0m\u001b[1;32m    164\u001b[0m       \u001b[0m_wrap_call_and_conditional_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       '{}_layer_call_and_return_conditional_losses'.format(layer.name))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36madd_function\u001b[0;34m(self, call_fn, name)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;31m# Manually add traces for layers that have keyword arguments and have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m       \u001b[0;31m# a fully defined input signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36madd_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mtrace_with_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mtrace_with_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36mtrace_with_training\u001b[0;34m(value, fn)\u001b[0m\n\u001b[1;32m    416\u001b[0m           \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_arg_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated_internal_learning_phase_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mtrace_with_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayerCall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1297\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mhas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myet\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0mconcrete\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \"\"\"\n\u001b[0;32m-> 1299\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m       concrete = self._stateful_fn._get_concrete_function_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   1217\u001b[0m           *args, **kwargs)\n\u001b[1;32m   1218\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         if x is not None)\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    407\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaturated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0msaturated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_acd_resource_resolvers_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m       \u001b[0;31m# Resolvers should return true if they are updating the list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m       \u001b[0;31m# resource_inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_path in model_paths:\n",
    "    if \"flowers\" in model_path:\n",
    "        model_quantization(model_path=model_path, ds=flowers_datasets[0])\n",
    "    if \"beans\" in model_path:\n",
    "        model_quantization(model_path=model_path, ds=beans_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_quantization(model_path='flowers_models/EfficentNetB0_flowers_model.h5', ds=load_flowers_dataset()[0])\n",
    "\n",
    "# ds_train, ds_validation, ds_test = load_flowers_dataset()\n",
    "\n",
    "# for i in [4, 8, 16]:\n",
    "#     weight_cluster_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', ds_train=ds_train, ds_validation=ds_validation, batch_size=BATCH_SIZE, epochs=PRUNING_EPOCHS, number_of_clusters=i)\n",
    "    \n",
    "# ds_train, ds_validation, ds_test = load_flowers_dataset()\n",
    "\n",
    "# prune_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', ds_train=ds_train, ds_validation=ds_validation, batch_size=BATCH_SIZE, pruning_epochs=PRUNING_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
