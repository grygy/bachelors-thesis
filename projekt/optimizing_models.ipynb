{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from os import path\n",
    "import pathlib\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the images to [0, 1]\n",
    "def normalize(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        image, size=[256, 256, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_flowers_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    return image, label\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess_flowers(image, label):\n",
    "    image = tf.image.resize(image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label\n",
    "\n",
    "def load_flowers_dataset():  \n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(name=\"tf_flowers\", \n",
    "                                             with_info=True,\n",
    "                                             split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],  #70/15/15 split\n",
    "                                             as_supervised=True)\n",
    "\n",
    "    ds_train = ds_train.map(normalize)    \n",
    "    ds_train = ds_train.map(preprocess_flowers)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    ds_validation = ds_validation.map(preprocess_flowers)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    ds_test = ds_test.map(preprocess_flowers)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test\n",
    "\n",
    "def load_beans_datasets():\n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
    "        'beans',\n",
    "        split=['train', 'validation', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    \n",
    "    ds_train = ds_train.map(normalize)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization after training\n",
    "\n",
    "1. Dynamic range quantization\n",
    "2. Full integer quantization\n",
    "3. Float16 quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dynamic range quantization\n",
    "\n",
    "Only the weights are converted from float to 8 bit int. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Full integer quantization\n",
    "\n",
    "Weights and activation outputs are quantizated. Good for microcontrolers and TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_quantization(model_path, ds):\n",
    "    # check if model was already in optimization folder\n",
    "    if (len(model_path.split('/')[0].split('_')) == 3):\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '/')\n",
    "    else:\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '_optimized/')\n",
    "    model_name = model_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # save converted tflite model\n",
    "    tf_model_path = optimized_dir/(model_name + '.tflite')\n",
    "    size = tf_model_path.write_bytes(tflite_model)\n",
    "    print('Converted TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_model_path))\n",
    "    \n",
    "    # 1. optimize model using dynamic range quantization\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model_quant = converter.convert()\n",
    "    \n",
    "    tf_quant_model_path = optimized_dir/(model_name + '_dynamic_rage_quantization.tflite')\n",
    "    size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "    print('Dynamic range quantizatized TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n",
    "    \n",
    "    # 2. Full integer quantization\n",
    "    def representative_data_gen():\n",
    "        for input_value, _ in ds.batch(1).take(100):\n",
    "            # Model has only one input so each data point has one element.\n",
    "            yield [input_value]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "\n",
    "    tflite_model_quant = converter.convert()\n",
    "    tf_quant_model_path = optimized_dir/(model_name + '_full_integer_quantization.tflite')\n",
    "    size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "    print('Full integer quantizatized TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n",
    "    \n",
    "    # 2.1 Full integer quantization with input and output in integer too\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_data_gen\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "\n",
    "        tflite_model_quant = converter.convert()\n",
    "        tf_quant_model_path = optimized_dir/(model_name + '_full_integer_quantization_integer_io.tflite')\n",
    "        size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "        print('Full integer quantizatized with integer io TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n",
    "    except:\n",
    "        print('ERROR: Failed Full integer quantizatized with integer io TFLite model')\n",
    "        \n",
    "    # 3. float16 quantization\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    tflite_model_quant = converter.convert()\n",
    "    tf_quant_model_path = optimized_dir/(model_name + '_float16_quantization.tflite')\n",
    "    size = tf_quant_model_path.write_bytes(tflite_model_quant)\n",
    "    print('float16 quantizatized TFLite model ('+ str(size) +' Bytes) saved to: ' + str(tf_quant_model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning and fine-tuning\n",
    "\n",
    "1. Prune model to different sparsity  ( tf uses magnitude-based pruning )\n",
    "    1. ConstantSparsity - sparsity is kept constant during training.\n",
    "    2. PolynomialDecay - the degree of sparsity is changed during training.\n",
    "2. Fine-tune model\n",
    "\n",
    "https://www.machinecurve.com/index.php/2020/09/29/tensorflow-pruning-schedules-constantsparsity-and-polynomialdecay/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# todo change this\n",
    "PRUNING_EPOCHS = 1\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some layers cannot be pruned\n",
    "\n",
    "def prune_prunable_layers(model, pruning_params):\n",
    "    \"\"\"returns model for pruning with avoided non prunable layers\"\"\"\n",
    "    \n",
    "    # Rescaling layer cannot be pruned\n",
    "    def apply_pruning_to_prunable(layer):\n",
    "        if isinstance(layer, tf.keras.layers.experimental.preprocessing.Rescaling) or isinstance(layer, tf.keras.layers.experimental.preprocessing.Normalization):\n",
    "            return layer\n",
    "        return prune_low_magnitude(layer, **pruning_params)\n",
    "    model_for_pruning = tf.keras.models.clone_model(\n",
    "                                model,\n",
    "                                clone_function=apply_pruning_to_prunable,\n",
    "                            )\n",
    "    return model_for_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model_path, batch_size, pruning_epochs, ds_train, ds_validation, sparsity):\n",
    "     # check if model was already in optimization folder\n",
    "    if (len(model_path.split('/')[0].split('_')) == 3):\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '/')\n",
    "    else:\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '_optimized/')\n",
    "    model_name = model_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    sparsities = [0.5, 0.75, 0.9]\n",
    "    \n",
    "    ds_train = ds_train.batch(batch_size)\n",
    "    ds_validation = ds_validation.batch(batch_size)\n",
    "    \n",
    "    ds_train.cache()\n",
    "    ds_validation.cache()\n",
    "    \n",
    "    # get number of images\n",
    "    num_images = 0\n",
    "    for i in ds_train.as_numpy_iterator():\n",
    "        num_images+=1\n",
    "\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * pruning_epochs\n",
    "\n",
    "    # Define pruning configuration\n",
    "    pruning_params_constant = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(target_sparsity=sparsity,\n",
    "                                                                    begin_step=0,\n",
    "                                                                    end_step=end_step)\n",
    "    }\n",
    "\n",
    "    pruning_params_polynomial = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0,\n",
    "                                                                final_sparsity=sparsity,\n",
    "                                                                begin_step=0,\n",
    "                                                                end_step=end_step)\n",
    "    }\n",
    "\n",
    "    # Rescaling layer cannot be wrapped in prune low magnitude\n",
    "    try:\n",
    "        model_for_pruning_constant = prune_low_magnitude(model, **pruning_params_constant)\n",
    "        model_for_pruning_polynomial = prune_low_magnitude(model, **pruning_params_polynomial)\n",
    "    except:\n",
    "        model_for_pruning_constant = prune_prunable_layers(model, pruning_params_constant) \n",
    "        model_for_pruning_polynomial = prune_prunable_layers(model, pruning_params_polynomial)\n",
    "\n",
    "\n",
    "    # Compile models for pruning\n",
    "    model_for_pruning_constant.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "    model_for_pruning_polynomial.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    constant_log_dir = pathlib.Path(\"./tmp/\" + model_name + '_ConstantSparsity' + str(int(sparsity*100)))\n",
    "    constant_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    polynomial_log_dir = pathlib.Path(\"./tmp/\" + model_name + '_PolynomialDecay' + str(int(sparsity*100)))\n",
    "    polynomial_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Model callbacks\n",
    "    constant_callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=str(constant_log_dir))\n",
    "    ]\n",
    "    polynomial_callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=str(polynomial_log_dir))\n",
    "    ]\n",
    "\n",
    "    # Fitting data\n",
    "    model_for_pruning_constant.fit(ds_train,\n",
    "                              validation_data=ds_validation,\n",
    "                              epochs=pruning_epochs,\n",
    "                              callbacks=constant_callbacks)\n",
    "\n",
    "    model_for_pruning_polynomial.fit(ds_train,\n",
    "                              validation_data=ds_validation,\n",
    "                              epochs=pruning_epochs,\n",
    "                              callbacks=polynomial_callbacks)\n",
    "\n",
    "    # Save pruned models\n",
    "    model_constant_path = optimized_dir/(model_name + '_ConstantSparsity' + str(int(sparsity*100)) + '.h5')\n",
    "    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning_constant)\n",
    "    model_for_export.save(str(model_constant_path))\n",
    "    print('saved ' + str(model_constant_path))\n",
    "\n",
    "    model_polynomial_path = optimized_dir/(model_name + '_PolynomialDecay' + str(int(sparsity*100)) + '.h5')\n",
    "    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning_polynomial)\n",
    "    model_for_export.save(str(model_polynomial_path))\n",
    "    print('saved ' + str(model_polynomial_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some layers cannot be weight clustered\n",
    "\n",
    "def cluster_clustred_layers(model, cluster_params):    \n",
    "    # Rescaling layer cannot be pruned\n",
    "    def apply_clustering_to_clusterable(layer):\n",
    "        if model.layers[0] == layer or  model.layers[-1] == layer:\n",
    "            return layer\n",
    "        try:\n",
    "            x = cluster_weights(layer, **cluster_params)\n",
    "            return x\n",
    "        except:\n",
    "            return layer\n",
    "\n",
    "    model_for_clustering = tf.keras.models.clone_model(\n",
    "                                model,\n",
    "                                clone_function=apply_clustering_to_clusterable,\n",
    "                            )\n",
    "    return model_for_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_cluster_model(model_path, batch_size, epochs, ds_train, ds_validation, number_of_clusters):\n",
    "    \"\"\" Weight clustering on given moodel \n",
    "    note: cannot use for cycle in this function to do different number of clusters because of compatibility issues\"\"\"\n",
    "     # check if model was already in optimization folder\n",
    "    if (len(model_path.split('/')[0].split('_')) == 3):\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '/')\n",
    "    else:\n",
    "        optimized_dir = pathlib.Path(model_path.split('/')[0] + '_optimized/')\n",
    "    model_name = model_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    ds_train = ds_train.batch(batch_size)\n",
    "    ds_validation = ds_validation.batch(batch_size)\n",
    "    \n",
    "    ds_train.cache()\n",
    "    ds_validation.cache()\n",
    "    \n",
    "    # get number of images\n",
    "    num_images = 0\n",
    "    for i in ds_train.as_numpy_iterator():\n",
    "        num_images+=1\n",
    "\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "    # Define weight clustering configuration\n",
    "    cluster_params_kmeans = {\n",
    "                                  'number_of_clusters': number_of_clusters,\n",
    "                                  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS\n",
    "                                }\n",
    "\n",
    "    # Rescaling layer cannot be wrapped in prune low magnitude\n",
    "    try:\n",
    "        model_for_clustering_kmeans = cluster_weights(model, **cluster_params_kmeans)\n",
    "    except:\n",
    "        model_for_clustering_kmeans = cluster_clustred_layers(model, cluster_params_kmeans)\n",
    "\n",
    "    # Compile models for clustering\n",
    "    model_for_clustering_kmeans.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    # Fitting data        \n",
    "    model_for_clustering_kmeans.fit(ds_train,\n",
    "                              validation_data=ds_validation,\n",
    "                              epochs=epochs)\n",
    "\n",
    "    # Save pruned models\n",
    "    model_kmeans_path = optimized_dir/(model_name + '_KMeansPlusPlus' + str(number_of_clusters) + '.h5')\n",
    "    model_for_export = tfmot.clustering.keras.strip_clustering(model_for_clustering_kmeans)\n",
    "    model_for_export.save(str(model_kmeans_path))\n",
    "    print('saved ' + str(model_kmeans_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimze models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "flowers_datasets = load_flowers_dataset()\n",
    "beans_datasets = load_beans_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Due to bad tensorflow optimization of calling fit function in a loop, calling prune_model in a loop is unusable.\n",
    "Solution: don't use for loops\n",
    "Issue: https://github.com/tensorflow/tensorflow/issues/34025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prune base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 543s 13s/step - loss: 0.4098 - accuracy: 0.8782 - val_loss: 6.9961 - val_accuracy: 0.2886\n",
      "41/41 [==============================] - 573s 13s/step - loss: 0.2249 - accuracy: 0.9301 - val_loss: 3.4788 - val_accuracy: 0.5118\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity50.h5\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay50.h5\n"
     ]
    }
   ],
   "source": [
    "prune_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "prune_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1107s 63s/step - loss: 0.6799 - accuracy: 0.8356 - val_loss: 4.9649 - val_accuracy: 0.4887\n",
      "17/17 [==============================] - 1159s 66s/step - loss: 0.2908 - accuracy: 0.8886 - val_loss: 2.5494 - val_accuracy: 0.6842\n",
      "saved beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity50.h5\n",
      "saved beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay50.h5\n",
      "15/17 [=========================>....] - ETA: 2:16 - loss: 1.3774 - accuracy: 0.6057"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-90ad8dd043fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mpruning_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRUNING_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             sparsity=0.5)\n\u001b[0;32m----> 7\u001b[0;31m prune_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mds_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeans_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mds_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeans_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a3166338c0d5>\u001b[0m in \u001b[0;36mprune_model\u001b[0;34m(model_path, batch_size, pruning_epochs, ds_train, ds_validation, sparsity)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Fitting data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     model_for_pruning_constant.fit(ds_train,\n\u001b[0m\u001b[1;32m     75\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruning_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prune_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "prune_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1], \n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight cluster base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 474s 11s/step - loss: 0.5320 - accuracy: 0.8901 - val_loss: 1.3263 - val_accuracy: 0.6733\n",
      "saved flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5\n"
     ]
    }
   ],
   "source": [
    "weight_cluster_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models/MobileNetV2_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models/EfficentNetB0_flowers_model.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 963s 56s/step - loss: 6.5371 - accuracy: 0.7326 - val_loss: 1.2269 - val_accuracy: 0.3534\n",
      "saved beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus32.h5\n",
      "17/17 [==============================] - 944s 54s/step - loss: 6.4488 - accuracy: 0.6275 - val_loss: 1.0090 - val_accuracy: 0.4211\n",
      "saved beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus128.h5\n"
     ]
    }
   ],
   "source": [
    "weight_cluster_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models/MobileNetV2_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models/EfficentNetB0_beans_model.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. prune weight clustered models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prune_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a829b480aa44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5', \n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mds_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mds_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowers_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mpruning_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPRUNING_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prune_model' is not defined"
     ]
    }
   ],
   "source": [
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "# ---------- EfficentNet ---------\n",
    "\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=flowers_datasets[0], \n",
    "            ds_validation=flowers_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='beans_models_optimized/MobileNetV2_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "# ---------- EfficentNet ---------\n",
    "\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus32.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)\n",
    "\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.5)\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.75)\n",
    "prune_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_KMeansPlusPlus128.h5', \n",
    "            ds_train=beans_datasets[0], \n",
    "            ds_validation=beans_datasets[1],\n",
    "            batch_size=BATCH_SIZE, \n",
    "            pruning_epochs=PRUNING_EPOCHS,\n",
    "            sparsity=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. weight cluster prunned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/MobileNetV2_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay50.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay75.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='flowers_models_optimized/EfficentNetB0_flowers_model_PolynomialDecay90.h5', \n",
    "                     ds_train=flowers_datasets[0], \n",
    "                     ds_validation=flowers_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beans models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/MobileNetV2_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay50.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay75.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_ConstantSparsity90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)\n",
    "\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=32)\n",
    "weight_cluster_model(model_path='beans_models_optimized/EfficentNetB0_beans_model_PolynomialDecay90.h5', \n",
    "                     ds_train=beans_datasets[0], \n",
    "                     ds_validation=beans_datasets[1], \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=PRUNING_EPOCHS, \n",
    "                     number_of_clusters=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all models paths\n",
    "model_paths = []\n",
    "import os\n",
    "for file in os.listdir(\"beans_models/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))\n",
    "for file in os.listdir(\"beans_models_optimized/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))\n",
    "for file in os.listdir(\"flowers_models/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))\n",
    "for file in os.listdir(\"flowers_models_optimized/\"):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_paths.append(str(os.path.join(\"beans_models/\", file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_path in model_paths:\n",
    "    if \"flowers\" in model_path:\n",
    "        model_quantization(model_path=model_path, ds=flowers_datasets[0])\n",
    "    if \"beans\" in model_path:\n",
    "        model_quantization(model_path=model_path, ds=beans_datasets[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
