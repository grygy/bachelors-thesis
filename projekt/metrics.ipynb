{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "\n",
    "https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "from json import JSONEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatedModel:\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    model_name = ''\n",
    "    avg_time = 0.0\n",
    "    gzip_size = 0\n",
    "    \n",
    "    accuracy = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    f1_score = None\n",
    "    \n",
    "\n",
    "    def __init__(self, dict1=None, y_pred=[], y_true=[], model_name='', avg_time=0.0, gzip_size=0):\n",
    "        if (dict1 == None):\n",
    "            self.y_pred = copy.deepcopy(y_pred)\n",
    "            self.y_true = copy.deepcopy(y_true)\n",
    "            self.model_name = model_name\n",
    "            self.avg_time = avg_time\n",
    "            self.gzip_size = gzip_size\n",
    "        else:\n",
    "            self.__dict__.update(dict1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute metrics for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def annote_models(path):\n",
    "    with open(path) as f:\n",
    "        evaluated_models = json.load(f, object_hook=EvaluatedModel)\n",
    "\n",
    "    for evaluated_model in evaluated_models:\n",
    "        evaluated_model.accuracy = accuracy_score(y_pred=evaluated_model.y_pred, y_true=evaluated_model.y_true)\n",
    "        evaluated_model.precision = precision_score(y_pred=evaluated_model.y_pred, y_true=evaluated_model.y_true, average='micro')\n",
    "        evaluated_model.recall = recall_score(y_pred=evaluated_model.y_pred, y_true=evaluated_model.y_true, average='micro')\n",
    "        evaluated_model.f1_score = f1_score(y_pred=evaluated_model.y_pred, y_true=evaluated_model.y_true, average='micro')\n",
    "        # change model name for table\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"Efficent\", \"Efficient\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"KMeansPlusPlus32\", \"WQ32\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"KMeansPlusPlus128\", \"WQ128\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"full_integer_quantization\", \"FIQ\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"dynamic_rage_quantization\", \"DRQ\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"float16_quantization\", \"F16Q\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"_integer_io\", \"-Iio\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"PolynomialDecay90\", \"PD90\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"PolynomialDecay75\", \"PD75\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"PolynomialDecay50\", \"PD50\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"flowers_model_\", \"\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"beans_model_\", \"\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"ConstantSparsity90\", \"CS90\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"ConstantSparsity75\", \"CS75\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"ConstantSparsity50\", \"CS50\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\".tflite\", \"\")\n",
    "        evaluated_model.model_name = evaluated_model.model_name.replace(\"_\", \" -> \")\n",
    "        \n",
    "    return evaluated_models\n",
    "\n",
    "def print_table(models):\n",
    "    print(\"|Model name|Accuarcy|Precision|Recall|F1 score|Compressed size|Average time|\")\n",
    "    print(\"|-----|-----|-----|-----|-----|-----|-----|\")\n",
    "    for model in models:\n",
    "        print(\"|\"+model.model_name+\"|\"+str(round(model.accuracy * 100, 2))+\"|\"+str(round(model.precision * 100, 2))\n",
    "              +\"|\"+str(round(model.recall * 100, 2))+\"|\"+str(round(model.f1_score * 100, 2))+\"|\"\n",
    "              +str(round(model.gzip_size / 1024))+\" kB|\"+str(round(model.avg_time, 2))+\" ms|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Model name|Accuarcy|Precision|Recall|F1 score|Compressed size|Average time|\n",
      "|-----|-----|-----|-----|-----|-----|-----|\n",
      "|MobileNetV2 -> CS50 -> FIQ-Iio|0.36|0.36|0.36|0.36|2334 kB|51.8 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ32 -> FIQ-Iio|0.36|0.36|0.36|0.36|1855 kB|50.74 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ128 -> DRQ|0.24|0.24|0.24|0.24|2224 kB|75.25 ms|\n",
      "|MobileNetV2 -> CS75 -> FIQ-Iio|0.36|0.36|0.36|0.36|2155 kB|46.59 ms|\n",
      "|MobileNetV2 -> CS90 -> DRQ|0.33|0.33|0.33|0.33|1971 kB|76.82 ms|\n",
      "|MobileNetV2 -> PD50 -> FIQ-Iio|0.36|0.36|0.36|0.36|2334 kB|47.71 ms|\n",
      "|MobileNetV2 -> PD75 -> FIQ|0.37|0.37|0.37|0.37|2154 kB|46.01 ms|\n",
      "|MobileNetV2 -> WQ128 -> FIQ-Iio|0.33|0.33|0.33|0.33|2370 kB|46.65 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ32 -> FIQ|0.28|0.28|0.28|0.28|1955 kB|45.95 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ128 -> F16Q|0.37|0.37|0.37|0.37|3734 kB|64.18 ms|\n",
      "|MobileNetV2 -> CS75|0.36|0.36|0.36|0.36|8559 kB|65.56 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ128 -> FIQ|0.54|0.54|0.54|0.54|2022 kB|45.94 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ32 -> FIQ-Iio|0.41|0.41|0.41|0.41|1755 kB|46.64 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ32|0.41|0.41|0.41|0.41|2954 kB|62.49 ms|\n",
      "|MobileNetV2 -> FIQ-Iio|0.49|0.49|0.49|0.49|2432 kB|46.65 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ32 -> F16Q|0.37|0.37|0.37|0.37|2817 kB|62.68 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ32|0.29|0.29|0.29|0.29|3159 kB|63.39 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ32 -> DRQ|0.39|0.39|0.39|0.39|1721 kB|75.7 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ128 -> FIQ-Iio|0.41|0.41|0.41|0.41|2021 kB|50.11 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ32|0.28|0.28|0.28|0.28|3372 kB|62.99 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ128 -> FIQ|0.34|0.34|0.34|0.34|2072 kB|45.96 ms|\n",
      "|MobileNetV2 -> WQ32 -> DRQ|0.42|0.42|0.42|0.42|1972 kB|78.74 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ32 -> DRQ|0.44|0.44|0.44|0.44|1721 kB|75.68 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ32|0.39|0.39|0.39|0.39|3158 kB|62.87 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ128 -> F16Q|0.34|0.34|0.34|0.34|3837 kB|62.65 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ128 -> DRQ|0.33|0.33|0.33|0.33|2039 kB|76.23 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ128 -> F16Q|0.5|0.5|0.5|0.5|3759 kB|62.86 ms|\n",
      "|MobileNetV2 -> WQ128 -> DRQ|0.31|0.31|0.31|0.31|2336 kB|75.64 ms|\n",
      "|MobileNetV2 -> CS90|0.33|0.33|0.33|0.33|8512 kB|62.66 ms|\n",
      "|MobileNetV2 -> PD75 -> FIQ-Iio|0.36|0.36|0.36|0.36|2154 kB|46.59 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ32 -> DRQ|0.26|0.26|0.26|0.26|1925 kB|75.5 ms|\n",
      "|MobileNetV2 -> PD90|0.33|0.33|0.33|0.33|8512 kB|62.68 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ128 -> FIQ-Iio|0.26|0.26|0.26|0.26|2071 kB|46.6 ms|\n",
      "|MobileNetV2 -> F16Q|0.47|0.47|0.47|0.47|4201 kB|62.43 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ32 -> FIQ-Iio|0.26|0.26|0.26|0.26|1962 kB|46.6 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ128|0.26|0.26|0.26|0.26|5055 kB|62.8 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ32 -> FIQ|0.41|0.41|0.41|0.41|1755 kB|46.19 ms|\n",
      "|MobileNetV2 -> WQ32 -> FIQ-Iio|0.39|0.39|0.39|0.39|2001 kB|46.6 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ128|0.24|0.24|0.24|0.24|5051 kB|63.13 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ32 -> F16Q|0.29|0.29|0.29|0.29|2815 kB|71.09 ms|\n",
      "|MobileNetV2 -> CS90 -> FIQ-Iio|0.33|0.33|0.33|0.33|2008 kB|47.43 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ32|0.28|0.28|0.28|0.28|3371 kB|62.97 ms|\n",
      "|MobileNetV2 -> WQ32 -> F16Q|0.34|0.34|0.34|0.34|3238 kB|62.46 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ32 -> FIQ-Iio|0.44|0.44|0.44|0.44|1755 kB|46.67 ms|\n",
      "|MobileNetV2 -> PD50 -> DRQ|0.36|0.36|0.36|0.36|2301 kB|78.64 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ128 -> DRQ|0.37|0.37|0.37|0.37|1988 kB|77.23 ms|\n",
      "|MobileNetV2 -> WQ32 -> FIQ|0.34|0.34|0.34|0.34|2001 kB|50.66 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ32 -> F16Q|0.41|0.41|0.41|0.41|2604 kB|62.39 ms|\n",
      "|MobileNetV2 -> DRQ|0.47|0.47|0.47|0.47|2399 kB|82.38 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ32 -> FIQ-Iio|0.29|0.29|0.29|0.29|1833 kB|52.09 ms|\n",
      "|MobileNetV2 -> PD50 -> F16Q|0.36|0.36|0.36|0.36|4262 kB|62.45 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ128 -> FIQ-Iio|0.31|0.31|0.31|0.31|2072 kB|46.59 ms|\n",
      "|MobileNetV2 -> WQ128 -> F16Q|0.31|0.31|0.31|0.31|3961 kB|62.69 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ128 -> F16Q|0.24|0.24|0.24|0.24|3802 kB|67.31 ms|\n",
      "|MobileNetV2 -> CS90 -> F16Q|0.33|0.33|0.33|0.33|4236 kB|67.05 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ128 -> FIQ|0.26|0.26|0.26|0.26|2071 kB|46.09 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ128 -> F16Q|0.26|0.26|0.26|0.26|3967 kB|62.41 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ128 -> FIQ|0.24|0.24|0.24|0.24|2257 kB|47.37 ms|\n",
      "|MobileNetV2 -> CS75 -> DRQ|0.36|0.36|0.36|0.36|2122 kB|77.76 ms|\n",
      "|MobileNetV2 -> CS50 -> DRQ|0.36|0.36|0.36|0.36|2301 kB|76.01 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ128 -> FIQ-Iio|0.5|0.5|0.5|0.5|2022 kB|46.65 ms|\n",
      "|MobileNetV2 -> PD75 -> F16Q|0.36|0.36|0.36|0.36|4265 kB|62.63 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ128 -> FIQ|0.28|0.28|0.28|0.28|2254 kB|45.95 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ32 -> FIQ-Iio|0.26|0.26|0.26|0.26|1955 kB|46.68 ms|\n",
      "|MobileNetV2 -> CS75 -> F16Q|0.36|0.36|0.36|0.36|4265 kB|62.73 ms|\n",
      "|MobileNetV2 -> WQ128|0.31|0.31|0.31|0.31|5170 kB|62.64 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ32 -> DRQ|0.31|0.31|0.31|0.31|1920 kB|77.27 ms|\n",
      "|MobileNetV2 -> PD90 -> DRQ|0.33|0.33|0.33|0.33|1971 kB|75.3 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ128 -> FIQ-Iio|0.26|0.26|0.26|0.26|2257 kB|51.65 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ32 -> F16Q|0.46|0.46|0.46|0.46|2604 kB|62.75 ms|\n",
      "|MobileNetV2 -> CS50 -> F16Q|0.36|0.36|0.36|0.36|4262 kB|62.55 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ32 -> DRQ|0.28|0.28|0.28|0.28|1801 kB|75.76 ms|\n",
      "|MobileNetV2 -> PD75 -> DRQ|0.36|0.36|0.36|0.36|2122 kB|87.98 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ128|0.5|0.5|0.5|0.5|4531 kB|63.56 ms|\n",
      "|MobileNetV2 -> FIQ|0.49|0.49|0.49|0.49|2433 kB|46.11 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ128|0.37|0.37|0.37|0.37|4532 kB|63.32 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ128|0.24|0.24|0.24|0.24|4778 kB|62.68 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ32 -> DRQ|0.34|0.34|0.34|0.34|1822 kB|75.46 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ128 -> FIQ|0.39|0.39|0.39|0.39|2021 kB|46.14 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ32 -> F16Q|0.28|0.28|0.28|0.28|3117 kB|62.49 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ128 -> FIQ-Iio|0.23|0.23|0.23|0.23|2254 kB|50.11 ms|\n",
      "|MobileNetV2 -> PD50 -> FIQ|0.34|0.34|0.34|0.34|2334 kB|52.81 ms|\n",
      "|MobileNetV2 -> PD90 -> F16Q|0.33|0.33|0.33|0.33|4236 kB|62.37 ms|\n",
      "|MobileNetV2 -> CS50 -> FIQ|0.36|0.36|0.36|0.36|2334 kB|45.93 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ128|0.34|0.34|0.34|0.34|4783 kB|62.51 ms|\n",
      "|MobileNetV2 -> PD75|0.36|0.36|0.36|0.36|8559 kB|62.88 ms|\n",
      "|MobileNetV2 -> WQ128 -> FIQ|0.29|0.29|0.29|0.29|2370 kB|52.9 ms|\n",
      "|MobileNetV2 -> CS50 -> WQ128 -> DRQ|0.24|0.24|0.24|0.24|2221 kB|75.86 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ128 -> DRQ|0.49|0.49|0.49|0.49|1989 kB|75.81 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ32 -> FIQ|0.26|0.26|0.26|0.26|1833 kB|45.96 ms|\n",
      "|MobileNetV2 -> CS90 -> FIQ|0.33|0.33|0.33|0.33|2009 kB|46.62 ms|\n",
      "|MobileNetV2 -> CS50|0.36|0.36|0.36|0.36|8536 kB|62.65 ms|\n",
      "|MobileNetV2 -> PD50|0.36|0.36|0.36|0.36|8536 kB|64.76 ms|\n",
      "|MobileNetV2 -> WQ32|0.34|0.34|0.34|0.34|3470 kB|63.59 ms|\n",
      "|MobileNetV2 -> CS75 -> FIQ|0.37|0.37|0.37|0.37|2154 kB|48.44 ms|\n",
      "|MobileNetV2 -> flowers -> model|0.47|0.47|0.47|0.47|8463 kB|63.58 ms|\n",
      "|MobileNetV2 -> PD90 -> WQ32|0.46|0.46|0.46|0.46|2955 kB|66.98 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ32 -> FIQ|0.28|0.28|0.28|0.28|1962 kB|45.95 ms|\n",
      "|MobileNetV2 -> CS75 -> WQ32 -> FIQ|0.36|0.36|0.36|0.36|1855 kB|49.25 ms|\n",
      "|MobileNetV2 -> PD90 -> FIQ|0.33|0.33|0.33|0.33|2008 kB|56.86 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ32 -> F16Q|0.28|0.28|0.28|0.28|3120 kB|62.44 ms|\n",
      "|MobileNetV2 -> PD90 -> FIQ-Iio|0.33|0.33|0.33|0.33|2009 kB|49.82 ms|\n",
      "|MobileNetV2 -> PD50 -> WQ128 -> F16Q|0.24|0.24|0.24|0.24|3939 kB|62.31 ms|\n",
      "|MobileNetV2 -> CS90 -> WQ32 -> FIQ|0.36|0.36|0.36|0.36|1755 kB|45.9 ms|\n",
      "|MobileNetV2 -> PD75 -> WQ128 -> DRQ|0.24|0.24|0.24|0.24|2040 kB|75.9 ms|\n"
     ]
    }
   ],
   "source": [
    "flowers_models = annote_models('results/evaluated_flowers_models_on_MobileNetV2.json')\n",
    "print_table(flowers_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from os import path\n",
    "import pathlib\n",
    "import tempfile\n",
    "\n",
    "# normalizing the images to [0, 1]\n",
    "def normalize(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        image, size=[256, 256, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_flowers_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    return image, label\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess_flowers(image, label):\n",
    "    image = tf.image.resize(image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label\n",
    "\n",
    "def load_flowers_dataset():  \n",
    "    ds_test, ds_info = tfds.load(name=\"tf_flowers\", \n",
    "                                with_info=True,\n",
    "                                split=['test'],\n",
    "                                as_supervised=True)\n",
    "\n",
    "#     ds_train = ds_train.map(normalize)    \n",
    "#     ds_train = ds_train.map(preprocess_flowers)\n",
    "#     ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    \n",
    "#     ds_validation = ds_validation.map(normalize)\n",
    "#     ds_validation = ds_validation.map(preprocess_flowers)\n",
    "    ds_train = None\n",
    "    ds_validation= None\n",
    "    ds_test = ds_test.map(normalize)\n",
    "    ds_test = ds_test.map(preprocess_flowers)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test\n",
    "\n",
    "def load_beans_datasets():\n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
    "        'beans',\n",
    "        split=['train', 'validation', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    \n",
    "    ds_train = ds_train.map(normalize)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('flowers_models_optimized/EfficentNetB0_flowers_model_ConstantSparsity50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"test\". Should be one of ['train'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-191890867562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflowers_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_flowers_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mflowers_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflowers_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflowers_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-5534fb472985>\u001b[0m in \u001b[0;36mload_flowers_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_flowers_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     ds_test, ds_info = tfds.load(name=\"tf_flowers\", \n\u001b[0m\u001b[1;32m     49\u001b[0m                                 \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                 \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0mas_dataset_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read_config'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mas_dataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_supervised\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     )\n\u001b[0;32m--> 593\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_single_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mtypes_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       mapped = [map_nested(function, v, dict_only, map_tuple)\n\u001b[0m\u001b[1;32m    178\u001b[0m                 for v in data_struct]\n\u001b[1;32m    179\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mtypes_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       mapped = [map_nested(function, v, dict_only, map_tuple)\n\u001b[0m\u001b[1;32m    178\u001b[0m                 for v in data_struct]\n\u001b[1;32m    179\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[0;34m(self, split, shuffle_files, batch_size, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Build base dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     ds = self._as_dataset(\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_as_dataset\u001b[0;34m(self, split, decoders, read_config, shuffle_files)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     )\n\u001b[0;32m--> 968\u001b[0;31m     return self._tfrecords_reader.read(\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/tfrecords_reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, name, instructions, split_infos, read_config, shuffle_files, decode_fn)\u001b[0m\n\u001b[1;32m    360\u001b[0m       )\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_read_instruction_to_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m   def read_files(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/tfrecords_reader.py\u001b[0m in \u001b[0;36m_read_instruction_to_ds\u001b[0;34m(instruction)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_instruction_to_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       file_instructions = make_file_instructions(\n\u001b[0m\u001b[1;32m    354\u001b[0m           name, split_infos, instruction, file_format=self._file_format)\n\u001b[1;32m    355\u001b[0m       return self.read_files(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/tfrecords_reader.py\u001b[0m in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, file_format)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0minstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;31m# Create the absolute instruction (per split)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m   \u001b[0mabsolute_instructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_absolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname2len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   return _make_file_instructions_from_absolutes(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/tfrecords_reader.py\u001b[0m in \u001b[0;36mto_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0m_AbsoluteInstruction\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \"\"\"\n\u001b[0;32m--> 667\u001b[0;31m     return [_rel_to_abs_instr(rel_instr, name2len)\n\u001b[0m\u001b[1;32m    668\u001b[0m             for rel_instr in self._relative_instructions]\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/tfrecords_reader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0m_AbsoluteInstruction\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \"\"\"\n\u001b[0;32m--> 667\u001b[0;31m     return [_rel_to_abs_instr(rel_instr, name2len)\n\u001b[0m\u001b[1;32m    668\u001b[0m             for rel_instr in self._relative_instructions]\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow_datasets/core/tfrecords_reader.py\u001b[0m in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_instr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname2len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     raise ValueError('Unknown split \"{}\". Should be one of {}.'.format(\n\u001b[0m\u001b[1;32m    495\u001b[0m         split, list(name2len)))\n\u001b[1;32m    496\u001b[0m   \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname2len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"test\". Should be one of ['train']."
     ]
    }
   ],
   "source": [
    "flowers_test = load_flowers_dataset()[2]\n",
    "flowers_test = flowers_test.batch(1)\n",
    "new_model.evaluate(flowers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
