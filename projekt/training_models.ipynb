{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "from os import path\n",
    "%load_ext tensorboard\n",
    "\n",
    "def normalize(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo change this to 100+\n",
    "\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, ds_train, ds_validation, model_name, batch_size=64):\n",
    "#    if path.exists(model_path):\n",
    "#        print(\"Model is already trained and saved here: \" + model_path)\n",
    "#        return\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    \n",
    "    save = callbacks.ModelCheckpoint(\n",
    "        os.path.join('tmp', model_name + \"_e_{epoch:02d}.h5\"),\n",
    "        monitor='loss',\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        mode='auto')\n",
    "\n",
    "    early = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                    min_delta=0,\n",
    "                                    patience=30,\n",
    "                                    verbose=1,\n",
    "                                    mode='auto')\n",
    "\n",
    "    hist = model.fit(ds_train, \n",
    "                     epochs=NUM_EPOCHS, \n",
    "                     validation_data=ds_validation,\n",
    "                     callbacks=[save]\n",
    "                    )\n",
    "    \n",
    "    # save model\n",
    "    model.save(model_name + \".h5\")\n",
    "    print('Saved to: ' + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ds_to_tensors(ds):\n",
    "    \"\"\"returns tuple of train_X, train_y\"\"\"\n",
    "    a = ds.map(lambda a, b: a)\n",
    "    tf_list = []\n",
    "    for i in a:\n",
    "        tf_list.append(i)\n",
    "    train_X = tf.stack(tf_list, axis=0)\n",
    "    \n",
    "    b = ds.map(lambda a, b: b)\n",
    "    tf_list = []\n",
    "    for i in b:\n",
    "        tf_list.append([i.numpy()])\n",
    "    train_y = np.array(tf_list, dtype=np.uint8)\n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beans dataset\n",
    "\n",
    "https://www.tensorflow.org/datasets/catalog/beans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_beans_train(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)    \n",
    "    return image, label\n",
    "\n",
    "def preprocess_beans_test_and_val(image, label):\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrap model for beans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model_for_beans(base_model, num_classes):\n",
    "    inputs = base_model.inputs\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "INPUT_SHAPE = (500, 500, 3)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_beans_datasets():\n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
    "        'beans',\n",
    "        split=['train', 'validation', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    \n",
    "    ds_train = ds_train.map(normalize)\n",
    "    #ds_train = ds_train.map(preprocess_beans_train)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples).batch(BATCH_SIZE)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    #ds_validation = ds_validation.map(preprocess_beans_test_and_val)\n",
    "    ds_validation = ds_validation.batch(BATCH_SIZE)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    #ds_test = ds_test.map(preprocess_beans_test_and_val)\n",
    "    ds_test = ds_test.batch(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_validation, ds_test = load_beans_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'MobileNetV2_beans_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "33/33 [==============================] - 40s 951ms/step - loss: 1.0113 - sparse_categorical_accuracy: 0.4955 - val_loss: 1.0988 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00001: saving model to tmp/MobileNetV2_beans_model_e_01.h5\n",
      "Epoch 2/300\n",
      "33/33 [==============================] - 33s 892ms/step - loss: 0.7942 - sparse_categorical_accuracy: 0.6594 - val_loss: 1.0999 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00002: saving model to tmp/MobileNetV2_beans_model_e_02.h5\n",
      "Epoch 3/300\n",
      "33/33 [==============================] - 33s 911ms/step - loss: 0.7157 - sparse_categorical_accuracy: 0.6887 - val_loss: 1.1011 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00003: saving model to tmp/MobileNetV2_beans_model_e_03.h5\n",
      "Epoch 4/300\n",
      "33/33 [==============================] - 33s 887ms/step - loss: 0.6170 - sparse_categorical_accuracy: 0.7321 - val_loss: 1.1077 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00004: saving model to tmp/MobileNetV2_beans_model_e_04.h5\n",
      "Epoch 5/300\n",
      "33/33 [==============================] - 33s 907ms/step - loss: 0.4606 - sparse_categorical_accuracy: 0.8178 - val_loss: 1.1073 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00005: saving model to tmp/MobileNetV2_beans_model_e_05.h5\n",
      "Epoch 6/300\n",
      "33/33 [==============================] - 33s 901ms/step - loss: 0.5380 - sparse_categorical_accuracy: 0.7787 - val_loss: 1.1228 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00006: saving model to tmp/MobileNetV2_beans_model_e_06.h5\n",
      "Epoch 7/300\n",
      "33/33 [==============================] - 33s 903ms/step - loss: 0.4304 - sparse_categorical_accuracy: 0.8242 - val_loss: 1.1385 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00007: saving model to tmp/MobileNetV2_beans_model_e_07.h5\n",
      "Epoch 8/300\n",
      "33/33 [==============================] - 33s 910ms/step - loss: 0.3088 - sparse_categorical_accuracy: 0.8626 - val_loss: 1.1566 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00008: saving model to tmp/MobileNetV2_beans_model_e_08.h5\n",
      "Epoch 9/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.2998 - sparse_categorical_accuracy: 0.8881 - val_loss: 1.1687 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00009: saving model to tmp/MobileNetV2_beans_model_e_09.h5\n",
      "Epoch 10/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.2238 - sparse_categorical_accuracy: 0.9141 - val_loss: 1.2109 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00010: saving model to tmp/MobileNetV2_beans_model_e_10.h5\n",
      "Epoch 11/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.1683 - sparse_categorical_accuracy: 0.9446 - val_loss: 1.2682 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00011: saving model to tmp/MobileNetV2_beans_model_e_11.h5\n",
      "Epoch 12/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.2051 - sparse_categorical_accuracy: 0.9140 - val_loss: 1.3116 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00012: saving model to tmp/MobileNetV2_beans_model_e_12.h5\n",
      "Epoch 13/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.1610 - sparse_categorical_accuracy: 0.9288 - val_loss: 1.3502 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00013: saving model to tmp/MobileNetV2_beans_model_e_13.h5\n",
      "Epoch 14/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.1562 - sparse_categorical_accuracy: 0.9410 - val_loss: 1.3890 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00014: saving model to tmp/MobileNetV2_beans_model_e_14.h5\n",
      "Epoch 15/300\n",
      "33/33 [==============================] - 33s 892ms/step - loss: 0.0913 - sparse_categorical_accuracy: 0.9694 - val_loss: 1.4444 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00015: saving model to tmp/MobileNetV2_beans_model_e_15.h5\n",
      "Epoch 16/300\n",
      "33/33 [==============================] - 33s 898ms/step - loss: 0.0819 - sparse_categorical_accuracy: 0.9699 - val_loss: 1.5417 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00016: saving model to tmp/MobileNetV2_beans_model_e_16.h5\n",
      "Epoch 17/300\n",
      "33/33 [==============================] - 33s 898ms/step - loss: 0.3076 - sparse_categorical_accuracy: 0.9010 - val_loss: 1.5353 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00017: saving model to tmp/MobileNetV2_beans_model_e_17.h5\n",
      "Epoch 18/300\n",
      "33/33 [==============================] - 33s 895ms/step - loss: 0.1484 - sparse_categorical_accuracy: 0.9505 - val_loss: 1.6127 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00018: saving model to tmp/MobileNetV2_beans_model_e_18.h5\n",
      "Epoch 19/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9785 - val_loss: 1.6301 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00019: saving model to tmp/MobileNetV2_beans_model_e_19.h5\n",
      "Epoch 20/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0731 - sparse_categorical_accuracy: 0.9760 - val_loss: 1.7643 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00020: saving model to tmp/MobileNetV2_beans_model_e_20.h5\n",
      "Epoch 21/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.1161 - sparse_categorical_accuracy: 0.9618 - val_loss: 1.8152 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00021: saving model to tmp/MobileNetV2_beans_model_e_21.h5\n",
      "Epoch 22/300\n",
      "33/33 [==============================] - 32s 891ms/step - loss: 0.0666 - sparse_categorical_accuracy: 0.9810 - val_loss: 2.0343 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00022: saving model to tmp/MobileNetV2_beans_model_e_22.h5\n",
      "Epoch 23/300\n",
      "33/33 [==============================] - 33s 898ms/step - loss: 0.0574 - sparse_categorical_accuracy: 0.9770 - val_loss: 2.0278 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00023: saving model to tmp/MobileNetV2_beans_model_e_23.h5\n",
      "Epoch 24/300\n",
      "33/33 [==============================] - 33s 908ms/step - loss: 0.0286 - sparse_categorical_accuracy: 0.9913 - val_loss: 2.1318 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00024: saving model to tmp/MobileNetV2_beans_model_e_24.h5\n",
      "Epoch 25/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.1026 - sparse_categorical_accuracy: 0.9564 - val_loss: 2.1116 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00025: saving model to tmp/MobileNetV2_beans_model_e_25.h5\n",
      "Epoch 26/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9724 - val_loss: 2.2796 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00026: saving model to tmp/MobileNetV2_beans_model_e_26.h5\n",
      "Epoch 27/300\n",
      "33/33 [==============================] - 33s 908ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9752 - val_loss: 2.2762 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00027: saving model to tmp/MobileNetV2_beans_model_e_27.h5\n",
      "Epoch 28/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0685 - sparse_categorical_accuracy: 0.9681 - val_loss: 2.3990 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00028: saving model to tmp/MobileNetV2_beans_model_e_28.h5\n",
      "Epoch 29/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9932 - val_loss: 2.3707 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00029: saving model to tmp/MobileNetV2_beans_model_e_29.h5\n",
      "Epoch 30/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0807 - sparse_categorical_accuracy: 0.9718 - val_loss: 2.2857 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00030: saving model to tmp/MobileNetV2_beans_model_e_30.h5\n",
      "Epoch 31/300\n",
      "33/33 [==============================] - 33s 897ms/step - loss: 0.0719 - sparse_categorical_accuracy: 0.9769 - val_loss: 2.3983 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00031: saving model to tmp/MobileNetV2_beans_model_e_31.h5\n",
      "Epoch 32/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0777 - sparse_categorical_accuracy: 0.9724 - val_loss: 2.2997 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00032: saving model to tmp/MobileNetV2_beans_model_e_32.h5\n",
      "Epoch 33/300\n",
      "33/33 [==============================] - 33s 914ms/step - loss: 0.0791 - sparse_categorical_accuracy: 0.9759 - val_loss: 2.4652 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00033: saving model to tmp/MobileNetV2_beans_model_e_33.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.0628 - sparse_categorical_accuracy: 0.9837 - val_loss: 2.4276 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00034: saving model to tmp/MobileNetV2_beans_model_e_34.h5\n",
      "Epoch 35/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9847 - val_loss: 2.6159 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00035: saving model to tmp/MobileNetV2_beans_model_e_35.h5\n",
      "Epoch 36/300\n",
      "33/33 [==============================] - 33s 923ms/step - loss: 0.0382 - sparse_categorical_accuracy: 0.9843 - val_loss: 2.6075 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00036: saving model to tmp/MobileNetV2_beans_model_e_36.h5\n",
      "Epoch 37/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.6848 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00037: saving model to tmp/MobileNetV2_beans_model_e_37.h5\n",
      "Epoch 38/300\n",
      "33/33 [==============================] - 33s 897ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9916 - val_loss: 2.7227 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00038: saving model to tmp/MobileNetV2_beans_model_e_38.h5\n",
      "Epoch 39/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9913 - val_loss: 2.7526 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00039: saving model to tmp/MobileNetV2_beans_model_e_39.h5\n",
      "Epoch 40/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9932 - val_loss: 2.9065 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00040: saving model to tmp/MobileNetV2_beans_model_e_40.h5\n",
      "Epoch 41/300\n",
      "33/33 [==============================] - 33s 899ms/step - loss: 0.0458 - sparse_categorical_accuracy: 0.9841 - val_loss: 2.9312 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00041: saving model to tmp/MobileNetV2_beans_model_e_41.h5\n",
      "Epoch 42/300\n",
      "33/33 [==============================] - 33s 901ms/step - loss: 0.0598 - sparse_categorical_accuracy: 0.9740 - val_loss: 3.0970 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00042: saving model to tmp/MobileNetV2_beans_model_e_42.h5\n",
      "Epoch 43/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0453 - sparse_categorical_accuracy: 0.9803 - val_loss: 3.1286 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00043: saving model to tmp/MobileNetV2_beans_model_e_43.h5\n",
      "Epoch 44/300\n",
      "33/33 [==============================] - 32s 891ms/step - loss: 0.0496 - sparse_categorical_accuracy: 0.9817 - val_loss: 3.0185 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00044: saving model to tmp/MobileNetV2_beans_model_e_44.h5\n",
      "Epoch 45/300\n",
      "33/33 [==============================] - 33s 908ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9856 - val_loss: 3.1470 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00045: saving model to tmp/MobileNetV2_beans_model_e_45.h5\n",
      "Epoch 46/300\n",
      "33/33 [==============================] - 32s 896ms/step - loss: 0.0829 - sparse_categorical_accuracy: 0.9740 - val_loss: 3.3641 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00046: saving model to tmp/MobileNetV2_beans_model_e_46.h5\n",
      "Epoch 47/300\n",
      "33/33 [==============================] - 32s 886ms/step - loss: 0.0461 - sparse_categorical_accuracy: 0.9867 - val_loss: 3.2139 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00047: saving model to tmp/MobileNetV2_beans_model_e_47.h5\n",
      "Epoch 48/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9833 - val_loss: 3.3091 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00048: saving model to tmp/MobileNetV2_beans_model_e_48.h5\n",
      "Epoch 49/300\n",
      "33/33 [==============================] - 33s 901ms/step - loss: 0.0389 - sparse_categorical_accuracy: 0.9839 - val_loss: 3.1989 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00049: saving model to tmp/MobileNetV2_beans_model_e_49.h5\n",
      "Epoch 50/300\n",
      "33/33 [==============================] - 32s 889ms/step - loss: 0.0716 - sparse_categorical_accuracy: 0.9742 - val_loss: 3.3891 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00050: saving model to tmp/MobileNetV2_beans_model_e_50.h5\n",
      "Epoch 51/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9890 - val_loss: 3.2537 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00051: saving model to tmp/MobileNetV2_beans_model_e_51.h5\n",
      "Epoch 52/300\n",
      "33/33 [==============================] - 33s 924ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.9797 - val_loss: 2.9671 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00052: saving model to tmp/MobileNetV2_beans_model_e_52.h5\n",
      "Epoch 53/300\n",
      "33/33 [==============================] - 33s 903ms/step - loss: 0.1006 - sparse_categorical_accuracy: 0.9738 - val_loss: 3.2558 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00053: saving model to tmp/MobileNetV2_beans_model_e_53.h5\n",
      "Epoch 54/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0442 - sparse_categorical_accuracy: 0.9856 - val_loss: 3.1869 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00054: saving model to tmp/MobileNetV2_beans_model_e_54.h5\n",
      "Epoch 55/300\n",
      "33/33 [==============================] - 33s 895ms/step - loss: 0.0550 - sparse_categorical_accuracy: 0.9868 - val_loss: 2.9456 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00055: saving model to tmp/MobileNetV2_beans_model_e_55.h5\n",
      "Epoch 56/300\n",
      "33/33 [==============================] - 33s 900ms/step - loss: 0.0540 - sparse_categorical_accuracy: 0.9863 - val_loss: 3.4752 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00056: saving model to tmp/MobileNetV2_beans_model_e_56.h5\n",
      "Epoch 57/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9927 - val_loss: 3.1094 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00057: saving model to tmp/MobileNetV2_beans_model_e_57.h5\n",
      "Epoch 58/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9887 - val_loss: 3.2019 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00058: saving model to tmp/MobileNetV2_beans_model_e_58.h5\n",
      "Epoch 59/300\n",
      "33/33 [==============================] - 33s 903ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9916 - val_loss: 3.1228 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00059: saving model to tmp/MobileNetV2_beans_model_e_59.h5\n",
      "Epoch 60/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9944 - val_loss: 2.9408 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00060: saving model to tmp/MobileNetV2_beans_model_e_60.h5\n",
      "Epoch 61/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9949 - val_loss: 3.1170 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00061: saving model to tmp/MobileNetV2_beans_model_e_61.h5\n",
      "Epoch 62/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9990 - val_loss: 3.0382 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00062: saving model to tmp/MobileNetV2_beans_model_e_62.h5\n",
      "Epoch 63/300\n",
      "33/33 [==============================] - 33s 919ms/step - loss: 0.0098 - sparse_categorical_accuracy: 0.9961 - val_loss: 3.1393 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00063: saving model to tmp/MobileNetV2_beans_model_e_63.h5\n",
      "Epoch 64/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.1655 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00064: saving model to tmp/MobileNetV2_beans_model_e_64.h5\n",
      "Epoch 65/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9991 - val_loss: 3.1023 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00065: saving model to tmp/MobileNetV2_beans_model_e_65.h5\n",
      "Epoch 66/300\n",
      "33/33 [==============================] - 32s 896ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9950 - val_loss: 3.2445 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00066: saving model to tmp/MobileNetV2_beans_model_e_66.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0606 - sparse_categorical_accuracy: 0.9877 - val_loss: 3.0392 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00067: saving model to tmp/MobileNetV2_beans_model_e_67.h5\n",
      "Epoch 68/300\n",
      "33/33 [==============================] - 33s 899ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9948 - val_loss: 2.9968 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00068: saving model to tmp/MobileNetV2_beans_model_e_68.h5\n",
      "Epoch 69/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9972 - val_loss: 3.0462 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00069: saving model to tmp/MobileNetV2_beans_model_e_69.h5\n",
      "Epoch 70/300\n",
      "33/33 [==============================] - 32s 905ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9906 - val_loss: 2.8533 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00070: saving model to tmp/MobileNetV2_beans_model_e_70.h5\n",
      "Epoch 71/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9954 - val_loss: 2.8983 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00071: saving model to tmp/MobileNetV2_beans_model_e_71.h5\n",
      "Epoch 72/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9948 - val_loss: 2.9778 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00072: saving model to tmp/MobileNetV2_beans_model_e_72.h5\n",
      "Epoch 73/300\n",
      "33/33 [==============================] - 33s 914ms/step - loss: 0.0962 - sparse_categorical_accuracy: 0.9717 - val_loss: 3.0333 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00073: saving model to tmp/MobileNetV2_beans_model_e_73.h5\n",
      "Epoch 74/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0731 - sparse_categorical_accuracy: 0.9745 - val_loss: 2.6533 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00074: saving model to tmp/MobileNetV2_beans_model_e_74.h5\n",
      "Epoch 75/300\n",
      "33/33 [==============================] - 32s 901ms/step - loss: 0.0730 - sparse_categorical_accuracy: 0.9688 - val_loss: 2.6010 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00075: saving model to tmp/MobileNetV2_beans_model_e_75.h5\n",
      "Epoch 76/300\n",
      "33/33 [==============================] - 33s 916ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.9856 - val_loss: 3.0496 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00076: saving model to tmp/MobileNetV2_beans_model_e_76.h5\n",
      "Epoch 77/300\n",
      "33/33 [==============================] - 33s 926ms/step - loss: 0.1433 - sparse_categorical_accuracy: 0.9465 - val_loss: 2.9965 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00077: saving model to tmp/MobileNetV2_beans_model_e_77.h5\n",
      "Epoch 78/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9827 - val_loss: 2.9814 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00078: saving model to tmp/MobileNetV2_beans_model_e_78.h5\n",
      "Epoch 79/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0756 - sparse_categorical_accuracy: 0.9669 - val_loss: 2.8056 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00079: saving model to tmp/MobileNetV2_beans_model_e_79.h5\n",
      "Epoch 80/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9801 - val_loss: 2.7624 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00080: saving model to tmp/MobileNetV2_beans_model_e_80.h5\n",
      "Epoch 81/300\n",
      "33/33 [==============================] - 32s 882ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9912 - val_loss: 2.9095 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00081: saving model to tmp/MobileNetV2_beans_model_e_81.h5\n",
      "Epoch 82/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9884 - val_loss: 2.8128 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00082: saving model to tmp/MobileNetV2_beans_model_e_82.h5\n",
      "Epoch 83/300\n",
      "33/33 [==============================] - 32s 881ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9913 - val_loss: 2.7993 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00083: saving model to tmp/MobileNetV2_beans_model_e_83.h5\n",
      "Epoch 84/300\n",
      "33/33 [==============================] - 33s 909ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9880 - val_loss: 2.7389 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00084: saving model to tmp/MobileNetV2_beans_model_e_84.h5\n",
      "Epoch 85/300\n",
      "33/33 [==============================] - 32s 898ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9887 - val_loss: 2.8444 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00085: saving model to tmp/MobileNetV2_beans_model_e_85.h5\n",
      "Epoch 86/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0225 - sparse_categorical_accuracy: 0.9898 - val_loss: 2.8489 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00086: saving model to tmp/MobileNetV2_beans_model_e_86.h5\n",
      "Epoch 87/300\n",
      "33/33 [==============================] - 33s 910ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9928 - val_loss: 2.8065 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00087: saving model to tmp/MobileNetV2_beans_model_e_87.h5\n",
      "Epoch 88/300\n",
      "33/33 [==============================] - 33s 919ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9990 - val_loss: 2.6690 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00088: saving model to tmp/MobileNetV2_beans_model_e_88.h5\n",
      "Epoch 89/300\n",
      "33/33 [==============================] - 33s 896ms/step - loss: 0.0047 - sparse_categorical_accuracy: 0.9997 - val_loss: 2.7360 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00089: saving model to tmp/MobileNetV2_beans_model_e_89.h5\n",
      "Epoch 90/300\n",
      "33/33 [==============================] - 33s 910ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9966 - val_loss: 2.7778 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00090: saving model to tmp/MobileNetV2_beans_model_e_90.h5\n",
      "Epoch 91/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.9852 - val_loss: 2.8815 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00091: saving model to tmp/MobileNetV2_beans_model_e_91.h5\n",
      "Epoch 92/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0402 - sparse_categorical_accuracy: 0.9897 - val_loss: 2.8523 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00092: saving model to tmp/MobileNetV2_beans_model_e_92.h5\n",
      "Epoch 93/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0709 - sparse_categorical_accuracy: 0.9765 - val_loss: 2.7930 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00093: saving model to tmp/MobileNetV2_beans_model_e_93.h5\n",
      "Epoch 94/300\n",
      "33/33 [==============================] - 32s 881ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9706 - val_loss: 2.6783 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00094: saving model to tmp/MobileNetV2_beans_model_e_94.h5\n",
      "Epoch 95/300\n",
      "33/33 [==============================] - 32s 881ms/step - loss: 0.0243 - sparse_categorical_accuracy: 0.9904 - val_loss: 2.5414 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00095: saving model to tmp/MobileNetV2_beans_model_e_95.h5\n",
      "Epoch 96/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9865 - val_loss: 2.6023 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00096: saving model to tmp/MobileNetV2_beans_model_e_96.h5\n",
      "Epoch 97/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9913 - val_loss: 2.5925 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00097: saving model to tmp/MobileNetV2_beans_model_e_97.h5\n",
      "Epoch 98/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9989 - val_loss: 2.6558 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00098: saving model to tmp/MobileNetV2_beans_model_e_98.h5\n",
      "Epoch 99/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9992 - val_loss: 2.6465 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00099: saving model to tmp/MobileNetV2_beans_model_e_99.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300\n",
      "33/33 [==============================] - 33s 907ms/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9993 - val_loss: 2.5970 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00100: saving model to tmp/MobileNetV2_beans_model_e_100.h5\n",
      "Epoch 101/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.5848 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00101: saving model to tmp/MobileNetV2_beans_model_e_101.h5\n",
      "Epoch 102/300\n",
      "33/33 [==============================] - 32s 889ms/step - loss: 0.0050 - sparse_categorical_accuracy: 0.9983 - val_loss: 2.5315 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00102: saving model to tmp/MobileNetV2_beans_model_e_102.h5\n",
      "Epoch 103/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0039 - sparse_categorical_accuracy: 0.9998 - val_loss: 2.5037 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00103: saving model to tmp/MobileNetV2_beans_model_e_103.h5\n",
      "Epoch 104/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9685 - val_loss: 2.7221 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00104: saving model to tmp/MobileNetV2_beans_model_e_104.h5\n",
      "Epoch 105/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9910 - val_loss: 2.7973 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00105: saving model to tmp/MobileNetV2_beans_model_e_105.h5\n",
      "Epoch 106/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9871 - val_loss: 2.8954 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00106: saving model to tmp/MobileNetV2_beans_model_e_106.h5\n",
      "Epoch 107/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9956 - val_loss: 2.9835 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00107: saving model to tmp/MobileNetV2_beans_model_e_107.h5\n",
      "Epoch 108/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9966 - val_loss: 2.9646 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00108: saving model to tmp/MobileNetV2_beans_model_e_108.h5\n",
      "Epoch 109/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9941 - val_loss: 2.8225 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00109: saving model to tmp/MobileNetV2_beans_model_e_109.h5\n",
      "Epoch 110/300\n",
      "33/33 [==============================] - 33s 903ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9914 - val_loss: 2.9933 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00110: saving model to tmp/MobileNetV2_beans_model_e_110.h5\n",
      "Epoch 111/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9966 - val_loss: 3.0294 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00111: saving model to tmp/MobileNetV2_beans_model_e_111.h5\n",
      "Epoch 112/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9915 - val_loss: 3.0453 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00112: saving model to tmp/MobileNetV2_beans_model_e_112.h5\n",
      "Epoch 113/300\n",
      "33/33 [==============================] - 33s 911ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9896 - val_loss: 2.8955 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00113: saving model to tmp/MobileNetV2_beans_model_e_113.h5\n",
      "Epoch 114/300\n",
      "33/33 [==============================] - 33s 909ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9951 - val_loss: 2.9068 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00114: saving model to tmp/MobileNetV2_beans_model_e_114.h5\n",
      "Epoch 115/300\n",
      "33/33 [==============================] - 32s 905ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9757 - val_loss: 3.0137 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00115: saving model to tmp/MobileNetV2_beans_model_e_115.h5\n",
      "Epoch 116/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0887 - sparse_categorical_accuracy: 0.9732 - val_loss: 3.4696 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00116: saving model to tmp/MobileNetV2_beans_model_e_116.h5\n",
      "Epoch 117/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9629 - val_loss: 2.9944 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00117: saving model to tmp/MobileNetV2_beans_model_e_117.h5\n",
      "Epoch 118/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9894 - val_loss: 3.1296 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00118: saving model to tmp/MobileNetV2_beans_model_e_118.h5\n",
      "Epoch 119/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9882 - val_loss: 3.2667 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00119: saving model to tmp/MobileNetV2_beans_model_e_119.h5\n",
      "Epoch 120/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9919 - val_loss: 3.3291 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00120: saving model to tmp/MobileNetV2_beans_model_e_120.h5\n",
      "Epoch 121/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0276 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.1612 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00121: saving model to tmp/MobileNetV2_beans_model_e_121.h5\n",
      "Epoch 122/300\n",
      "33/33 [==============================] - 33s 915ms/step - loss: 0.0462 - sparse_categorical_accuracy: 0.9816 - val_loss: 3.3903 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00122: saving model to tmp/MobileNetV2_beans_model_e_122.h5\n",
      "Epoch 123/300\n",
      "33/33 [==============================] - 33s 897ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9962 - val_loss: 3.4757 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00123: saving model to tmp/MobileNetV2_beans_model_e_123.h5\n",
      "Epoch 124/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9935 - val_loss: 3.5495 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00124: saving model to tmp/MobileNetV2_beans_model_e_124.h5\n",
      "Epoch 125/300\n",
      "33/33 [==============================] - 32s 901ms/step - loss: 0.0253 - sparse_categorical_accuracy: 0.9926 - val_loss: 3.2210 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00125: saving model to tmp/MobileNetV2_beans_model_e_125.h5\n",
      "Epoch 126/300\n",
      "33/33 [==============================] - 32s 882ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9893 - val_loss: 3.0903 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00126: saving model to tmp/MobileNetV2_beans_model_e_126.h5\n",
      "Epoch 127/300\n",
      "33/33 [==============================] - 32s 883ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9962 - val_loss: 3.3124 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00127: saving model to tmp/MobileNetV2_beans_model_e_127.h5\n",
      "Epoch 128/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9892 - val_loss: 3.5355 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00128: saving model to tmp/MobileNetV2_beans_model_e_128.h5\n",
      "Epoch 129/300\n",
      "33/33 [==============================] - 32s 902ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9980 - val_loss: 3.3677 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00129: saving model to tmp/MobileNetV2_beans_model_e_129.h5\n",
      "Epoch 130/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9966 - val_loss: 3.0942 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00130: saving model to tmp/MobileNetV2_beans_model_e_130.h5\n",
      "Epoch 131/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0405 - sparse_categorical_accuracy: 0.9871 - val_loss: 3.3114 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00131: saving model to tmp/MobileNetV2_beans_model_e_131.h5\n",
      "Epoch 132/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9839 - val_loss: 3.4989 - val_sparse_categorical_accuracy: 0.3308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: saving model to tmp/MobileNetV2_beans_model_e_132.h5\n",
      "Epoch 133/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9927 - val_loss: 3.4513 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00133: saving model to tmp/MobileNetV2_beans_model_e_133.h5\n",
      "Epoch 134/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.5356 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00134: saving model to tmp/MobileNetV2_beans_model_e_134.h5\n",
      "Epoch 135/300\n",
      "33/33 [==============================] - 32s 884ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9990 - val_loss: 3.4644 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00135: saving model to tmp/MobileNetV2_beans_model_e_135.h5\n",
      "Epoch 136/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9946 - val_loss: 3.3916 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00136: saving model to tmp/MobileNetV2_beans_model_e_136.h5\n",
      "Epoch 137/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9975 - val_loss: 3.5429 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00137: saving model to tmp/MobileNetV2_beans_model_e_137.h5\n",
      "Epoch 138/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0076 - sparse_categorical_accuracy: 0.9993 - val_loss: 3.7554 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00138: saving model to tmp/MobileNetV2_beans_model_e_138.h5\n",
      "Epoch 139/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9961 - val_loss: 3.4592 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00139: saving model to tmp/MobileNetV2_beans_model_e_139.h5\n",
      "Epoch 140/300\n",
      "33/33 [==============================] - 32s 878ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9932 - val_loss: 3.4153 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00140: saving model to tmp/MobileNetV2_beans_model_e_140.h5\n",
      "Epoch 141/300\n",
      "33/33 [==============================] - 32s 909ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9908 - val_loss: 3.5843 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00141: saving model to tmp/MobileNetV2_beans_model_e_141.h5\n",
      "Epoch 142/300\n",
      "33/33 [==============================] - 33s 914ms/step - loss: 0.0046 - sparse_categorical_accuracy: 0.9994 - val_loss: 3.5153 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00142: saving model to tmp/MobileNetV2_beans_model_e_142.h5\n",
      "Epoch 143/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9956 - val_loss: 3.5261 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00143: saving model to tmp/MobileNetV2_beans_model_e_143.h5\n",
      "Epoch 144/300\n",
      "33/33 [==============================] - 32s 889ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9975 - val_loss: 3.4732 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00144: saving model to tmp/MobileNetV2_beans_model_e_144.h5\n",
      "Epoch 145/300\n",
      "33/33 [==============================] - 32s 889ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9980 - val_loss: 3.6732 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00145: saving model to tmp/MobileNetV2_beans_model_e_145.h5\n",
      "Epoch 146/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9999 - val_loss: 3.7218 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00146: saving model to tmp/MobileNetV2_beans_model_e_146.h5\n",
      "Epoch 147/300\n",
      "33/33 [==============================] - 33s 912ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9985 - val_loss: 3.5227 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00147: saving model to tmp/MobileNetV2_beans_model_e_147.h5\n",
      "Epoch 148/300\n",
      "33/33 [==============================] - 33s 915ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9968 - val_loss: 3.3950 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00148: saving model to tmp/MobileNetV2_beans_model_e_148.h5\n",
      "Epoch 149/300\n",
      "33/33 [==============================] - 33s 913ms/step - loss: 0.0277 - sparse_categorical_accuracy: 0.9903 - val_loss: 3.3390 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00149: saving model to tmp/MobileNetV2_beans_model_e_149.h5\n",
      "Epoch 150/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9934 - val_loss: 3.5362 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00150: saving model to tmp/MobileNetV2_beans_model_e_150.h5\n",
      "Epoch 151/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.9851 - val_loss: 3.3103 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00151: saving model to tmp/MobileNetV2_beans_model_e_151.h5\n",
      "Epoch 152/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9901 - val_loss: 3.4852 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00152: saving model to tmp/MobileNetV2_beans_model_e_152.h5\n",
      "Epoch 153/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0456 - sparse_categorical_accuracy: 0.9795 - val_loss: 3.7472 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00153: saving model to tmp/MobileNetV2_beans_model_e_153.h5\n",
      "Epoch 154/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0649 - sparse_categorical_accuracy: 0.9757 - val_loss: 3.5337 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00154: saving model to tmp/MobileNetV2_beans_model_e_154.h5\n",
      "Epoch 155/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0600 - sparse_categorical_accuracy: 0.9780 - val_loss: 3.8920 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00155: saving model to tmp/MobileNetV2_beans_model_e_155.h5\n",
      "Epoch 156/300\n",
      "33/33 [==============================] - 32s 882ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9947 - val_loss: 3.9479 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00156: saving model to tmp/MobileNetV2_beans_model_e_156.h5\n",
      "Epoch 157/300\n",
      "33/33 [==============================] - 33s 914ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9906 - val_loss: 3.5769 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00157: saving model to tmp/MobileNetV2_beans_model_e_157.h5\n",
      "Epoch 158/300\n",
      "33/33 [==============================] - 32s 909ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9979 - val_loss: 3.7214 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00158: saving model to tmp/MobileNetV2_beans_model_e_158.h5\n",
      "Epoch 159/300\n",
      "33/33 [==============================] - 32s 903ms/step - loss: 0.0574 - sparse_categorical_accuracy: 0.9893 - val_loss: 3.8824 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00159: saving model to tmp/MobileNetV2_beans_model_e_159.h5\n",
      "Epoch 160/300\n",
      "33/33 [==============================] - 32s 879ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9979 - val_loss: 4.0324 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00160: saving model to tmp/MobileNetV2_beans_model_e_160.h5\n",
      "Epoch 161/300\n",
      "33/33 [==============================] - 33s 913ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9978 - val_loss: 3.5935 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00161: saving model to tmp/MobileNetV2_beans_model_e_161.h5\n",
      "Epoch 162/300\n",
      "33/33 [==============================] - 33s 909ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9974 - val_loss: 3.2317 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00162: saving model to tmp/MobileNetV2_beans_model_e_162.h5\n",
      "Epoch 163/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9954 - val_loss: 3.4797 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00163: saving model to tmp/MobileNetV2_beans_model_e_163.h5\n",
      "Epoch 164/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9976 - val_loss: 3.3560 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00164: saving model to tmp/MobileNetV2_beans_model_e_164.h5\n",
      "Epoch 165/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.5119 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00165: saving model to tmp/MobileNetV2_beans_model_e_165.h5\n",
      "Epoch 166/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.5845 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00166: saving model to tmp/MobileNetV2_beans_model_e_166.h5\n",
      "Epoch 167/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9998 - val_loss: 3.4601 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00167: saving model to tmp/MobileNetV2_beans_model_e_167.h5\n",
      "Epoch 168/300\n",
      "33/33 [==============================] - 32s 896ms/step - loss: 0.0292 - sparse_categorical_accuracy: 0.9876 - val_loss: 4.1157 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00168: saving model to tmp/MobileNetV2_beans_model_e_168.h5\n",
      "Epoch 169/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9791 - val_loss: 4.0789 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00169: saving model to tmp/MobileNetV2_beans_model_e_169.h5\n",
      "Epoch 170/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9941 - val_loss: 3.6528 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00170: saving model to tmp/MobileNetV2_beans_model_e_170.h5\n",
      "Epoch 171/300\n",
      "33/33 [==============================] - 32s 905ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9923 - val_loss: 3.6479 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00171: saving model to tmp/MobileNetV2_beans_model_e_171.h5\n",
      "Epoch 172/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9937 - val_loss: 3.2903 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00172: saving model to tmp/MobileNetV2_beans_model_e_172.h5\n",
      "Epoch 173/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9973 - val_loss: 3.4639 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00173: saving model to tmp/MobileNetV2_beans_model_e_173.h5\n",
      "Epoch 174/300\n",
      "33/33 [==============================] - 32s 884ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9949 - val_loss: 3.5386 - val_sparse_categorical_accuracy: 0.3684\n",
      "\n",
      "Epoch 00174: saving model to tmp/MobileNetV2_beans_model_e_174.h5\n",
      "Epoch 175/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.0433 - sparse_categorical_accuracy: 0.9797 - val_loss: 4.2063 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00175: saving model to tmp/MobileNetV2_beans_model_e_175.h5\n",
      "Epoch 176/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9846 - val_loss: 3.4478 - val_sparse_categorical_accuracy: 0.3534\n",
      "\n",
      "Epoch 00176: saving model to tmp/MobileNetV2_beans_model_e_176.h5\n",
      "Epoch 177/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9920 - val_loss: 4.2995 - val_sparse_categorical_accuracy: 0.3158\n",
      "\n",
      "Epoch 00177: saving model to tmp/MobileNetV2_beans_model_e_177.h5\n",
      "Epoch 178/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9856 - val_loss: 3.8566 - val_sparse_categorical_accuracy: 0.4060\n",
      "\n",
      "Epoch 00178: saving model to tmp/MobileNetV2_beans_model_e_178.h5\n",
      "Epoch 179/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9920 - val_loss: 2.9401 - val_sparse_categorical_accuracy: 0.3835\n",
      "\n",
      "Epoch 00179: saving model to tmp/MobileNetV2_beans_model_e_179.h5\n",
      "Epoch 180/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0605 - sparse_categorical_accuracy: 0.9854 - val_loss: 2.5708 - val_sparse_categorical_accuracy: 0.3835\n",
      "\n",
      "Epoch 00180: saving model to tmp/MobileNetV2_beans_model_e_180.h5\n",
      "Epoch 181/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9911 - val_loss: 3.2655 - val_sparse_categorical_accuracy: 0.3609\n",
      "\n",
      "Epoch 00181: saving model to tmp/MobileNetV2_beans_model_e_181.h5\n",
      "Epoch 182/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9932 - val_loss: 2.8031 - val_sparse_categorical_accuracy: 0.3759\n",
      "\n",
      "Epoch 00182: saving model to tmp/MobileNetV2_beans_model_e_182.h5\n",
      "Epoch 183/300\n",
      "33/33 [==============================] - 32s 908ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9949 - val_loss: 3.4748 - val_sparse_categorical_accuracy: 0.4135\n",
      "\n",
      "Epoch 00183: saving model to tmp/MobileNetV2_beans_model_e_183.h5\n",
      "Epoch 184/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9970 - val_loss: 3.6812 - val_sparse_categorical_accuracy: 0.3985\n",
      "\n",
      "Epoch 00184: saving model to tmp/MobileNetV2_beans_model_e_184.h5\n",
      "Epoch 185/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9977 - val_loss: 3.8724 - val_sparse_categorical_accuracy: 0.4586\n",
      "\n",
      "Epoch 00185: saving model to tmp/MobileNetV2_beans_model_e_185.h5\n",
      "Epoch 186/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0047 - sparse_categorical_accuracy: 0.9992 - val_loss: 3.7355 - val_sparse_categorical_accuracy: 0.4511\n",
      "\n",
      "Epoch 00186: saving model to tmp/MobileNetV2_beans_model_e_186.h5\n",
      "Epoch 187/300\n",
      "33/33 [==============================] - 33s 922ms/step - loss: 0.0044 - sparse_categorical_accuracy: 1.0000 - val_loss: 3.8589 - val_sparse_categorical_accuracy: 0.4662\n",
      "\n",
      "Epoch 00187: saving model to tmp/MobileNetV2_beans_model_e_187.h5\n",
      "Epoch 188/300\n",
      "33/33 [==============================] - 33s 915ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9975 - val_loss: 3.5107 - val_sparse_categorical_accuracy: 0.4586\n",
      "\n",
      "Epoch 00188: saving model to tmp/MobileNetV2_beans_model_e_188.h5\n",
      "Epoch 189/300\n",
      "33/33 [==============================] - 33s 903ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9998 - val_loss: 3.2818 - val_sparse_categorical_accuracy: 0.4586\n",
      "\n",
      "Epoch 00189: saving model to tmp/MobileNetV2_beans_model_e_189.h5\n",
      "Epoch 190/300\n",
      "33/33 [==============================] - 33s 922ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9940 - val_loss: 2.5394 - val_sparse_categorical_accuracy: 0.5414\n",
      "\n",
      "Epoch 00190: saving model to tmp/MobileNetV2_beans_model_e_190.h5\n",
      "Epoch 191/300\n",
      "33/33 [==============================] - 32s 891ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9919 - val_loss: 3.3947 - val_sparse_categorical_accuracy: 0.5113\n",
      "\n",
      "Epoch 00191: saving model to tmp/MobileNetV2_beans_model_e_191.h5\n",
      "Epoch 192/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9915 - val_loss: 2.2869 - val_sparse_categorical_accuracy: 0.6165\n",
      "\n",
      "Epoch 00192: saving model to tmp/MobileNetV2_beans_model_e_192.h5\n",
      "Epoch 193/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9970 - val_loss: 2.2630 - val_sparse_categorical_accuracy: 0.6015\n",
      "\n",
      "Epoch 00193: saving model to tmp/MobileNetV2_beans_model_e_193.h5\n",
      "Epoch 194/300\n",
      "33/33 [==============================] - 32s 903ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9941 - val_loss: 3.0633 - val_sparse_categorical_accuracy: 0.5188\n",
      "\n",
      "Epoch 00194: saving model to tmp/MobileNetV2_beans_model_e_194.h5\n",
      "Epoch 195/300\n",
      "33/33 [==============================] - 33s 908ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9908 - val_loss: 1.3595 - val_sparse_categorical_accuracy: 0.6541\n",
      "\n",
      "Epoch 00195: saving model to tmp/MobileNetV2_beans_model_e_195.h5\n",
      "Epoch 196/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0635 - sparse_categorical_accuracy: 0.9812 - val_loss: 2.5289 - val_sparse_categorical_accuracy: 0.6391\n",
      "\n",
      "Epoch 00196: saving model to tmp/MobileNetV2_beans_model_e_196.h5\n",
      "Epoch 197/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0483 - sparse_categorical_accuracy: 0.9819 - val_loss: 2.1485 - val_sparse_categorical_accuracy: 0.6992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00197: saving model to tmp/MobileNetV2_beans_model_e_197.h5\n",
      "Epoch 198/300\n",
      "33/33 [==============================] - 33s 913ms/step - loss: 0.0571 - sparse_categorical_accuracy: 0.9776 - val_loss: 3.7284 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00198: saving model to tmp/MobileNetV2_beans_model_e_198.h5\n",
      "Epoch 199/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0574 - sparse_categorical_accuracy: 0.9849 - val_loss: 3.4972 - val_sparse_categorical_accuracy: 0.6617\n",
      "\n",
      "Epoch 00199: saving model to tmp/MobileNetV2_beans_model_e_199.h5\n",
      "Epoch 200/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9838 - val_loss: 2.4087 - val_sparse_categorical_accuracy: 0.7068\n",
      "\n",
      "Epoch 00200: saving model to tmp/MobileNetV2_beans_model_e_200.h5\n",
      "Epoch 201/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9942 - val_loss: 3.1148 - val_sparse_categorical_accuracy: 0.6316\n",
      "\n",
      "Epoch 00201: saving model to tmp/MobileNetV2_beans_model_e_201.h5\n",
      "Epoch 202/300\n",
      "33/33 [==============================] - 32s 896ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9938 - val_loss: 3.1042 - val_sparse_categorical_accuracy: 0.6391\n",
      "\n",
      "Epoch 00202: saving model to tmp/MobileNetV2_beans_model_e_202.h5\n",
      "Epoch 203/300\n",
      "33/33 [==============================] - 32s 896ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9968 - val_loss: 2.1671 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00203: saving model to tmp/MobileNetV2_beans_model_e_203.h5\n",
      "Epoch 204/300\n",
      "33/33 [==============================] - 32s 905ms/step - loss: 0.0038 - sparse_categorical_accuracy: 0.9987 - val_loss: 1.5449 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00204: saving model to tmp/MobileNetV2_beans_model_e_204.h5\n",
      "Epoch 205/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9991 - val_loss: 1.3745 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00205: saving model to tmp/MobileNetV2_beans_model_e_205.h5\n",
      "Epoch 206/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9954 - val_loss: 1.5992 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00206: saving model to tmp/MobileNetV2_beans_model_e_206.h5\n",
      "Epoch 207/300\n",
      "33/33 [==============================] - 32s 883ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9997 - val_loss: 1.4684 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00207: saving model to tmp/MobileNetV2_beans_model_e_207.h5\n",
      "Epoch 208/300\n",
      "33/33 [==============================] - 33s 907ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9976 - val_loss: 1.4903 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00208: saving model to tmp/MobileNetV2_beans_model_e_208.h5\n",
      "Epoch 209/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9996 - val_loss: 1.6882 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00209: saving model to tmp/MobileNetV2_beans_model_e_209.h5\n",
      "Epoch 210/300\n",
      "33/33 [==============================] - 32s 904ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.0281 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00210: saving model to tmp/MobileNetV2_beans_model_e_210.h5\n",
      "Epoch 211/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9999 - val_loss: 1.7162 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00211: saving model to tmp/MobileNetV2_beans_model_e_211.h5\n",
      "Epoch 212/300\n",
      "33/33 [==============================] - 32s 904ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9965 - val_loss: 2.4588 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00212: saving model to tmp/MobileNetV2_beans_model_e_212.h5\n",
      "Epoch 213/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9943 - val_loss: 2.9406 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00213: saving model to tmp/MobileNetV2_beans_model_e_213.h5\n",
      "Epoch 214/300\n",
      "33/33 [==============================] - 33s 912ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.7127 - val_sparse_categorical_accuracy: 0.7068\n",
      "\n",
      "Epoch 00214: saving model to tmp/MobileNetV2_beans_model_e_214.h5\n",
      "Epoch 215/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9956 - val_loss: 2.1017 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00215: saving model to tmp/MobileNetV2_beans_model_e_215.h5\n",
      "Epoch 216/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.2742 - val_sparse_categorical_accuracy: 0.6767\n",
      "\n",
      "Epoch 00216: saving model to tmp/MobileNetV2_beans_model_e_216.h5\n",
      "Epoch 217/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9988 - val_loss: 3.2923 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00217: saving model to tmp/MobileNetV2_beans_model_e_217.h5\n",
      "Epoch 218/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0455 - sparse_categorical_accuracy: 0.9812 - val_loss: 9.3721 - val_sparse_categorical_accuracy: 0.4511\n",
      "\n",
      "Epoch 00218: saving model to tmp/MobileNetV2_beans_model_e_218.h5\n",
      "Epoch 219/300\n",
      "33/33 [==============================] - 32s 905ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9889 - val_loss: 6.0278 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00219: saving model to tmp/MobileNetV2_beans_model_e_219.h5\n",
      "Epoch 220/300\n",
      "33/33 [==============================] - 32s 902ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9832 - val_loss: 5.1454 - val_sparse_categorical_accuracy: 0.6617\n",
      "\n",
      "Epoch 00220: saving model to tmp/MobileNetV2_beans_model_e_220.h5\n",
      "Epoch 221/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9967 - val_loss: 2.4746 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00221: saving model to tmp/MobileNetV2_beans_model_e_221.h5\n",
      "Epoch 222/300\n",
      "33/33 [==============================] - 32s 902ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9998 - val_loss: 2.5767 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00222: saving model to tmp/MobileNetV2_beans_model_e_222.h5\n",
      "Epoch 223/300\n",
      "33/33 [==============================] - 33s 911ms/step - loss: 0.0791 - sparse_categorical_accuracy: 0.9778 - val_loss: 3.4589 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00223: saving model to tmp/MobileNetV2_beans_model_e_223.h5\n",
      "Epoch 224/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0708 - sparse_categorical_accuracy: 0.9728 - val_loss: 3.9111 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00224: saving model to tmp/MobileNetV2_beans_model_e_224.h5\n",
      "Epoch 225/300\n",
      "33/33 [==============================] - 33s 904ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9922 - val_loss: 3.1003 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00225: saving model to tmp/MobileNetV2_beans_model_e_225.h5\n",
      "Epoch 226/300\n",
      "33/33 [==============================] - 32s 880ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9964 - val_loss: 3.2761 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00226: saving model to tmp/MobileNetV2_beans_model_e_226.h5\n",
      "Epoch 227/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9985 - val_loss: 4.2905 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00227: saving model to tmp/MobileNetV2_beans_model_e_227.h5\n",
      "Epoch 228/300\n",
      "33/33 [==============================] - 32s 891ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9996 - val_loss: 2.5965 - val_sparse_categorical_accuracy: 0.7068\n",
      "\n",
      "Epoch 00228: saving model to tmp/MobileNetV2_beans_model_e_228.h5\n",
      "Epoch 229/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0046 - sparse_categorical_accuracy: 0.9994 - val_loss: 2.8864 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00229: saving model to tmp/MobileNetV2_beans_model_e_229.h5\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9984 - val_loss: 1.7926 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00230: saving model to tmp/MobileNetV2_beans_model_e_230.h5\n",
      "Epoch 231/300\n",
      "33/33 [==============================] - 32s 908ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9977 - val_loss: 5.7187 - val_sparse_categorical_accuracy: 0.6165\n",
      "\n",
      "Epoch 00231: saving model to tmp/MobileNetV2_beans_model_e_231.h5\n",
      "Epoch 232/300\n",
      "33/33 [==============================] - 33s 909ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9965 - val_loss: 2.1374 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00232: saving model to tmp/MobileNetV2_beans_model_e_232.h5\n",
      "Epoch 233/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9993 - val_loss: 1.9002 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00233: saving model to tmp/MobileNetV2_beans_model_e_233.h5\n",
      "Epoch 234/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9996 - val_loss: 2.0758 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00234: saving model to tmp/MobileNetV2_beans_model_e_234.h5\n",
      "Epoch 235/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0046 - sparse_categorical_accuracy: 0.9989 - val_loss: 2.1430 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00235: saving model to tmp/MobileNetV2_beans_model_e_235.h5\n",
      "Epoch 236/300\n",
      "33/33 [==============================] - 32s 901ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - val_loss: 2.0841 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00236: saving model to tmp/MobileNetV2_beans_model_e_236.h5\n",
      "Epoch 237/300\n",
      "33/33 [==============================] - 32s 879ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.7047 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00237: saving model to tmp/MobileNetV2_beans_model_e_237.h5\n",
      "Epoch 238/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0014 - sparse_categorical_accuracy: 0.9999 - val_loss: 1.8452 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00238: saving model to tmp/MobileNetV2_beans_model_e_238.h5\n",
      "Epoch 239/300\n",
      "33/33 [==============================] - 32s 898ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9931 - val_loss: 3.8550 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00239: saving model to tmp/MobileNetV2_beans_model_e_239.h5\n",
      "Epoch 240/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0220 - sparse_categorical_accuracy: 0.9933 - val_loss: 3.9431 - val_sparse_categorical_accuracy: 0.5639\n",
      "\n",
      "Epoch 00240: saving model to tmp/MobileNetV2_beans_model_e_240.h5\n",
      "Epoch 241/300\n",
      "33/33 [==============================] - 33s 903ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9979 - val_loss: 1.6439 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00241: saving model to tmp/MobileNetV2_beans_model_e_241.h5\n",
      "Epoch 242/300\n",
      "33/33 [==============================] - 32s 895ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9978 - val_loss: 1.8104 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00242: saving model to tmp/MobileNetV2_beans_model_e_242.h5\n",
      "Epoch 243/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9791 - val_loss: 6.0718 - val_sparse_categorical_accuracy: 0.6015\n",
      "\n",
      "Epoch 00243: saving model to tmp/MobileNetV2_beans_model_e_243.h5\n",
      "Epoch 244/300\n",
      "33/33 [==============================] - 32s 891ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9937 - val_loss: 2.1765 - val_sparse_categorical_accuracy: 0.7068\n",
      "\n",
      "Epoch 00244: saving model to tmp/MobileNetV2_beans_model_e_244.h5\n",
      "Epoch 245/300\n",
      "33/33 [==============================] - 32s 898ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9939 - val_loss: 1.8267 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00245: saving model to tmp/MobileNetV2_beans_model_e_245.h5\n",
      "Epoch 246/300\n",
      "33/33 [==============================] - 32s 885ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9973 - val_loss: 2.8606 - val_sparse_categorical_accuracy: 0.6241\n",
      "\n",
      "Epoch 00246: saving model to tmp/MobileNetV2_beans_model_e_246.h5\n",
      "Epoch 247/300\n",
      "33/33 [==============================] - 31s 884ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9993 - val_loss: 1.9732 - val_sparse_categorical_accuracy: 0.6767\n",
      "\n",
      "Epoch 00247: saving model to tmp/MobileNetV2_beans_model_e_247.h5\n",
      "Epoch 248/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9994 - val_loss: 1.6777 - val_sparse_categorical_accuracy: 0.7068\n",
      "\n",
      "Epoch 00248: saving model to tmp/MobileNetV2_beans_model_e_248.h5\n",
      "Epoch 249/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9958 - val_loss: 2.7934 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00249: saving model to tmp/MobileNetV2_beans_model_e_249.h5\n",
      "Epoch 250/300\n",
      "33/33 [==============================] - 33s 902ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9971 - val_loss: 5.3582 - val_sparse_categorical_accuracy: 0.6241\n",
      "\n",
      "Epoch 00250: saving model to tmp/MobileNetV2_beans_model_e_250.h5\n",
      "Epoch 251/300\n",
      "33/33 [==============================] - 32s 898ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9820 - val_loss: 10.0604 - val_sparse_categorical_accuracy: 0.5564\n",
      "\n",
      "Epoch 00251: saving model to tmp/MobileNetV2_beans_model_e_251.h5\n",
      "Epoch 252/300\n",
      "33/33 [==============================] - 32s 900ms/step - loss: 0.0128 - sparse_categorical_accuracy: 0.9951 - val_loss: 6.4626 - val_sparse_categorical_accuracy: 0.6015\n",
      "\n",
      "Epoch 00252: saving model to tmp/MobileNetV2_beans_model_e_252.h5\n",
      "Epoch 253/300\n",
      "33/33 [==============================] - 32s 904ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9934 - val_loss: 4.4610 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00253: saving model to tmp/MobileNetV2_beans_model_e_253.h5\n",
      "Epoch 254/300\n",
      "33/33 [==============================] - 32s 903ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9916 - val_loss: 5.1364 - val_sparse_categorical_accuracy: 0.6165\n",
      "\n",
      "Epoch 00254: saving model to tmp/MobileNetV2_beans_model_e_254.h5\n",
      "Epoch 255/300\n",
      "33/33 [==============================] - 32s 879ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9876 - val_loss: 6.9044 - val_sparse_categorical_accuracy: 0.5714\n",
      "\n",
      "Epoch 00255: saving model to tmp/MobileNetV2_beans_model_e_255.h5\n",
      "Epoch 256/300\n",
      "33/33 [==============================] - 32s 903ms/step - loss: 0.0687 - sparse_categorical_accuracy: 0.9794 - val_loss: 14.1659 - val_sparse_categorical_accuracy: 0.4662\n",
      "\n",
      "Epoch 00256: saving model to tmp/MobileNetV2_beans_model_e_256.h5\n",
      "Epoch 257/300\n",
      "33/33 [==============================] - 32s 894ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9923 - val_loss: 10.9875 - val_sparse_categorical_accuracy: 0.5414\n",
      "\n",
      "Epoch 00257: saving model to tmp/MobileNetV2_beans_model_e_257.h5\n",
      "Epoch 258/300\n",
      "33/33 [==============================] - 32s 902ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9922 - val_loss: 3.4556 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00258: saving model to tmp/MobileNetV2_beans_model_e_258.h5\n",
      "Epoch 259/300\n",
      "33/33 [==============================] - 33s 912ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9951 - val_loss: 2.4975 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00259: saving model to tmp/MobileNetV2_beans_model_e_259.h5\n",
      "Epoch 260/300\n",
      "33/33 [==============================] - 33s 907ms/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9985 - val_loss: 2.9729 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00260: saving model to tmp/MobileNetV2_beans_model_e_260.h5\n",
      "Epoch 261/300\n",
      "33/33 [==============================] - 32s 901ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.7405 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00261: saving model to tmp/MobileNetV2_beans_model_e_261.h5\n",
      "Epoch 262/300\n",
      "33/33 [==============================] - 32s 901ms/step - loss: 0.0035 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.3320 - val_sparse_categorical_accuracy: 0.7820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00262: saving model to tmp/MobileNetV2_beans_model_e_262.h5\n",
      "Epoch 263/300\n",
      "33/33 [==============================] - 32s 887ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9991 - val_loss: 1.9124 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00263: saving model to tmp/MobileNetV2_beans_model_e_263.h5\n",
      "Epoch 264/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9989 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00264: saving model to tmp/MobileNetV2_beans_model_e_264.h5\n",
      "Epoch 265/300\n",
      "33/33 [==============================] - 33s 907ms/step - loss: 0.0014 - sparse_categorical_accuracy: 0.9996 - val_loss: 2.2001 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00265: saving model to tmp/MobileNetV2_beans_model_e_265.h5\n",
      "Epoch 266/300\n",
      "33/33 [==============================] - 33s 912ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.5320 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00266: saving model to tmp/MobileNetV2_beans_model_e_266.h5\n",
      "Epoch 267/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9972 - val_loss: 2.0092 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00267: saving model to tmp/MobileNetV2_beans_model_e_267.h5\n",
      "Epoch 268/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9997 - val_loss: 1.8424 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00268: saving model to tmp/MobileNetV2_beans_model_e_268.h5\n",
      "Epoch 269/300\n",
      "33/33 [==============================] - 33s 900ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.7924 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00269: saving model to tmp/MobileNetV2_beans_model_e_269.h5\n",
      "Epoch 270/300\n",
      "33/33 [==============================] - 32s 901ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9988 - val_loss: 1.8675 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00270: saving model to tmp/MobileNetV2_beans_model_e_270.h5\n",
      "Epoch 271/300\n",
      "33/33 [==============================] - 33s 917ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9999 - val_loss: 2.3369 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00271: saving model to tmp/MobileNetV2_beans_model_e_271.h5\n",
      "Epoch 272/300\n",
      "33/33 [==============================] - 32s 903ms/step - loss: 0.0507 - sparse_categorical_accuracy: 0.9826 - val_loss: 2.1208 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00272: saving model to tmp/MobileNetV2_beans_model_e_272.h5\n",
      "Epoch 273/300\n",
      "33/33 [==============================] - 33s 918ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9800 - val_loss: 3.7849 - val_sparse_categorical_accuracy: 0.6617\n",
      "\n",
      "Epoch 00273: saving model to tmp/MobileNetV2_beans_model_e_273.h5\n",
      "Epoch 274/300\n",
      "33/33 [==============================] - 33s 911ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9830 - val_loss: 12.7656 - val_sparse_categorical_accuracy: 0.4436\n",
      "\n",
      "Epoch 00274: saving model to tmp/MobileNetV2_beans_model_e_274.h5\n",
      "Epoch 275/300\n",
      "33/33 [==============================] - 33s 905ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9956 - val_loss: 5.5180 - val_sparse_categorical_accuracy: 0.5639\n",
      "\n",
      "Epoch 00275: saving model to tmp/MobileNetV2_beans_model_e_275.h5\n",
      "Epoch 276/300\n",
      "33/33 [==============================] - 33s 898ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9981 - val_loss: 4.0286 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00276: saving model to tmp/MobileNetV2_beans_model_e_276.h5\n",
      "Epoch 277/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9954 - val_loss: 2.7654 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00277: saving model to tmp/MobileNetV2_beans_model_e_277.h5\n",
      "Epoch 278/300\n",
      "33/33 [==============================] - 33s 916ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9930 - val_loss: 7.6409 - val_sparse_categorical_accuracy: 0.6241\n",
      "\n",
      "Epoch 00278: saving model to tmp/MobileNetV2_beans_model_e_278.h5\n",
      "Epoch 279/300\n",
      "33/33 [==============================] - 32s 898ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9933 - val_loss: 4.1238 - val_sparse_categorical_accuracy: 0.6015\n",
      "\n",
      "Epoch 00279: saving model to tmp/MobileNetV2_beans_model_e_279.h5\n",
      "Epoch 280/300\n",
      "33/33 [==============================] - 32s 893ms/step - loss: 0.0096 - sparse_categorical_accuracy: 0.9971 - val_loss: 3.5295 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00280: saving model to tmp/MobileNetV2_beans_model_e_280.h5\n",
      "Epoch 281/300\n",
      "33/33 [==============================] - 33s 915ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9958 - val_loss: 3.4118 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00281: saving model to tmp/MobileNetV2_beans_model_e_281.h5\n",
      "Epoch 282/300\n",
      "33/33 [==============================] - 33s 906ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9931 - val_loss: 7.0870 - val_sparse_categorical_accuracy: 0.6466\n",
      "\n",
      "Epoch 00282: saving model to tmp/MobileNetV2_beans_model_e_282.h5\n",
      "Epoch 283/300\n",
      "33/33 [==============================] - 33s 911ms/step - loss: 0.0292 - sparse_categorical_accuracy: 0.9874 - val_loss: 5.2497 - val_sparse_categorical_accuracy: 0.6241\n",
      "\n",
      "Epoch 00283: saving model to tmp/MobileNetV2_beans_model_e_283.h5\n",
      "Epoch 284/300\n",
      "33/33 [==============================] - 33s 912ms/step - loss: 0.0089 - sparse_categorical_accuracy: 0.9965 - val_loss: 5.5572 - val_sparse_categorical_accuracy: 0.5414\n",
      "\n",
      "Epoch 00284: saving model to tmp/MobileNetV2_beans_model_e_284.h5\n",
      "Epoch 285/300\n",
      "33/33 [==============================] - 33s 909ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.9923 - val_loss: 11.5599 - val_sparse_categorical_accuracy: 0.3609\n",
      "\n",
      "Epoch 00285: saving model to tmp/MobileNetV2_beans_model_e_285.h5\n",
      "Epoch 286/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9928 - val_loss: 12.5561 - val_sparse_categorical_accuracy: 0.4135\n",
      "\n",
      "Epoch 00286: saving model to tmp/MobileNetV2_beans_model_e_286.h5\n",
      "Epoch 287/300\n",
      "33/33 [==============================] - 33s 898ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9918 - val_loss: 4.3402 - val_sparse_categorical_accuracy: 0.6391\n",
      "\n",
      "Epoch 00287: saving model to tmp/MobileNetV2_beans_model_e_287.h5\n",
      "Epoch 288/300\n",
      "33/33 [==============================] - 32s 881ms/step - loss: 0.0091 - sparse_categorical_accuracy: 0.9960 - val_loss: 2.9742 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00288: saving model to tmp/MobileNetV2_beans_model_e_288.h5\n",
      "Epoch 289/300\n",
      "33/33 [==============================] - 32s 898ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9952 - val_loss: 3.2568 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00289: saving model to tmp/MobileNetV2_beans_model_e_289.h5\n",
      "Epoch 290/300\n",
      "33/33 [==============================] - 32s 897ms/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9996 - val_loss: 3.3690 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00290: saving model to tmp/MobileNetV2_beans_model_e_290.h5\n",
      "Epoch 291/300\n",
      "33/33 [==============================] - 32s 904ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9969 - val_loss: 4.5156 - val_sparse_categorical_accuracy: 0.6466\n",
      "\n",
      "Epoch 00291: saving model to tmp/MobileNetV2_beans_model_e_291.h5\n",
      "Epoch 292/300\n",
      "33/33 [==============================] - 33s 912ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9917 - val_loss: 4.5389 - val_sparse_categorical_accuracy: 0.6767\n",
      "\n",
      "Epoch 00292: saving model to tmp/MobileNetV2_beans_model_e_292.h5\n",
      "Epoch 293/300\n",
      "33/33 [==============================] - 32s 892ms/step - loss: 0.0215 - sparse_categorical_accuracy: 0.9940 - val_loss: 5.0334 - val_sparse_categorical_accuracy: 0.6466\n",
      "\n",
      "Epoch 00293: saving model to tmp/MobileNetV2_beans_model_e_293.h5\n",
      "Epoch 294/300\n",
      "33/33 [==============================] - 32s 886ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9973 - val_loss: 11.1077 - val_sparse_categorical_accuracy: 0.5414\n",
      "\n",
      "Epoch 00294: saving model to tmp/MobileNetV2_beans_model_e_294.h5\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9846 - val_loss: 12.9825 - val_sparse_categorical_accuracy: 0.5188\n",
      "\n",
      "Epoch 00295: saving model to tmp/MobileNetV2_beans_model_e_295.h5\n",
      "Epoch 296/300\n",
      "33/33 [==============================] - 32s 890ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9948 - val_loss: 9.8595 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00296: saving model to tmp/MobileNetV2_beans_model_e_296.h5\n",
      "Epoch 297/300\n",
      "33/33 [==============================] - 33s 916ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9864 - val_loss: 10.5916 - val_sparse_categorical_accuracy: 0.5263\n",
      "\n",
      "Epoch 00297: saving model to tmp/MobileNetV2_beans_model_e_297.h5\n",
      "Epoch 298/300\n",
      "33/33 [==============================] - 32s 888ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9881 - val_loss: 5.8728 - val_sparse_categorical_accuracy: 0.6090\n",
      "\n",
      "Epoch 00298: saving model to tmp/MobileNetV2_beans_model_e_298.h5\n",
      "Epoch 299/300\n",
      "33/33 [==============================] - 32s 903ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9921 - val_loss: 5.7093 - val_sparse_categorical_accuracy: 0.6466\n",
      "\n",
      "Epoch 00299: saving model to tmp/MobileNetV2_beans_model_e_299.h5\n",
      "Epoch 300/300\n",
      "33/33 [==============================] - 32s 899ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9981 - val_loss: 11.3018 - val_sparse_categorical_accuracy: 0.6165\n",
      "\n",
      "Epoch 00300: saving model to tmp/MobileNetV2_beans_model_e_300.h5\n",
      "Saved to: MobileNetV2_beans_model.h5\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 140ms/step - loss: 12.9655 - sparse_categorical_accuracy: 0.5312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.965460777282715, 0.53125]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNets - B0, B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"EfficentNetB0_beans_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "33/33 [==============================] - 51s 1s/step - loss: 1.2395 - sparse_categorical_accuracy: 0.3663 - val_loss: 1.1039 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00001: saving model to tmp/EfficentNetB0_beans_model_e_01.h5\n",
      "Epoch 2/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.9288 - sparse_categorical_accuracy: 0.5635 - val_loss: 1.2294 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00002: saving model to tmp/EfficentNetB0_beans_model_e_02.h5\n",
      "Epoch 3/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.7894 - sparse_categorical_accuracy: 0.6756 - val_loss: 1.3099 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00003: saving model to tmp/EfficentNetB0_beans_model_e_03.h5\n",
      "Epoch 4/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.6316 - sparse_categorical_accuracy: 0.7251 - val_loss: 1.3615 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00004: saving model to tmp/EfficentNetB0_beans_model_e_04.h5\n",
      "Epoch 5/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.6195 - sparse_categorical_accuracy: 0.7458 - val_loss: 1.6457 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00005: saving model to tmp/EfficentNetB0_beans_model_e_05.h5\n",
      "Epoch 6/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.5497 - sparse_categorical_accuracy: 0.7666 - val_loss: 1.8792 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00006: saving model to tmp/EfficentNetB0_beans_model_e_06.h5\n",
      "Epoch 7/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.5627 - sparse_categorical_accuracy: 0.7801 - val_loss: 1.5065 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00007: saving model to tmp/EfficentNetB0_beans_model_e_07.h5\n",
      "Epoch 8/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.4861 - sparse_categorical_accuracy: 0.8033 - val_loss: 1.5020 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00008: saving model to tmp/EfficentNetB0_beans_model_e_08.h5\n",
      "Epoch 9/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.3754 - sparse_categorical_accuracy: 0.8484 - val_loss: 1.3419 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00009: saving model to tmp/EfficentNetB0_beans_model_e_09.h5\n",
      "Epoch 10/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.3426 - sparse_categorical_accuracy: 0.8676 - val_loss: 1.1281 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00010: saving model to tmp/EfficentNetB0_beans_model_e_10.h5\n",
      "Epoch 11/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.3261 - sparse_categorical_accuracy: 0.8771 - val_loss: 1.5869 - val_sparse_categorical_accuracy: 0.4662\n",
      "\n",
      "Epoch 00011: saving model to tmp/EfficentNetB0_beans_model_e_11.h5\n",
      "Epoch 12/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.2378 - sparse_categorical_accuracy: 0.9088 - val_loss: 2.2673 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00012: saving model to tmp/EfficentNetB0_beans_model_e_12.h5\n",
      "Epoch 13/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.2382 - sparse_categorical_accuracy: 0.9154 - val_loss: 1.1137 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00013: saving model to tmp/EfficentNetB0_beans_model_e_13.h5\n",
      "Epoch 14/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9296 - val_loss: 1.6822 - val_sparse_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00014: saving model to tmp/EfficentNetB0_beans_model_e_14.h5\n",
      "Epoch 15/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1601 - sparse_categorical_accuracy: 0.9449 - val_loss: 1.7311 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00015: saving model to tmp/EfficentNetB0_beans_model_e_15.h5\n",
      "Epoch 16/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1602 - sparse_categorical_accuracy: 0.9445 - val_loss: 1.9903 - val_sparse_categorical_accuracy: 0.3684\n",
      "\n",
      "Epoch 00016: saving model to tmp/EfficentNetB0_beans_model_e_16.h5\n",
      "Epoch 17/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1650 - sparse_categorical_accuracy: 0.9479 - val_loss: 2.2698 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00017: saving model to tmp/EfficentNetB0_beans_model_e_17.h5\n",
      "Epoch 18/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1224 - sparse_categorical_accuracy: 0.9538 - val_loss: 3.8370 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00018: saving model to tmp/EfficentNetB0_beans_model_e_18.h5\n",
      "Epoch 19/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1352 - sparse_categorical_accuracy: 0.9529 - val_loss: 4.4151 - val_sparse_categorical_accuracy: 0.3383\n",
      "\n",
      "Epoch 00019: saving model to tmp/EfficentNetB0_beans_model_e_19.h5\n",
      "Epoch 20/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1141 - sparse_categorical_accuracy: 0.9660 - val_loss: 2.1549 - val_sparse_categorical_accuracy: 0.3985\n",
      "\n",
      "Epoch 00020: saving model to tmp/EfficentNetB0_beans_model_e_20.h5\n",
      "Epoch 21/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0974 - sparse_categorical_accuracy: 0.9670 - val_loss: 3.4610 - val_sparse_categorical_accuracy: 0.3759\n",
      "\n",
      "Epoch 00021: saving model to tmp/EfficentNetB0_beans_model_e_21.h5\n",
      "Epoch 22/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0882 - sparse_categorical_accuracy: 0.9716 - val_loss: 1.1192 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00022: saving model to tmp/EfficentNetB0_beans_model_e_22.h5\n",
      "Epoch 23/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.1199 - sparse_categorical_accuracy: 0.9501 - val_loss: 1.6436 - val_sparse_categorical_accuracy: 0.5113\n",
      "\n",
      "Epoch 00023: saving model to tmp/EfficentNetB0_beans_model_e_23.h5\n",
      "Epoch 24/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0732 - sparse_categorical_accuracy: 0.9776 - val_loss: 0.8809 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00024: saving model to tmp/EfficentNetB0_beans_model_e_24.h5\n",
      "Epoch 25/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1191 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.9344 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00025: saving model to tmp/EfficentNetB0_beans_model_e_25.h5\n",
      "Epoch 26/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0947 - sparse_categorical_accuracy: 0.9637 - val_loss: 0.6464 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00026: saving model to tmp/EfficentNetB0_beans_model_e_26.h5\n",
      "Epoch 27/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9846 - val_loss: 0.7591 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00027: saving model to tmp/EfficentNetB0_beans_model_e_27.h5\n",
      "Epoch 28/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9845 - val_loss: 0.8496 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00028: saving model to tmp/EfficentNetB0_beans_model_e_28.h5\n",
      "Epoch 29/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0830 - sparse_categorical_accuracy: 0.9666 - val_loss: 1.4883 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00029: saving model to tmp/EfficentNetB0_beans_model_e_29.h5\n",
      "Epoch 30/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0843 - sparse_categorical_accuracy: 0.9719 - val_loss: 0.8528 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00030: saving model to tmp/EfficentNetB0_beans_model_e_30.h5\n",
      "Epoch 31/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0620 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.7933 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00031: saving model to tmp/EfficentNetB0_beans_model_e_31.h5\n",
      "Epoch 32/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0678 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.7610 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00032: saving model to tmp/EfficentNetB0_beans_model_e_32.h5\n",
      "Epoch 33/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0778 - sparse_categorical_accuracy: 0.9761 - val_loss: 0.6040 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00033: saving model to tmp/EfficentNetB0_beans_model_e_33.h5\n",
      "Epoch 34/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 45s 1s/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9626 - val_loss: 1.2128 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00034: saving model to tmp/EfficentNetB0_beans_model_e_34.h5\n",
      "Epoch 35/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.8073 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00035: saving model to tmp/EfficentNetB0_beans_model_e_35.h5\n",
      "Epoch 36/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0797 - sparse_categorical_accuracy: 0.9765 - val_loss: 2.0149 - val_sparse_categorical_accuracy: 0.6241\n",
      "\n",
      "Epoch 00036: saving model to tmp/EfficentNetB0_beans_model_e_36.h5\n",
      "Epoch 37/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9757 - val_loss: 2.2425 - val_sparse_categorical_accuracy: 0.5714\n",
      "\n",
      "Epoch 00037: saving model to tmp/EfficentNetB0_beans_model_e_37.h5\n",
      "Epoch 38/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0680 - sparse_categorical_accuracy: 0.9724 - val_loss: 0.6571 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00038: saving model to tmp/EfficentNetB0_beans_model_e_38.h5\n",
      "Epoch 39/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0356 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.6786 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00039: saving model to tmp/EfficentNetB0_beans_model_e_39.h5\n",
      "Epoch 40/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.6912 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00040: saving model to tmp/EfficentNetB0_beans_model_e_40.h5\n",
      "Epoch 41/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.8492 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00041: saving model to tmp/EfficentNetB0_beans_model_e_41.h5\n",
      "Epoch 42/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0656 - sparse_categorical_accuracy: 0.9791 - val_loss: 0.9631 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00042: saving model to tmp/EfficentNetB0_beans_model_e_42.h5\n",
      "Epoch 43/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0876 - sparse_categorical_accuracy: 0.9778 - val_loss: 0.9270 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00043: saving model to tmp/EfficentNetB0_beans_model_e_43.h5\n",
      "Epoch 44/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0484 - sparse_categorical_accuracy: 0.9816 - val_loss: 1.0037 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00044: saving model to tmp/EfficentNetB0_beans_model_e_44.h5\n",
      "Epoch 45/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0482 - sparse_categorical_accuracy: 0.9872 - val_loss: 0.9890 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00045: saving model to tmp/EfficentNetB0_beans_model_e_45.h5\n",
      "Epoch 46/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.8088 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00046: saving model to tmp/EfficentNetB0_beans_model_e_46.h5\n",
      "Epoch 47/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.9727 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00047: saving model to tmp/EfficentNetB0_beans_model_e_47.h5\n",
      "Epoch 48/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9863 - val_loss: 1.0634 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00048: saving model to tmp/EfficentNetB0_beans_model_e_48.h5\n",
      "Epoch 49/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9889 - val_loss: 1.4863 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00049: saving model to tmp/EfficentNetB0_beans_model_e_49.h5\n",
      "Epoch 50/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0451 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.8550 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00050: saving model to tmp/EfficentNetB0_beans_model_e_50.h5\n",
      "Epoch 51/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9863 - val_loss: 0.8496 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00051: saving model to tmp/EfficentNetB0_beans_model_e_51.h5\n",
      "Epoch 52/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9931 - val_loss: 1.1552 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00052: saving model to tmp/EfficentNetB0_beans_model_e_52.h5\n",
      "Epoch 53/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0554 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.6693 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00053: saving model to tmp/EfficentNetB0_beans_model_e_53.h5\n",
      "Epoch 54/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0406 - sparse_categorical_accuracy: 0.9848 - val_loss: 0.7158 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00054: saving model to tmp/EfficentNetB0_beans_model_e_54.h5\n",
      "Epoch 55/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0439 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.9569 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00055: saving model to tmp/EfficentNetB0_beans_model_e_55.h5\n",
      "Epoch 56/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9923 - val_loss: 1.5442 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00056: saving model to tmp/EfficentNetB0_beans_model_e_56.h5\n",
      "Epoch 57/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0272 - sparse_categorical_accuracy: 0.9912 - val_loss: 1.9068 - val_sparse_categorical_accuracy: 0.6541\n",
      "\n",
      "Epoch 00057: saving model to tmp/EfficentNetB0_beans_model_e_57.h5\n",
      "Epoch 58/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.8960 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00058: saving model to tmp/EfficentNetB0_beans_model_e_58.h5\n",
      "Epoch 59/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0477 - sparse_categorical_accuracy: 0.9851 - val_loss: 0.6422 - val_sparse_categorical_accuracy: 0.8271\n",
      "\n",
      "Epoch 00059: saving model to tmp/EfficentNetB0_beans_model_e_59.h5\n",
      "Epoch 60/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.6783 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00060: saving model to tmp/EfficentNetB0_beans_model_e_60.h5\n",
      "Epoch 61/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.5449 - val_sparse_categorical_accuracy: 0.8346\n",
      "\n",
      "Epoch 00061: saving model to tmp/EfficentNetB0_beans_model_e_61.h5\n",
      "Epoch 62/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6562 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00062: saving model to tmp/EfficentNetB0_beans_model_e_62.h5\n",
      "Epoch 63/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.9749 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00063: saving model to tmp/EfficentNetB0_beans_model_e_63.h5\n",
      "Epoch 64/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9932 - val_loss: 0.9523 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00064: saving model to tmp/EfficentNetB0_beans_model_e_64.h5\n",
      "Epoch 65/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.9373 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00065: saving model to tmp/EfficentNetB0_beans_model_e_65.h5\n",
      "Epoch 66/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9963 - val_loss: 0.7935 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00066: saving model to tmp/EfficentNetB0_beans_model_e_66.h5\n",
      "Epoch 67/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 45s 1s/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.7171 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00067: saving model to tmp/EfficentNetB0_beans_model_e_67.h5\n",
      "Epoch 68/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.6515 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00068: saving model to tmp/EfficentNetB0_beans_model_e_68.h5\n",
      "Epoch 69/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9945 - val_loss: 0.9901 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00069: saving model to tmp/EfficentNetB0_beans_model_e_69.h5\n",
      "Epoch 70/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.1010 - sparse_categorical_accuracy: 0.9862 - val_loss: 1.1109 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00070: saving model to tmp/EfficentNetB0_beans_model_e_70.h5\n",
      "Epoch 71/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9911 - val_loss: 1.2317 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00071: saving model to tmp/EfficentNetB0_beans_model_e_71.h5\n",
      "Epoch 72/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0674 - sparse_categorical_accuracy: 0.9761 - val_loss: 2.3286 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00072: saving model to tmp/EfficentNetB0_beans_model_e_72.h5\n",
      "Epoch 73/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9795 - val_loss: 1.5463 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00073: saving model to tmp/EfficentNetB0_beans_model_e_73.h5\n",
      "Epoch 74/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9831 - val_loss: 1.2072 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00074: saving model to tmp/EfficentNetB0_beans_model_e_74.h5\n",
      "Epoch 75/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0391 - sparse_categorical_accuracy: 0.9855 - val_loss: 1.2247 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00075: saving model to tmp/EfficentNetB0_beans_model_e_75.h5\n",
      "Epoch 76/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9889 - val_loss: 0.7540 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00076: saving model to tmp/EfficentNetB0_beans_model_e_76.h5\n",
      "Epoch 77/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.8754 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00077: saving model to tmp/EfficentNetB0_beans_model_e_77.h5\n",
      "Epoch 78/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0355 - sparse_categorical_accuracy: 0.9884 - val_loss: 0.9837 - val_sparse_categorical_accuracy: 0.7068\n",
      "\n",
      "Epoch 00078: saving model to tmp/EfficentNetB0_beans_model_e_78.h5\n",
      "Epoch 79/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9786 - val_loss: 1.1366 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00079: saving model to tmp/EfficentNetB0_beans_model_e_79.h5\n",
      "Epoch 80/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0611 - sparse_categorical_accuracy: 0.9776 - val_loss: 1.5176 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00080: saving model to tmp/EfficentNetB0_beans_model_e_80.h5\n",
      "Epoch 81/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0521 - sparse_categorical_accuracy: 0.9838 - val_loss: 1.7947 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00081: saving model to tmp/EfficentNetB0_beans_model_e_81.h5\n",
      "Epoch 82/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0453 - sparse_categorical_accuracy: 0.9858 - val_loss: 1.3948 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00082: saving model to tmp/EfficentNetB0_beans_model_e_82.h5\n",
      "Epoch 83/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9963 - val_loss: 1.1971 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00083: saving model to tmp/EfficentNetB0_beans_model_e_83.h5\n",
      "Epoch 84/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9889 - val_loss: 2.6644 - val_sparse_categorical_accuracy: 0.6090\n",
      "\n",
      "Epoch 00084: saving model to tmp/EfficentNetB0_beans_model_e_84.h5\n",
      "Epoch 85/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0705 - sparse_categorical_accuracy: 0.9821 - val_loss: 0.8099 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00085: saving model to tmp/EfficentNetB0_beans_model_e_85.h5\n",
      "Epoch 86/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9820 - val_loss: 2.0158 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00086: saving model to tmp/EfficentNetB0_beans_model_e_86.h5\n",
      "Epoch 87/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0823 - sparse_categorical_accuracy: 0.9660 - val_loss: 1.9697 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00087: saving model to tmp/EfficentNetB0_beans_model_e_87.h5\n",
      "Epoch 88/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0482 - sparse_categorical_accuracy: 0.9834 - val_loss: 0.8937 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00088: saving model to tmp/EfficentNetB0_beans_model_e_88.h5\n",
      "Epoch 89/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9948 - val_loss: 0.8049 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00089: saving model to tmp/EfficentNetB0_beans_model_e_89.h5\n",
      "Epoch 90/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9935 - val_loss: 0.8009 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00090: saving model to tmp/EfficentNetB0_beans_model_e_90.h5\n",
      "Epoch 91/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.8065 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00091: saving model to tmp/EfficentNetB0_beans_model_e_91.h5\n",
      "Epoch 92/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.8612 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00092: saving model to tmp/EfficentNetB0_beans_model_e_92.h5\n",
      "Epoch 93/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0400 - sparse_categorical_accuracy: 0.9863 - val_loss: 1.1876 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00093: saving model to tmp/EfficentNetB0_beans_model_e_93.h5\n",
      "Epoch 94/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9887 - val_loss: 1.0261 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00094: saving model to tmp/EfficentNetB0_beans_model_e_94.h5\n",
      "Epoch 95/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9999 - val_loss: 0.7867 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00095: saving model to tmp/EfficentNetB0_beans_model_e_95.h5\n",
      "Epoch 96/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9936 - val_loss: 1.0643 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00096: saving model to tmp/EfficentNetB0_beans_model_e_96.h5\n",
      "Epoch 97/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.9080 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00097: saving model to tmp/EfficentNetB0_beans_model_e_97.h5\n",
      "Epoch 98/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.8994 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00098: saving model to tmp/EfficentNetB0_beans_model_e_98.h5\n",
      "Epoch 99/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9924 - val_loss: 1.1550 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00099: saving model to tmp/EfficentNetB0_beans_model_e_99.h5\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 44s 1s/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.7509 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00100: saving model to tmp/EfficentNetB0_beans_model_e_100.h5\n",
      "Epoch 101/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.8328 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00101: saving model to tmp/EfficentNetB0_beans_model_e_101.h5\n",
      "Epoch 102/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.1206 - sparse_categorical_accuracy: 0.9565 - val_loss: 2.3333 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00102: saving model to tmp/EfficentNetB0_beans_model_e_102.h5\n",
      "Epoch 103/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9841 - val_loss: 1.2764 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00103: saving model to tmp/EfficentNetB0_beans_model_e_103.h5\n",
      "Epoch 104/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0220 - sparse_categorical_accuracy: 0.9958 - val_loss: 1.0393 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00104: saving model to tmp/EfficentNetB0_beans_model_e_104.h5\n",
      "Epoch 105/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.9701 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00105: saving model to tmp/EfficentNetB0_beans_model_e_105.h5\n",
      "Epoch 106/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9948 - val_loss: 0.9750 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00106: saving model to tmp/EfficentNetB0_beans_model_e_106.h5\n",
      "Epoch 107/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.9531 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00107: saving model to tmp/EfficentNetB0_beans_model_e_107.h5\n",
      "Epoch 108/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.8663 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00108: saving model to tmp/EfficentNetB0_beans_model_e_108.h5\n",
      "Epoch 109/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.8075 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00109: saving model to tmp/EfficentNetB0_beans_model_e_109.h5\n",
      "Epoch 110/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.7946 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00110: saving model to tmp/EfficentNetB0_beans_model_e_110.h5\n",
      "Epoch 111/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0073 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.8078 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00111: saving model to tmp/EfficentNetB0_beans_model_e_111.h5\n",
      "Epoch 112/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9954 - val_loss: 1.2233 - val_sparse_categorical_accuracy: 0.6466\n",
      "\n",
      "Epoch 00112: saving model to tmp/EfficentNetB0_beans_model_e_112.h5\n",
      "Epoch 113/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9856 - val_loss: 1.9215 - val_sparse_categorical_accuracy: 0.6767\n",
      "\n",
      "Epoch 00113: saving model to tmp/EfficentNetB0_beans_model_e_113.h5\n",
      "Epoch 114/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9947 - val_loss: 1.3984 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00114: saving model to tmp/EfficentNetB0_beans_model_e_114.h5\n",
      "Epoch 115/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9933 - val_loss: 1.6386 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00115: saving model to tmp/EfficentNetB0_beans_model_e_115.h5\n",
      "Epoch 116/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9959 - val_loss: 1.4538 - val_sparse_categorical_accuracy: 0.6767\n",
      "\n",
      "Epoch 00116: saving model to tmp/EfficentNetB0_beans_model_e_116.h5\n",
      "Epoch 117/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.9772 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00117: saving model to tmp/EfficentNetB0_beans_model_e_117.h5\n",
      "Epoch 118/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9934 - val_loss: 1.1029 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00118: saving model to tmp/EfficentNetB0_beans_model_e_118.h5\n",
      "Epoch 119/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.9079 - val_sparse_categorical_accuracy: 0.8421\n",
      "\n",
      "Epoch 00119: saving model to tmp/EfficentNetB0_beans_model_e_119.h5\n",
      "Epoch 120/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.0032 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00120: saving model to tmp/EfficentNetB0_beans_model_e_120.h5\n",
      "Epoch 121/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.5587 - val_sparse_categorical_accuracy: 0.8346\n",
      "\n",
      "Epoch 00121: saving model to tmp/EfficentNetB0_beans_model_e_121.h5\n",
      "Epoch 122/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0402 - sparse_categorical_accuracy: 0.9876 - val_loss: 0.9942 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00122: saving model to tmp/EfficentNetB0_beans_model_e_122.h5\n",
      "Epoch 123/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9997 - val_loss: 0.8521 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00123: saving model to tmp/EfficentNetB0_beans_model_e_123.h5\n",
      "Epoch 124/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.6417 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00124: saving model to tmp/EfficentNetB0_beans_model_e_124.h5\n",
      "Epoch 125/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.5637 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00125: saving model to tmp/EfficentNetB0_beans_model_e_125.h5\n",
      "Epoch 126/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9965 - val_loss: 0.6449 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00126: saving model to tmp/EfficentNetB0_beans_model_e_126.h5\n",
      "Epoch 127/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.6071 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00127: saving model to tmp/EfficentNetB0_beans_model_e_127.h5\n",
      "Epoch 128/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.6170 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00128: saving model to tmp/EfficentNetB0_beans_model_e_128.h5\n",
      "Epoch 129/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.6567 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00129: saving model to tmp/EfficentNetB0_beans_model_e_129.h5\n",
      "Epoch 130/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.8035 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00130: saving model to tmp/EfficentNetB0_beans_model_e_130.h5\n",
      "Epoch 131/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.9072 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00131: saving model to tmp/EfficentNetB0_beans_model_e_131.h5\n",
      "Epoch 132/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9921 - val_loss: 0.6715 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00132: saving model to tmp/EfficentNetB0_beans_model_e_132.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.5999 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00133: saving model to tmp/EfficentNetB0_beans_model_e_133.h5\n",
      "Epoch 134/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0287 - sparse_categorical_accuracy: 0.9933 - val_loss: 0.6840 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00134: saving model to tmp/EfficentNetB0_beans_model_e_134.h5\n",
      "Epoch 135/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.7929 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00135: saving model to tmp/EfficentNetB0_beans_model_e_135.h5\n",
      "Epoch 136/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.9999 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00136: saving model to tmp/EfficentNetB0_beans_model_e_136.h5\n",
      "Epoch 137/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.6652 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00137: saving model to tmp/EfficentNetB0_beans_model_e_137.h5\n",
      "Epoch 138/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.7367 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00138: saving model to tmp/EfficentNetB0_beans_model_e_138.h5\n",
      "Epoch 139/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.7300 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00139: saving model to tmp/EfficentNetB0_beans_model_e_139.h5\n",
      "Epoch 140/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.3945 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00140: saving model to tmp/EfficentNetB0_beans_model_e_140.h5\n",
      "Epoch 141/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9962 - val_loss: 1.5444 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00141: saving model to tmp/EfficentNetB0_beans_model_e_141.h5\n",
      "Epoch 142/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9989 - val_loss: 1.3259 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00142: saving model to tmp/EfficentNetB0_beans_model_e_142.h5\n",
      "Epoch 143/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9974 - val_loss: 1.1251 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00143: saving model to tmp/EfficentNetB0_beans_model_e_143.h5\n",
      "Epoch 144/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9993 - val_loss: 1.0315 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00144: saving model to tmp/EfficentNetB0_beans_model_e_144.h5\n",
      "Epoch 145/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9919 - val_loss: 1.2010 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00145: saving model to tmp/EfficentNetB0_beans_model_e_145.h5\n",
      "Epoch 146/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0093 - sparse_categorical_accuracy: 0.9975 - val_loss: 1.2064 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00146: saving model to tmp/EfficentNetB0_beans_model_e_146.h5\n",
      "Epoch 147/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9984 - val_loss: 1.1565 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00147: saving model to tmp/EfficentNetB0_beans_model_e_147.h5\n",
      "Epoch 148/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9996 - val_loss: 1.0571 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00148: saving model to tmp/EfficentNetB0_beans_model_e_148.h5\n",
      "Epoch 149/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0111 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.9225 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00149: saving model to tmp/EfficentNetB0_beans_model_e_149.h5\n",
      "Epoch 150/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0021 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.9713 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00150: saving model to tmp/EfficentNetB0_beans_model_e_150.h5\n",
      "Epoch 151/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0094 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.8765 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00151: saving model to tmp/EfficentNetB0_beans_model_e_151.h5\n",
      "Epoch 152/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9987 - val_loss: 1.0319 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00152: saving model to tmp/EfficentNetB0_beans_model_e_152.h5\n",
      "Epoch 153/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.9712 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00153: saving model to tmp/EfficentNetB0_beans_model_e_153.h5\n",
      "Epoch 154/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9955 - val_loss: 1.0611 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00154: saving model to tmp/EfficentNetB0_beans_model_e_154.h5\n",
      "Epoch 155/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.8695 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00155: saving model to tmp/EfficentNetB0_beans_model_e_155.h5\n",
      "Epoch 156/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7040 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00156: saving model to tmp/EfficentNetB0_beans_model_e_156.h5\n",
      "Epoch 157/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9927 - val_loss: 1.0678 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00157: saving model to tmp/EfficentNetB0_beans_model_e_157.h5\n",
      "Epoch 158/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.8609 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00158: saving model to tmp/EfficentNetB0_beans_model_e_158.h5\n",
      "Epoch 159/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.8690 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00159: saving model to tmp/EfficentNetB0_beans_model_e_159.h5\n",
      "Epoch 160/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.6544 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00160: saving model to tmp/EfficentNetB0_beans_model_e_160.h5\n",
      "Epoch 161/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9969 - val_loss: 1.1841 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00161: saving model to tmp/EfficentNetB0_beans_model_e_161.h5\n",
      "Epoch 162/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9995 - val_loss: 1.2708 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00162: saving model to tmp/EfficentNetB0_beans_model_e_162.h5\n",
      "Epoch 163/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9993 - val_loss: 1.1525 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00163: saving model to tmp/EfficentNetB0_beans_model_e_163.h5\n",
      "Epoch 164/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9995 - val_loss: 1.1923 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00164: saving model to tmp/EfficentNetB0_beans_model_e_164.h5\n",
      "Epoch 165/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9904 - val_loss: 1.4058 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00165: saving model to tmp/EfficentNetB0_beans_model_e_165.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9927 - val_loss: 1.3057 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00166: saving model to tmp/EfficentNetB0_beans_model_e_166.h5\n",
      "Epoch 167/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.7185 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00167: saving model to tmp/EfficentNetB0_beans_model_e_167.h5\n",
      "Epoch 168/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.8636 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00168: saving model to tmp/EfficentNetB0_beans_model_e_168.h5\n",
      "Epoch 169/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.7965 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00169: saving model to tmp/EfficentNetB0_beans_model_e_169.h5\n",
      "Epoch 170/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9995 - val_loss: 1.1223 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00170: saving model to tmp/EfficentNetB0_beans_model_e_170.h5\n",
      "Epoch 171/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0083 - sparse_categorical_accuracy: 0.9965 - val_loss: 1.1253 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00171: saving model to tmp/EfficentNetB0_beans_model_e_171.h5\n",
      "Epoch 172/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9971 - val_loss: 1.1729 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00172: saving model to tmp/EfficentNetB0_beans_model_e_172.h5\n",
      "Epoch 173/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.7851 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00173: saving model to tmp/EfficentNetB0_beans_model_e_173.h5\n",
      "Epoch 174/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0305 - sparse_categorical_accuracy: 0.9899 - val_loss: 1.9747 - val_sparse_categorical_accuracy: 0.6692\n",
      "\n",
      "Epoch 00174: saving model to tmp/EfficentNetB0_beans_model_e_174.h5\n",
      "Epoch 175/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0094 - sparse_categorical_accuracy: 0.9932 - val_loss: 2.4169 - val_sparse_categorical_accuracy: 0.6541\n",
      "\n",
      "Epoch 00175: saving model to tmp/EfficentNetB0_beans_model_e_175.h5\n",
      "Epoch 176/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9980 - val_loss: 1.2463 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00176: saving model to tmp/EfficentNetB0_beans_model_e_176.h5\n",
      "Epoch 177/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9962 - val_loss: 1.5413 - val_sparse_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00177: saving model to tmp/EfficentNetB0_beans_model_e_177.h5\n",
      "Epoch 178/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0079 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.9070 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00178: saving model to tmp/EfficentNetB0_beans_model_e_178.h5\n",
      "Epoch 179/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.8601 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00179: saving model to tmp/EfficentNetB0_beans_model_e_179.h5\n",
      "Epoch 180/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0037 - sparse_categorical_accuracy: 0.9999 - val_loss: 0.9857 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00180: saving model to tmp/EfficentNetB0_beans_model_e_180.h5\n",
      "Epoch 181/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.9845 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00181: saving model to tmp/EfficentNetB0_beans_model_e_181.h5\n",
      "Epoch 182/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9979 - val_loss: 1.1015 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00182: saving model to tmp/EfficentNetB0_beans_model_e_182.h5\n",
      "Epoch 183/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.7581 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00183: saving model to tmp/EfficentNetB0_beans_model_e_183.h5\n",
      "Epoch 184/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0050 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.8933 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00184: saving model to tmp/EfficentNetB0_beans_model_e_184.h5\n",
      "Epoch 185/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9988 - val_loss: 1.2444 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00185: saving model to tmp/EfficentNetB0_beans_model_e_185.h5\n",
      "Epoch 186/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9934 - val_loss: 2.0050 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00186: saving model to tmp/EfficentNetB0_beans_model_e_186.h5\n",
      "Epoch 187/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9992 - val_loss: 1.0807 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00187: saving model to tmp/EfficentNetB0_beans_model_e_187.h5\n",
      "Epoch 188/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9966 - val_loss: 1.1179 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00188: saving model to tmp/EfficentNetB0_beans_model_e_188.h5\n",
      "Epoch 189/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.9769 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00189: saving model to tmp/EfficentNetB0_beans_model_e_189.h5\n",
      "Epoch 190/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.7036 - val_sparse_categorical_accuracy: 0.8346\n",
      "\n",
      "Epoch 00190: saving model to tmp/EfficentNetB0_beans_model_e_190.h5\n",
      "Epoch 191/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9738 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00191: saving model to tmp/EfficentNetB0_beans_model_e_191.h5\n",
      "Epoch 192/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.9833 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00192: saving model to tmp/EfficentNetB0_beans_model_e_192.h5\n",
      "Epoch 193/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9983 - val_loss: 1.0392 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00193: saving model to tmp/EfficentNetB0_beans_model_e_193.h5\n",
      "Epoch 194/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9981 - val_loss: 1.0990 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00194: saving model to tmp/EfficentNetB0_beans_model_e_194.h5\n",
      "Epoch 195/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.9974 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00195: saving model to tmp/EfficentNetB0_beans_model_e_195.h5\n",
      "Epoch 196/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0064 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.8461 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00196: saving model to tmp/EfficentNetB0_beans_model_e_196.h5\n",
      "Epoch 197/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.8145 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00197: saving model to tmp/EfficentNetB0_beans_model_e_197.h5\n",
      "Epoch 198/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9949 - val_loss: 1.2130 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00198: saving model to tmp/EfficentNetB0_beans_model_e_198.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9963 - val_loss: 1.1064 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00199: saving model to tmp/EfficentNetB0_beans_model_e_199.h5\n",
      "Epoch 200/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0096 - sparse_categorical_accuracy: 0.9964 - val_loss: 1.0033 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00200: saving model to tmp/EfficentNetB0_beans_model_e_200.h5\n",
      "Epoch 201/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9994 - val_loss: 1.0722 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00201: saving model to tmp/EfficentNetB0_beans_model_e_201.h5\n",
      "Epoch 202/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0021 - sparse_categorical_accuracy: 0.9994 - val_loss: 1.0200 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00202: saving model to tmp/EfficentNetB0_beans_model_e_202.h5\n",
      "Epoch 203/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8608 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00203: saving model to tmp/EfficentNetB0_beans_model_e_203.h5\n",
      "Epoch 204/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9986 - val_loss: 1.0376 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00204: saving model to tmp/EfficentNetB0_beans_model_e_204.h5\n",
      "Epoch 205/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0567 - sparse_categorical_accuracy: 0.9744 - val_loss: 1.5813 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00205: saving model to tmp/EfficentNetB0_beans_model_e_205.h5\n",
      "Epoch 206/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9838 - val_loss: 0.8217 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00206: saving model to tmp/EfficentNetB0_beans_model_e_206.h5\n",
      "Epoch 207/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9979 - val_loss: 1.0899 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00207: saving model to tmp/EfficentNetB0_beans_model_e_207.h5\n",
      "Epoch 208/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0072 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.8342 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00208: saving model to tmp/EfficentNetB0_beans_model_e_208.h5\n",
      "Epoch 209/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0096 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.9365 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00209: saving model to tmp/EfficentNetB0_beans_model_e_209.h5\n",
      "Epoch 210/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9964 - val_loss: 1.1621 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00210: saving model to tmp/EfficentNetB0_beans_model_e_210.h5\n",
      "Epoch 211/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9989 - val_loss: 1.9052 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00211: saving model to tmp/EfficentNetB0_beans_model_e_211.h5\n",
      "Epoch 212/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9937 - val_loss: 1.1794 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00212: saving model to tmp/EfficentNetB0_beans_model_e_212.h5\n",
      "Epoch 213/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.9954 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00213: saving model to tmp/EfficentNetB0_beans_model_e_213.h5\n",
      "Epoch 214/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.9372 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00214: saving model to tmp/EfficentNetB0_beans_model_e_214.h5\n",
      "Epoch 215/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9960 - val_loss: 1.0841 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00215: saving model to tmp/EfficentNetB0_beans_model_e_215.h5\n",
      "Epoch 216/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9952 - val_loss: 1.2306 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00216: saving model to tmp/EfficentNetB0_beans_model_e_216.h5\n",
      "Epoch 217/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9941 - val_loss: 1.2772 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00217: saving model to tmp/EfficentNetB0_beans_model_e_217.h5\n",
      "Epoch 218/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9996 - val_loss: 1.2908 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00218: saving model to tmp/EfficentNetB0_beans_model_e_218.h5\n",
      "Epoch 219/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9981 - val_loss: 1.3027 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00219: saving model to tmp/EfficentNetB0_beans_model_e_219.h5\n",
      "Epoch 220/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9958 - val_loss: 1.2295 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00220: saving model to tmp/EfficentNetB0_beans_model_e_220.h5\n",
      "Epoch 221/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.9927 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00221: saving model to tmp/EfficentNetB0_beans_model_e_221.h5\n",
      "Epoch 222/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8598 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00222: saving model to tmp/EfficentNetB0_beans_model_e_222.h5\n",
      "Epoch 223/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.8848 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00223: saving model to tmp/EfficentNetB0_beans_model_e_223.h5\n",
      "Epoch 224/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8706 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00224: saving model to tmp/EfficentNetB0_beans_model_e_224.h5\n",
      "Epoch 225/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9799 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00225: saving model to tmp/EfficentNetB0_beans_model_e_225.h5\n",
      "Epoch 226/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.9470 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00226: saving model to tmp/EfficentNetB0_beans_model_e_226.h5\n",
      "Epoch 227/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.8478 - val_sparse_categorical_accuracy: 0.8346\n",
      "\n",
      "Epoch 00227: saving model to tmp/EfficentNetB0_beans_model_e_227.h5\n",
      "Epoch 228/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8177 - val_sparse_categorical_accuracy: 0.8271\n",
      "\n",
      "Epoch 00228: saving model to tmp/EfficentNetB0_beans_model_e_228.h5\n",
      "Epoch 229/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9966 - val_loss: 1.0451 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00229: saving model to tmp/EfficentNetB0_beans_model_e_229.h5\n",
      "Epoch 230/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0027 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9916 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00230: saving model to tmp/EfficentNetB0_beans_model_e_230.h5\n",
      "Epoch 231/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.9337 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00231: saving model to tmp/EfficentNetB0_beans_model_e_231.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9973 - val_loss: 1.1518 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00232: saving model to tmp/EfficentNetB0_beans_model_e_232.h5\n",
      "Epoch 233/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0107 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.3647 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00233: saving model to tmp/EfficentNetB0_beans_model_e_233.h5\n",
      "Epoch 234/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0699 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00234: saving model to tmp/EfficentNetB0_beans_model_e_234.h5\n",
      "Epoch 235/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.9702 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00235: saving model to tmp/EfficentNetB0_beans_model_e_235.h5\n",
      "Epoch 236/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.9613 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00236: saving model to tmp/EfficentNetB0_beans_model_e_236.h5\n",
      "Epoch 237/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0010 - sparse_categorical_accuracy: 0.9999 - val_loss: 0.8897 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00237: saving model to tmp/EfficentNetB0_beans_model_e_237.h5\n",
      "Epoch 238/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0038 - sparse_categorical_accuracy: 0.9979 - val_loss: 1.0091 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00238: saving model to tmp/EfficentNetB0_beans_model_e_238.h5\n",
      "Epoch 239/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0033 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9742 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00239: saving model to tmp/EfficentNetB0_beans_model_e_239.h5\n",
      "Epoch 240/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9514 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00240: saving model to tmp/EfficentNetB0_beans_model_e_240.h5\n",
      "Epoch 241/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 3.3923e-04 - sparse_categorical_accuracy: 0.9999 - val_loss: 1.0401 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00241: saving model to tmp/EfficentNetB0_beans_model_e_241.h5\n",
      "Epoch 242/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9955 - val_loss: 1.5873 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00242: saving model to tmp/EfficentNetB0_beans_model_e_242.h5\n",
      "Epoch 243/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0261 - sparse_categorical_accuracy: 0.9927 - val_loss: 1.1243 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00243: saving model to tmp/EfficentNetB0_beans_model_e_243.h5\n",
      "Epoch 244/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0098 - sparse_categorical_accuracy: 0.9961 - val_loss: 1.3368 - val_sparse_categorical_accuracy: 0.7293\n",
      "\n",
      "Epoch 00244: saving model to tmp/EfficentNetB0_beans_model_e_244.h5\n",
      "Epoch 245/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0675 - sparse_categorical_accuracy: 0.9837 - val_loss: 5.6747 - val_sparse_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00245: saving model to tmp/EfficentNetB0_beans_model_e_245.h5\n",
      "Epoch 246/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9902 - val_loss: 2.0582 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00246: saving model to tmp/EfficentNetB0_beans_model_e_246.h5\n",
      "Epoch 247/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9887 - val_loss: 1.4710 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00247: saving model to tmp/EfficentNetB0_beans_model_e_247.h5\n",
      "Epoch 248/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9951 - val_loss: 1.1833 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00248: saving model to tmp/EfficentNetB0_beans_model_e_248.h5\n",
      "Epoch 249/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.7531 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00249: saving model to tmp/EfficentNetB0_beans_model_e_249.h5\n",
      "Epoch 250/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.9002 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00250: saving model to tmp/EfficentNetB0_beans_model_e_250.h5\n",
      "Epoch 251/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8443 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00251: saving model to tmp/EfficentNetB0_beans_model_e_251.h5\n",
      "Epoch 252/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.7670 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00252: saving model to tmp/EfficentNetB0_beans_model_e_252.h5\n",
      "Epoch 253/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9987 - val_loss: 1.3110 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00253: saving model to tmp/EfficentNetB0_beans_model_e_253.h5\n",
      "Epoch 254/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.6229 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00254: saving model to tmp/EfficentNetB0_beans_model_e_254.h5\n",
      "Epoch 255/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9996 - val_loss: 1.3049 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00255: saving model to tmp/EfficentNetB0_beans_model_e_255.h5\n",
      "Epoch 256/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9992 - val_loss: 1.0524 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00256: saving model to tmp/EfficentNetB0_beans_model_e_256.h5\n",
      "Epoch 257/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7829 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00257: saving model to tmp/EfficentNetB0_beans_model_e_257.h5\n",
      "Epoch 258/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 3.0263e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7186 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00258: saving model to tmp/EfficentNetB0_beans_model_e_258.h5\n",
      "Epoch 259/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 6.8182e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7623 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00259: saving model to tmp/EfficentNetB0_beans_model_e_259.h5\n",
      "Epoch 260/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9986 - val_loss: 0.7218 - val_sparse_categorical_accuracy: 0.8346\n",
      "\n",
      "Epoch 00260: saving model to tmp/EfficentNetB0_beans_model_e_260.h5\n",
      "Epoch 261/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0021 - sparse_categorical_accuracy: 0.9988 - val_loss: 1.0498 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00261: saving model to tmp/EfficentNetB0_beans_model_e_261.h5\n",
      "Epoch 262/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.9793 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00262: saving model to tmp/EfficentNetB0_beans_model_e_262.h5\n",
      "Epoch 263/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0279 - sparse_categorical_accuracy: 0.9926 - val_loss: 1.4400 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00263: saving model to tmp/EfficentNetB0_beans_model_e_263.h5\n",
      "Epoch 264/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9958 - val_loss: 1.5507 - val_sparse_categorical_accuracy: 0.7669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00264: saving model to tmp/EfficentNetB0_beans_model_e_264.h5\n",
      "Epoch 265/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9943 - val_loss: 1.4264 - val_sparse_categorical_accuracy: 0.6992\n",
      "\n",
      "Epoch 00265: saving model to tmp/EfficentNetB0_beans_model_e_265.h5\n",
      "Epoch 266/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9959 - val_loss: 1.2413 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00266: saving model to tmp/EfficentNetB0_beans_model_e_266.h5\n",
      "Epoch 267/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9969 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00267: saving model to tmp/EfficentNetB0_beans_model_e_267.h5\n",
      "Epoch 268/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9056 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00268: saving model to tmp/EfficentNetB0_beans_model_e_268.h5\n",
      "Epoch 269/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.8099 - val_sparse_categorical_accuracy: 0.8045\n",
      "\n",
      "Epoch 00269: saving model to tmp/EfficentNetB0_beans_model_e_269.h5\n",
      "Epoch 270/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7709 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00270: saving model to tmp/EfficentNetB0_beans_model_e_270.h5\n",
      "Epoch 271/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 2.8986e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7744 - val_sparse_categorical_accuracy: 0.7970\n",
      "\n",
      "Epoch 00271: saving model to tmp/EfficentNetB0_beans_model_e_271.h5\n",
      "Epoch 272/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 4.5418e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7941 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00272: saving model to tmp/EfficentNetB0_beans_model_e_272.h5\n",
      "Epoch 273/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 2.4503e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8080 - val_sparse_categorical_accuracy: 0.8120\n",
      "\n",
      "Epoch 00273: saving model to tmp/EfficentNetB0_beans_model_e_273.h5\n",
      "Epoch 274/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0038 - sparse_categorical_accuracy: 0.9971 - val_loss: 1.0112 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00274: saving model to tmp/EfficentNetB0_beans_model_e_274.h5\n",
      "Epoch 275/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 8.9265e-04 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.8936 - val_sparse_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00275: saving model to tmp/EfficentNetB0_beans_model_e_275.h5\n",
      "Epoch 276/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 4.6694e-04 - sparse_categorical_accuracy: 0.9999 - val_loss: 0.9080 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00276: saving model to tmp/EfficentNetB0_beans_model_e_276.h5\n",
      "Epoch 277/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1108 - val_sparse_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00277: saving model to tmp/EfficentNetB0_beans_model_e_277.h5\n",
      "Epoch 278/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 1.8857e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0149 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00278: saving model to tmp/EfficentNetB0_beans_model_e_278.h5\n",
      "Epoch 279/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9960 - val_loss: 1.2283 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00279: saving model to tmp/EfficentNetB0_beans_model_e_279.h5\n",
      "Epoch 280/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9958 - val_loss: 1.2727 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00280: saving model to tmp/EfficentNetB0_beans_model_e_280.h5\n",
      "Epoch 281/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9867 - val_loss: 1.8004 - val_sparse_categorical_accuracy: 0.7143\n",
      "\n",
      "Epoch 00281: saving model to tmp/EfficentNetB0_beans_model_e_281.h5\n",
      "Epoch 282/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0395 - sparse_categorical_accuracy: 0.9882 - val_loss: 1.5705 - val_sparse_categorical_accuracy: 0.7218\n",
      "\n",
      "Epoch 00282: saving model to tmp/EfficentNetB0_beans_model_e_282.h5\n",
      "Epoch 283/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9896 - val_loss: 2.3651 - val_sparse_categorical_accuracy: 0.6391\n",
      "\n",
      "Epoch 00283: saving model to tmp/EfficentNetB0_beans_model_e_283.h5\n",
      "Epoch 284/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9979 - val_loss: 1.4567 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00284: saving model to tmp/EfficentNetB0_beans_model_e_284.h5\n",
      "Epoch 285/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9956 - val_loss: 1.3251 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00285: saving model to tmp/EfficentNetB0_beans_model_e_285.h5\n",
      "Epoch 286/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9988 - val_loss: 1.2764 - val_sparse_categorical_accuracy: 0.7368\n",
      "\n",
      "Epoch 00286: saving model to tmp/EfficentNetB0_beans_model_e_286.h5\n",
      "Epoch 287/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.9598 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00287: saving model to tmp/EfficentNetB0_beans_model_e_287.h5\n",
      "Epoch 288/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.8542 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00288: saving model to tmp/EfficentNetB0_beans_model_e_288.h5\n",
      "Epoch 289/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.8741 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00289: saving model to tmp/EfficentNetB0_beans_model_e_289.h5\n",
      "Epoch 290/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9997 - val_loss: 0.9027 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00290: saving model to tmp/EfficentNetB0_beans_model_e_290.h5\n",
      "Epoch 291/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.8099 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00291: saving model to tmp/EfficentNetB0_beans_model_e_291.h5\n",
      "Epoch 292/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.7740 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00292: saving model to tmp/EfficentNetB0_beans_model_e_292.h5\n",
      "Epoch 293/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.8239 - val_sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "Epoch 00293: saving model to tmp/EfficentNetB0_beans_model_e_293.h5\n",
      "Epoch 294/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9950 - val_loss: 1.0473 - val_sparse_categorical_accuracy: 0.7444\n",
      "\n",
      "Epoch 00294: saving model to tmp/EfficentNetB0_beans_model_e_294.h5\n",
      "Epoch 295/300\n",
      "33/33 [==============================] - 45s 1s/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9690 - val_sparse_categorical_accuracy: 0.7895\n",
      "\n",
      "Epoch 00295: saving model to tmp/EfficentNetB0_beans_model_e_295.h5\n",
      "Epoch 296/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8479 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00296: saving model to tmp/EfficentNetB0_beans_model_e_296.h5\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 45s 1s/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.8668 - val_sparse_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00297: saving model to tmp/EfficentNetB0_beans_model_e_297.h5\n",
      "Epoch 298/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9989 - val_loss: 1.1285 - val_sparse_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00298: saving model to tmp/EfficentNetB0_beans_model_e_298.h5\n",
      "Epoch 299/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9983 - val_loss: 1.1546 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00299: saving model to tmp/EfficentNetB0_beans_model_e_299.h5\n",
      "Epoch 300/300\n",
      "33/33 [==============================] - 44s 1s/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.8381 - val_sparse_categorical_accuracy: 0.8195\n",
      "\n",
      "Epoch 00300: saving model to tmp/EfficentNetB0_beans_model_e_300.h5\n",
      "Saved to: EfficentNetB0_beans_model.h5\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB0(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'EfficentNetB4_beans_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[10752] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_2/block3c_se_squeeze/Mean (defined at <ipython-input-3-66930933bfe0>:24) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_922807]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4cebf2a8a683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_model_for_beans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-66930933bfe0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, ds_train, ds_validation, model_name, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                     mode='auto')\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     hist = model.fit(ds_train, \n\u001b[0m\u001b[1;32m     25\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[10752] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_2/block3c_se_squeeze/Mean (defined at <ipython-input-3-66930933bfe0>:24) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_922807]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB4(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flowers dataset\n",
    "\n",
    "https://www.tensorflow.org/datasets/catalog/oxford_flowers102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        image, size=[256, 256, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_flowers_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    return image, label\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess_flowers(image, label):\n",
    "    image = tf.image.resize(image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrap model for flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model_for_flowers(base_model, num_classes):\n",
    "    inputs = base_model.inputs\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 102\n",
    "INPUT_SHAPE = (256, 256, 3)\n",
    "BATCH_SIZE = 32\n",
    "RESIZE_DIMENSION = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flowers_dataset():  \n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(name=\"tf_flowers\", \n",
    "                                             with_info=True,\n",
    "                                             split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],  #70/15/15 split\n",
    "                                             as_supervised=True)\n",
    "\n",
    "    ds_train = ds_train.map(normalize, \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)    \n",
    "    ds_train = ds_train.map(preprocess_flowers)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE)\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize, \n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_validation = ds_validation.map(preprocess_flowers)\n",
    "    ds_validation = ds_validation.batch(BATCH_SIZE)\n",
    "    ds_validation = ds_validation.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize, \n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_test = ds_test.map(preprocess_flowers)\n",
    "    ds_test = ds_test.batch(BATCH_SIZE)\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_validation, ds_test = load_flowers_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'MobileNetV2_flowers_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "81/81 [==============================] - 34s 236ms/step - loss: 2.3661 - sparse_categorical_accuracy: 0.3575 - val_loss: 4.3391 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00001: saving model to tmp/MobileNetV2_flowers_model_e_01.h5\n",
      "Epoch 2/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 1.1504 - sparse_categorical_accuracy: 0.5644 - val_loss: 4.0571 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00002: saving model to tmp/MobileNetV2_flowers_model_e_02.h5\n",
      "Epoch 3/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 1.0400 - sparse_categorical_accuracy: 0.5973 - val_loss: 3.7580 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00003: saving model to tmp/MobileNetV2_flowers_model_e_03.h5\n",
      "Epoch 4/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.9275 - sparse_categorical_accuracy: 0.6320 - val_loss: 3.4233 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00004: saving model to tmp/MobileNetV2_flowers_model_e_04.h5\n",
      "Epoch 5/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.8386 - sparse_categorical_accuracy: 0.6863 - val_loss: 3.0715 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00005: saving model to tmp/MobileNetV2_flowers_model_e_05.h5\n",
      "Epoch 6/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.7413 - sparse_categorical_accuracy: 0.7334 - val_loss: 2.7970 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00006: saving model to tmp/MobileNetV2_flowers_model_e_06.h5\n",
      "Epoch 7/300\n",
      "81/81 [==============================] - 28s 223ms/step - loss: 0.6219 - sparse_categorical_accuracy: 0.7709 - val_loss: 2.5740 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00007: saving model to tmp/MobileNetV2_flowers_model_e_07.h5\n",
      "Epoch 8/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.5734 - sparse_categorical_accuracy: 0.7968 - val_loss: 2.4336 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00008: saving model to tmp/MobileNetV2_flowers_model_e_08.h5\n",
      "Epoch 9/300\n",
      "81/81 [==============================] - 28s 223ms/step - loss: 0.5491 - sparse_categorical_accuracy: 0.8022 - val_loss: 2.4147 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00009: saving model to tmp/MobileNetV2_flowers_model_e_09.h5\n",
      "Epoch 10/300\n",
      "81/81 [==============================] - 28s 227ms/step - loss: 0.4533 - sparse_categorical_accuracy: 0.8341 - val_loss: 2.5383 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00010: saving model to tmp/MobileNetV2_flowers_model_e_10.h5\n",
      "Epoch 11/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.4280 - sparse_categorical_accuracy: 0.8384 - val_loss: 2.8333 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00011: saving model to tmp/MobileNetV2_flowers_model_e_11.h5\n",
      "Epoch 12/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.3839 - sparse_categorical_accuracy: 0.8595 - val_loss: 3.0238 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00012: saving model to tmp/MobileNetV2_flowers_model_e_12.h5\n",
      "Epoch 13/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.3169 - sparse_categorical_accuracy: 0.8863 - val_loss: 3.6783 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00013: saving model to tmp/MobileNetV2_flowers_model_e_13.h5\n",
      "Epoch 14/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.2988 - sparse_categorical_accuracy: 0.8812 - val_loss: 4.0624 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00014: saving model to tmp/MobileNetV2_flowers_model_e_14.h5\n",
      "Epoch 15/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.2353 - sparse_categorical_accuracy: 0.9243 - val_loss: 4.4783 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00015: saving model to tmp/MobileNetV2_flowers_model_e_15.h5\n",
      "Epoch 16/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.1932 - sparse_categorical_accuracy: 0.9320 - val_loss: 5.2473 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00016: saving model to tmp/MobileNetV2_flowers_model_e_16.h5\n",
      "Epoch 17/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.2001 - sparse_categorical_accuracy: 0.9299 - val_loss: 5.5310 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00017: saving model to tmp/MobileNetV2_flowers_model_e_17.h5\n",
      "Epoch 18/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.2381 - sparse_categorical_accuracy: 0.9198 - val_loss: 5.3940 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00018: saving model to tmp/MobileNetV2_flowers_model_e_18.h5\n",
      "Epoch 19/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.2454 - sparse_categorical_accuracy: 0.9047 - val_loss: 5.7535 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00019: saving model to tmp/MobileNetV2_flowers_model_e_19.h5\n",
      "Epoch 20/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1539 - sparse_categorical_accuracy: 0.9464 - val_loss: 6.4253 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00020: saving model to tmp/MobileNetV2_flowers_model_e_20.h5\n",
      "Epoch 21/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.1654 - sparse_categorical_accuracy: 0.9476 - val_loss: 6.6589 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00021: saving model to tmp/MobileNetV2_flowers_model_e_21.h5\n",
      "Epoch 22/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1406 - sparse_categorical_accuracy: 0.9545 - val_loss: 6.9113 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00022: saving model to tmp/MobileNetV2_flowers_model_e_22.h5\n",
      "Epoch 23/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.1513 - sparse_categorical_accuracy: 0.9483 - val_loss: 7.1779 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00023: saving model to tmp/MobileNetV2_flowers_model_e_23.h5\n",
      "Epoch 24/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0881 - sparse_categorical_accuracy: 0.9718 - val_loss: 7.5013 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00024: saving model to tmp/MobileNetV2_flowers_model_e_24.h5\n",
      "Epoch 25/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1745 - sparse_categorical_accuracy: 0.9414 - val_loss: 7.7910 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00025: saving model to tmp/MobileNetV2_flowers_model_e_25.h5\n",
      "Epoch 26/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.1511 - sparse_categorical_accuracy: 0.9529 - val_loss: 7.5009 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00026: saving model to tmp/MobileNetV2_flowers_model_e_26.h5\n",
      "Epoch 27/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.1200 - sparse_categorical_accuracy: 0.9559 - val_loss: 8.2301 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00027: saving model to tmp/MobileNetV2_flowers_model_e_27.h5\n",
      "Epoch 28/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.1024 - sparse_categorical_accuracy: 0.9629 - val_loss: 8.1933 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00028: saving model to tmp/MobileNetV2_flowers_model_e_28.h5\n",
      "Epoch 29/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0754 - sparse_categorical_accuracy: 0.9728 - val_loss: 8.6533 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00029: saving model to tmp/MobileNetV2_flowers_model_e_29.h5\n",
      "Epoch 30/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0780 - sparse_categorical_accuracy: 0.9722 - val_loss: 8.6180 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00030: saving model to tmp/MobileNetV2_flowers_model_e_30.h5\n",
      "Epoch 31/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1528 - sparse_categorical_accuracy: 0.9470 - val_loss: 8.4707 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00031: saving model to tmp/MobileNetV2_flowers_model_e_31.h5\n",
      "Epoch 32/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.1399 - sparse_categorical_accuracy: 0.9494 - val_loss: 9.0170 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00032: saving model to tmp/MobileNetV2_flowers_model_e_32.h5\n",
      "Epoch 33/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1448 - sparse_categorical_accuracy: 0.9552 - val_loss: 9.0791 - val_sparse_categorical_accuracy: 0.2668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: saving model to tmp/MobileNetV2_flowers_model_e_33.h5\n",
      "Epoch 34/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9454 - val_loss: 8.5937 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00034: saving model to tmp/MobileNetV2_flowers_model_e_34.h5\n",
      "Epoch 35/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1054 - sparse_categorical_accuracy: 0.9648 - val_loss: 9.5211 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00035: saving model to tmp/MobileNetV2_flowers_model_e_35.h5\n",
      "Epoch 36/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9650 - val_loss: 9.7898 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00036: saving model to tmp/MobileNetV2_flowers_model_e_36.h5\n",
      "Epoch 37/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9522 - val_loss: 9.4237 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00037: saving model to tmp/MobileNetV2_flowers_model_e_37.h5\n",
      "Epoch 38/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9686 - val_loss: 10.4046 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00038: saving model to tmp/MobileNetV2_flowers_model_e_38.h5\n",
      "Epoch 39/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0916 - sparse_categorical_accuracy: 0.9685 - val_loss: 10.3394 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00039: saving model to tmp/MobileNetV2_flowers_model_e_39.h5\n",
      "Epoch 40/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.1236 - sparse_categorical_accuracy: 0.9521 - val_loss: 11.4813 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00040: saving model to tmp/MobileNetV2_flowers_model_e_40.h5\n",
      "Epoch 41/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.1117 - sparse_categorical_accuracy: 0.9648 - val_loss: 10.7383 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00041: saving model to tmp/MobileNetV2_flowers_model_e_41.h5\n",
      "Epoch 42/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9675 - val_loss: 10.6787 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00042: saving model to tmp/MobileNetV2_flowers_model_e_42.h5\n",
      "Epoch 43/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0808 - sparse_categorical_accuracy: 0.9720 - val_loss: 11.3021 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00043: saving model to tmp/MobileNetV2_flowers_model_e_43.h5\n",
      "Epoch 44/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9783 - val_loss: 10.9296 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00044: saving model to tmp/MobileNetV2_flowers_model_e_44.h5\n",
      "Epoch 45/300\n",
      "81/81 [==============================] - 28s 223ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9719 - val_loss: 10.3650 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00045: saving model to tmp/MobileNetV2_flowers_model_e_45.h5\n",
      "Epoch 46/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0800 - sparse_categorical_accuracy: 0.9713 - val_loss: 10.6344 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00046: saving model to tmp/MobileNetV2_flowers_model_e_46.h5\n",
      "Epoch 47/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0957 - sparse_categorical_accuracy: 0.9651 - val_loss: 10.3125 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00047: saving model to tmp/MobileNetV2_flowers_model_e_47.h5\n",
      "Epoch 48/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0661 - sparse_categorical_accuracy: 0.9796 - val_loss: 10.7199 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00048: saving model to tmp/MobileNetV2_flowers_model_e_48.h5\n",
      "Epoch 49/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9816 - val_loss: 11.1027 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00049: saving model to tmp/MobileNetV2_flowers_model_e_49.h5\n",
      "Epoch 50/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.1200 - sparse_categorical_accuracy: 0.9582 - val_loss: 10.2958 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00050: saving model to tmp/MobileNetV2_flowers_model_e_50.h5\n",
      "Epoch 51/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.1160 - sparse_categorical_accuracy: 0.9649 - val_loss: 10.0588 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00051: saving model to tmp/MobileNetV2_flowers_model_e_51.h5\n",
      "Epoch 52/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0763 - sparse_categorical_accuracy: 0.9750 - val_loss: 10.3334 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00052: saving model to tmp/MobileNetV2_flowers_model_e_52.h5\n",
      "Epoch 53/300\n",
      "81/81 [==============================] - 29s 228ms/step - loss: 0.0736 - sparse_categorical_accuracy: 0.9769 - val_loss: 11.4789 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00053: saving model to tmp/MobileNetV2_flowers_model_e_53.h5\n",
      "Epoch 54/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0836 - sparse_categorical_accuracy: 0.9727 - val_loss: 11.7535 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00054: saving model to tmp/MobileNetV2_flowers_model_e_54.h5\n",
      "Epoch 55/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9820 - val_loss: 11.7712 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00055: saving model to tmp/MobileNetV2_flowers_model_e_55.h5\n",
      "Epoch 56/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0319 - sparse_categorical_accuracy: 0.9900 - val_loss: 12.6343 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00056: saving model to tmp/MobileNetV2_flowers_model_e_56.h5\n",
      "Epoch 57/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0563 - sparse_categorical_accuracy: 0.9781 - val_loss: 11.9280 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00057: saving model to tmp/MobileNetV2_flowers_model_e_57.h5\n",
      "Epoch 58/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0681 - sparse_categorical_accuracy: 0.9733 - val_loss: 13.3288 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00058: saving model to tmp/MobileNetV2_flowers_model_e_58.h5\n",
      "Epoch 59/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0781 - sparse_categorical_accuracy: 0.9729 - val_loss: 12.0435 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00059: saving model to tmp/MobileNetV2_flowers_model_e_59.h5\n",
      "Epoch 60/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0938 - sparse_categorical_accuracy: 0.9701 - val_loss: 13.4689 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00060: saving model to tmp/MobileNetV2_flowers_model_e_60.h5\n",
      "Epoch 61/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0788 - sparse_categorical_accuracy: 0.9699 - val_loss: 11.4484 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00061: saving model to tmp/MobileNetV2_flowers_model_e_61.h5\n",
      "Epoch 62/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0513 - sparse_categorical_accuracy: 0.9840 - val_loss: 12.2762 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00062: saving model to tmp/MobileNetV2_flowers_model_e_62.h5\n",
      "Epoch 63/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0500 - sparse_categorical_accuracy: 0.9838 - val_loss: 11.2797 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00063: saving model to tmp/MobileNetV2_flowers_model_e_63.h5\n",
      "Epoch 64/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0460 - sparse_categorical_accuracy: 0.9847 - val_loss: 11.2764 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00064: saving model to tmp/MobileNetV2_flowers_model_e_64.h5\n",
      "Epoch 65/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0376 - sparse_categorical_accuracy: 0.9883 - val_loss: 11.1794 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00065: saving model to tmp/MobileNetV2_flowers_model_e_65.h5\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9817 - val_loss: 12.0353 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00066: saving model to tmp/MobileNetV2_flowers_model_e_66.h5\n",
      "Epoch 67/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9916 - val_loss: 12.5005 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00067: saving model to tmp/MobileNetV2_flowers_model_e_67.h5\n",
      "Epoch 68/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9860 - val_loss: 12.2187 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00068: saving model to tmp/MobileNetV2_flowers_model_e_68.h5\n",
      "Epoch 69/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.1067 - sparse_categorical_accuracy: 0.9708 - val_loss: 11.8989 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00069: saving model to tmp/MobileNetV2_flowers_model_e_69.h5\n",
      "Epoch 70/300\n",
      "81/81 [==============================] - 30s 222ms/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9634 - val_loss: 13.5446 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00070: saving model to tmp/MobileNetV2_flowers_model_e_70.h5\n",
      "Epoch 71/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0675 - sparse_categorical_accuracy: 0.9783 - val_loss: 13.3028 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00071: saving model to tmp/MobileNetV2_flowers_model_e_71.h5\n",
      "Epoch 72/300\n",
      "81/81 [==============================] - 30s 222ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9761 - val_loss: 12.4658 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00072: saving model to tmp/MobileNetV2_flowers_model_e_72.h5\n",
      "Epoch 73/300\n",
      "81/81 [==============================] - 29s 220ms/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9843 - val_loss: 11.4955 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00073: saving model to tmp/MobileNetV2_flowers_model_e_73.h5\n",
      "Epoch 74/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0351 - sparse_categorical_accuracy: 0.9873 - val_loss: 12.3363 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00074: saving model to tmp/MobileNetV2_flowers_model_e_74.h5\n",
      "Epoch 75/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0675 - sparse_categorical_accuracy: 0.9774 - val_loss: 11.5260 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00075: saving model to tmp/MobileNetV2_flowers_model_e_75.h5\n",
      "Epoch 76/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0596 - sparse_categorical_accuracy: 0.9792 - val_loss: 11.8633 - val_sparse_categorical_accuracy: 0.2686\n",
      "\n",
      "Epoch 00076: saving model to tmp/MobileNetV2_flowers_model_e_76.h5\n",
      "Epoch 77/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9833 - val_loss: 8.4599 - val_sparse_categorical_accuracy: 0.2958\n",
      "\n",
      "Epoch 00077: saving model to tmp/MobileNetV2_flowers_model_e_77.h5\n",
      "Epoch 78/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0333 - sparse_categorical_accuracy: 0.9880 - val_loss: 6.5124 - val_sparse_categorical_accuracy: 0.3339\n",
      "\n",
      "Epoch 00078: saving model to tmp/MobileNetV2_flowers_model_e_78.h5\n",
      "Epoch 79/300\n",
      "81/81 [==============================] - 29s 221ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9892 - val_loss: 6.4493 - val_sparse_categorical_accuracy: 0.3466\n",
      "\n",
      "Epoch 00079: saving model to tmp/MobileNetV2_flowers_model_e_79.h5\n",
      "Epoch 80/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9695 - val_loss: 4.4416 - val_sparse_categorical_accuracy: 0.4737\n",
      "\n",
      "Epoch 00080: saving model to tmp/MobileNetV2_flowers_model_e_80.h5\n",
      "Epoch 81/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0758 - sparse_categorical_accuracy: 0.9724 - val_loss: 3.9414 - val_sparse_categorical_accuracy: 0.4719\n",
      "\n",
      "Epoch 00081: saving model to tmp/MobileNetV2_flowers_model_e_81.h5\n",
      "Epoch 82/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.9868 - val_loss: 3.1626 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00082: saving model to tmp/MobileNetV2_flowers_model_e_82.h5\n",
      "Epoch 83/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9928 - val_loss: 2.4473 - val_sparse_categorical_accuracy: 0.6134\n",
      "\n",
      "Epoch 00083: saving model to tmp/MobileNetV2_flowers_model_e_83.h5\n",
      "Epoch 84/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.0867 - val_sparse_categorical_accuracy: 0.6515\n",
      "\n",
      "Epoch 00084: saving model to tmp/MobileNetV2_flowers_model_e_84.h5\n",
      "Epoch 85/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0239 - sparse_categorical_accuracy: 0.9941 - val_loss: 3.0627 - val_sparse_categorical_accuracy: 0.5372\n",
      "\n",
      "Epoch 00085: saving model to tmp/MobileNetV2_flowers_model_e_85.h5\n",
      "Epoch 86/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9917 - val_loss: 2.4501 - val_sparse_categorical_accuracy: 0.6080\n",
      "\n",
      "Epoch 00086: saving model to tmp/MobileNetV2_flowers_model_e_86.h5\n",
      "Epoch 87/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9871 - val_loss: 2.2988 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00087: saving model to tmp/MobileNetV2_flowers_model_e_87.h5\n",
      "Epoch 88/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9888 - val_loss: 2.3796 - val_sparse_categorical_accuracy: 0.6098\n",
      "\n",
      "Epoch 00088: saving model to tmp/MobileNetV2_flowers_model_e_88.h5\n",
      "Epoch 89/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0576 - sparse_categorical_accuracy: 0.9749 - val_loss: 3.7970 - val_sparse_categorical_accuracy: 0.5445\n",
      "\n",
      "Epoch 00089: saving model to tmp/MobileNetV2_flowers_model_e_89.h5\n",
      "Epoch 90/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9643 - val_loss: 3.9889 - val_sparse_categorical_accuracy: 0.5572\n",
      "\n",
      "Epoch 00090: saving model to tmp/MobileNetV2_flowers_model_e_90.h5\n",
      "Epoch 91/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.1311 - sparse_categorical_accuracy: 0.9605 - val_loss: 3.3764 - val_sparse_categorical_accuracy: 0.6116\n",
      "\n",
      "Epoch 00091: saving model to tmp/MobileNetV2_flowers_model_e_91.h5\n",
      "Epoch 92/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0850 - sparse_categorical_accuracy: 0.9749 - val_loss: 2.5084 - val_sparse_categorical_accuracy: 0.7024\n",
      "\n",
      "Epoch 00092: saving model to tmp/MobileNetV2_flowers_model_e_92.h5\n",
      "Epoch 93/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0575 - sparse_categorical_accuracy: 0.9784 - val_loss: 5.4419 - val_sparse_categorical_accuracy: 0.5336\n",
      "\n",
      "Epoch 00093: saving model to tmp/MobileNetV2_flowers_model_e_93.h5\n",
      "Epoch 94/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0539 - sparse_categorical_accuracy: 0.9825 - val_loss: 4.2389 - val_sparse_categorical_accuracy: 0.5263\n",
      "\n",
      "Epoch 00094: saving model to tmp/MobileNetV2_flowers_model_e_94.h5\n",
      "Epoch 95/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0292 - sparse_categorical_accuracy: 0.9899 - val_loss: 3.3498 - val_sparse_categorical_accuracy: 0.5953\n",
      "\n",
      "Epoch 00095: saving model to tmp/MobileNetV2_flowers_model_e_95.h5\n",
      "Epoch 96/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9858 - val_loss: 2.9953 - val_sparse_categorical_accuracy: 0.6080\n",
      "\n",
      "Epoch 00096: saving model to tmp/MobileNetV2_flowers_model_e_96.h5\n",
      "Epoch 97/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9901 - val_loss: 3.2796 - val_sparse_categorical_accuracy: 0.6116\n",
      "\n",
      "Epoch 00097: saving model to tmp/MobileNetV2_flowers_model_e_97.h5\n",
      "Epoch 98/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9967 - val_loss: 2.0122 - val_sparse_categorical_accuracy: 0.6661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00098: saving model to tmp/MobileNetV2_flowers_model_e_98.h5\n",
      "Epoch 99/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9978 - val_loss: 2.8886 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00099: saving model to tmp/MobileNetV2_flowers_model_e_99.h5\n",
      "Epoch 100/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0925 - sparse_categorical_accuracy: 0.9663 - val_loss: 5.1146 - val_sparse_categorical_accuracy: 0.5154\n",
      "\n",
      "Epoch 00100: saving model to tmp/MobileNetV2_flowers_model_e_100.h5\n",
      "Epoch 101/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9788 - val_loss: 4.5641 - val_sparse_categorical_accuracy: 0.5263\n",
      "\n",
      "Epoch 00101: saving model to tmp/MobileNetV2_flowers_model_e_101.h5\n",
      "Epoch 102/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0671 - sparse_categorical_accuracy: 0.9788 - val_loss: 4.6656 - val_sparse_categorical_accuracy: 0.5445\n",
      "\n",
      "Epoch 00102: saving model to tmp/MobileNetV2_flowers_model_e_102.h5\n",
      "Epoch 103/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0586 - sparse_categorical_accuracy: 0.9788 - val_loss: 4.7734 - val_sparse_categorical_accuracy: 0.5499\n",
      "\n",
      "Epoch 00103: saving model to tmp/MobileNetV2_flowers_model_e_103.h5\n",
      "Epoch 104/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0594 - sparse_categorical_accuracy: 0.9798 - val_loss: 4.9785 - val_sparse_categorical_accuracy: 0.5753\n",
      "\n",
      "Epoch 00104: saving model to tmp/MobileNetV2_flowers_model_e_104.h5\n",
      "Epoch 105/300\n",
      "81/81 [==============================] - 28s 223ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9914 - val_loss: 7.1264 - val_sparse_categorical_accuracy: 0.4719\n",
      "\n",
      "Epoch 00105: saving model to tmp/MobileNetV2_flowers_model_e_105.h5\n",
      "Epoch 106/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9961 - val_loss: 6.0837 - val_sparse_categorical_accuracy: 0.5154\n",
      "\n",
      "Epoch 00106: saving model to tmp/MobileNetV2_flowers_model_e_106.h5\n",
      "Epoch 107/300\n",
      "81/81 [==============================] - 30s 228ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9975 - val_loss: 5.8805 - val_sparse_categorical_accuracy: 0.4973\n",
      "\n",
      "Epoch 00107: saving model to tmp/MobileNetV2_flowers_model_e_107.h5\n",
      "Epoch 108/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9907 - val_loss: 4.0863 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00108: saving model to tmp/MobileNetV2_flowers_model_e_108.h5\n",
      "Epoch 109/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9892 - val_loss: 5.3304 - val_sparse_categorical_accuracy: 0.5172\n",
      "\n",
      "Epoch 00109: saving model to tmp/MobileNetV2_flowers_model_e_109.h5\n",
      "Epoch 110/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9880 - val_loss: 4.5390 - val_sparse_categorical_accuracy: 0.5445\n",
      "\n",
      "Epoch 00110: saving model to tmp/MobileNetV2_flowers_model_e_110.h5\n",
      "Epoch 111/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.9895 - val_loss: 3.3399 - val_sparse_categorical_accuracy: 0.5971\n",
      "\n",
      "Epoch 00111: saving model to tmp/MobileNetV2_flowers_model_e_111.h5\n",
      "Epoch 112/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0306 - sparse_categorical_accuracy: 0.9906 - val_loss: 7.3582 - val_sparse_categorical_accuracy: 0.4537\n",
      "\n",
      "Epoch 00112: saving model to tmp/MobileNetV2_flowers_model_e_112.h5\n",
      "Epoch 113/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0935 - sparse_categorical_accuracy: 0.9690 - val_loss: 6.2696 - val_sparse_categorical_accuracy: 0.4955\n",
      "\n",
      "Epoch 00113: saving model to tmp/MobileNetV2_flowers_model_e_113.h5\n",
      "Epoch 114/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.3257 - val_sparse_categorical_accuracy: 0.5753\n",
      "\n",
      "Epoch 00114: saving model to tmp/MobileNetV2_flowers_model_e_114.h5\n",
      "Epoch 115/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0495 - sparse_categorical_accuracy: 0.9838 - val_loss: 5.5931 - val_sparse_categorical_accuracy: 0.5191\n",
      "\n",
      "Epoch 00115: saving model to tmp/MobileNetV2_flowers_model_e_115.h5\n",
      "Epoch 116/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0849 - sparse_categorical_accuracy: 0.9719 - val_loss: 11.5019 - val_sparse_categorical_accuracy: 0.3884\n",
      "\n",
      "Epoch 00116: saving model to tmp/MobileNetV2_flowers_model_e_116.h5\n",
      "Epoch 117/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9667 - val_loss: 10.2966 - val_sparse_categorical_accuracy: 0.4211\n",
      "\n",
      "Epoch 00117: saving model to tmp/MobileNetV2_flowers_model_e_117.h5\n",
      "Epoch 118/300\n",
      "81/81 [==============================] - 28s 225ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9785 - val_loss: 6.7335 - val_sparse_categorical_accuracy: 0.5209\n",
      "\n",
      "Epoch 00118: saving model to tmp/MobileNetV2_flowers_model_e_118.h5\n",
      "Epoch 119/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9848 - val_loss: 4.6903 - val_sparse_categorical_accuracy: 0.5898\n",
      "\n",
      "Epoch 00119: saving model to tmp/MobileNetV2_flowers_model_e_119.h5\n",
      "Epoch 120/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9954 - val_loss: 5.0913 - val_sparse_categorical_accuracy: 0.5463\n",
      "\n",
      "Epoch 00120: saving model to tmp/MobileNetV2_flowers_model_e_120.h5\n",
      "Epoch 121/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0375 - sparse_categorical_accuracy: 0.9865 - val_loss: 2.7790 - val_sparse_categorical_accuracy: 0.6570\n",
      "\n",
      "Epoch 00121: saving model to tmp/MobileNetV2_flowers_model_e_121.h5\n",
      "Epoch 122/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.9840 - val_loss: 4.0642 - val_sparse_categorical_accuracy: 0.5771\n",
      "\n",
      "Epoch 00122: saving model to tmp/MobileNetV2_flowers_model_e_122.h5\n",
      "Epoch 123/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0280 - sparse_categorical_accuracy: 0.9922 - val_loss: 5.9328 - val_sparse_categorical_accuracy: 0.5717\n",
      "\n",
      "Epoch 00123: saving model to tmp/MobileNetV2_flowers_model_e_123.h5\n",
      "Epoch 124/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9838 - val_loss: 5.1740 - val_sparse_categorical_accuracy: 0.5699\n",
      "\n",
      "Epoch 00124: saving model to tmp/MobileNetV2_flowers_model_e_124.h5\n",
      "Epoch 125/300\n",
      "81/81 [==============================] - 29s 221ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9962 - val_loss: 3.7934 - val_sparse_categorical_accuracy: 0.5626\n",
      "\n",
      "Epoch 00125: saving model to tmp/MobileNetV2_flowers_model_e_125.h5\n",
      "Epoch 126/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9760 - val_loss: 4.6323 - val_sparse_categorical_accuracy: 0.5771\n",
      "\n",
      "Epoch 00126: saving model to tmp/MobileNetV2_flowers_model_e_126.h5\n",
      "Epoch 127/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0497 - sparse_categorical_accuracy: 0.9814 - val_loss: 5.1344 - val_sparse_categorical_accuracy: 0.5281\n",
      "\n",
      "Epoch 00127: saving model to tmp/MobileNetV2_flowers_model_e_127.h5\n",
      "Epoch 128/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9847 - val_loss: 5.3657 - val_sparse_categorical_accuracy: 0.5136\n",
      "\n",
      "Epoch 00128: saving model to tmp/MobileNetV2_flowers_model_e_128.h5\n",
      "Epoch 129/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9879 - val_loss: 7.6183 - val_sparse_categorical_accuracy: 0.4555\n",
      "\n",
      "Epoch 00129: saving model to tmp/MobileNetV2_flowers_model_e_129.h5\n",
      "Epoch 130/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9888 - val_loss: 5.2698 - val_sparse_categorical_accuracy: 0.5372\n",
      "\n",
      "Epoch 00130: saving model to tmp/MobileNetV2_flowers_model_e_130.h5\n",
      "Epoch 131/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9940 - val_loss: 4.6065 - val_sparse_categorical_accuracy: 0.5753\n",
      "\n",
      "Epoch 00131: saving model to tmp/MobileNetV2_flowers_model_e_131.h5\n",
      "Epoch 132/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9934 - val_loss: 4.9787 - val_sparse_categorical_accuracy: 0.5681\n",
      "\n",
      "Epoch 00132: saving model to tmp/MobileNetV2_flowers_model_e_132.h5\n",
      "Epoch 133/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.8844 - val_sparse_categorical_accuracy: 0.6334\n",
      "\n",
      "Epoch 00133: saving model to tmp/MobileNetV2_flowers_model_e_133.h5\n",
      "Epoch 134/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9961 - val_loss: 4.9554 - val_sparse_categorical_accuracy: 0.5064\n",
      "\n",
      "Epoch 00134: saving model to tmp/MobileNetV2_flowers_model_e_134.h5\n",
      "Epoch 135/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9896 - val_loss: 5.0002 - val_sparse_categorical_accuracy: 0.5245\n",
      "\n",
      "Epoch 00135: saving model to tmp/MobileNetV2_flowers_model_e_135.h5\n",
      "Epoch 136/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9923 - val_loss: 8.7162 - val_sparse_categorical_accuracy: 0.4211\n",
      "\n",
      "Epoch 00136: saving model to tmp/MobileNetV2_flowers_model_e_136.h5\n",
      "Epoch 137/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0389 - sparse_categorical_accuracy: 0.9838 - val_loss: 9.6460 - val_sparse_categorical_accuracy: 0.4338\n",
      "\n",
      "Epoch 00137: saving model to tmp/MobileNetV2_flowers_model_e_137.h5\n",
      "Epoch 138/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9864 - val_loss: 6.2826 - val_sparse_categorical_accuracy: 0.4991\n",
      "\n",
      "Epoch 00138: saving model to tmp/MobileNetV2_flowers_model_e_138.h5\n",
      "Epoch 139/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0360 - sparse_categorical_accuracy: 0.9856 - val_loss: 6.2516 - val_sparse_categorical_accuracy: 0.4973\n",
      "\n",
      "Epoch 00139: saving model to tmp/MobileNetV2_flowers_model_e_139.h5\n",
      "Epoch 140/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0508 - sparse_categorical_accuracy: 0.9790 - val_loss: 6.3554 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00140: saving model to tmp/MobileNetV2_flowers_model_e_140.h5\n",
      "Epoch 141/300\n",
      "81/81 [==============================] - 29s 221ms/step - loss: 0.0277 - sparse_categorical_accuracy: 0.9903 - val_loss: 7.1540 - val_sparse_categorical_accuracy: 0.5245\n",
      "\n",
      "Epoch 00141: saving model to tmp/MobileNetV2_flowers_model_e_141.h5\n",
      "Epoch 142/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9951 - val_loss: 5.3130 - val_sparse_categorical_accuracy: 0.4864\n",
      "\n",
      "Epoch 00142: saving model to tmp/MobileNetV2_flowers_model_e_142.h5\n",
      "Epoch 143/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0173 - sparse_categorical_accuracy: 0.9938 - val_loss: 6.3063 - val_sparse_categorical_accuracy: 0.5172\n",
      "\n",
      "Epoch 00143: saving model to tmp/MobileNetV2_flowers_model_e_143.h5\n",
      "Epoch 144/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0520 - sparse_categorical_accuracy: 0.9808 - val_loss: 4.8679 - val_sparse_categorical_accuracy: 0.5336\n",
      "\n",
      "Epoch 00144: saving model to tmp/MobileNetV2_flowers_model_e_144.h5\n",
      "Epoch 145/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0598 - sparse_categorical_accuracy: 0.9805 - val_loss: 7.2969 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00145: saving model to tmp/MobileNetV2_flowers_model_e_145.h5\n",
      "Epoch 146/300\n",
      "81/81 [==============================] - 28s 224ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9916 - val_loss: 7.5907 - val_sparse_categorical_accuracy: 0.4864\n",
      "\n",
      "Epoch 00146: saving model to tmp/MobileNetV2_flowers_model_e_146.h5\n",
      "Epoch 147/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9916 - val_loss: 5.3091 - val_sparse_categorical_accuracy: 0.5481\n",
      "\n",
      "Epoch 00147: saving model to tmp/MobileNetV2_flowers_model_e_147.h5\n",
      "Epoch 148/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9943 - val_loss: 6.2521 - val_sparse_categorical_accuracy: 0.5499\n",
      "\n",
      "Epoch 00148: saving model to tmp/MobileNetV2_flowers_model_e_148.h5\n",
      "Epoch 149/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9945 - val_loss: 3.5964 - val_sparse_categorical_accuracy: 0.6352\n",
      "\n",
      "Epoch 00149: saving model to tmp/MobileNetV2_flowers_model_e_149.h5\n",
      "Epoch 150/300\n",
      "81/81 [==============================] - 28s 224ms/step - loss: 0.0379 - sparse_categorical_accuracy: 0.9882 - val_loss: 5.3082 - val_sparse_categorical_accuracy: 0.6025\n",
      "\n",
      "Epoch 00150: saving model to tmp/MobileNetV2_flowers_model_e_150.h5\n",
      "Epoch 151/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0682 - sparse_categorical_accuracy: 0.9720 - val_loss: 7.2416 - val_sparse_categorical_accuracy: 0.4102\n",
      "\n",
      "Epoch 00151: saving model to tmp/MobileNetV2_flowers_model_e_151.h5\n",
      "Epoch 152/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0557 - sparse_categorical_accuracy: 0.9778 - val_loss: 5.3298 - val_sparse_categorical_accuracy: 0.5136\n",
      "\n",
      "Epoch 00152: saving model to tmp/MobileNetV2_flowers_model_e_152.h5\n",
      "Epoch 153/300\n",
      "81/81 [==============================] - 28s 225ms/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9787 - val_loss: 6.1609 - val_sparse_categorical_accuracy: 0.4936\n",
      "\n",
      "Epoch 00153: saving model to tmp/MobileNetV2_flowers_model_e_153.h5\n",
      "Epoch 154/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9931 - val_loss: 5.5266 - val_sparse_categorical_accuracy: 0.5408\n",
      "\n",
      "Epoch 00154: saving model to tmp/MobileNetV2_flowers_model_e_154.h5\n",
      "Epoch 155/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9934 - val_loss: 5.4006 - val_sparse_categorical_accuracy: 0.5154\n",
      "\n",
      "Epoch 00155: saving model to tmp/MobileNetV2_flowers_model_e_155.h5\n",
      "Epoch 156/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0410 - sparse_categorical_accuracy: 0.9878 - val_loss: 6.4358 - val_sparse_categorical_accuracy: 0.5172\n",
      "\n",
      "Epoch 00156: saving model to tmp/MobileNetV2_flowers_model_e_156.h5\n",
      "Epoch 157/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9908 - val_loss: 8.5821 - val_sparse_categorical_accuracy: 0.5172\n",
      "\n",
      "Epoch 00157: saving model to tmp/MobileNetV2_flowers_model_e_157.h5\n",
      "Epoch 158/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9957 - val_loss: 7.3647 - val_sparse_categorical_accuracy: 0.5136\n",
      "\n",
      "Epoch 00158: saving model to tmp/MobileNetV2_flowers_model_e_158.h5\n",
      "Epoch 159/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9947 - val_loss: 6.8661 - val_sparse_categorical_accuracy: 0.5336\n",
      "\n",
      "Epoch 00159: saving model to tmp/MobileNetV2_flowers_model_e_159.h5\n",
      "Epoch 160/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9909 - val_loss: 5.4061 - val_sparse_categorical_accuracy: 0.5209\n",
      "\n",
      "Epoch 00160: saving model to tmp/MobileNetV2_flowers_model_e_160.h5\n",
      "Epoch 161/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0493 - sparse_categorical_accuracy: 0.9823 - val_loss: 7.9745 - val_sparse_categorical_accuracy: 0.4846\n",
      "\n",
      "Epoch 00161: saving model to tmp/MobileNetV2_flowers_model_e_161.h5\n",
      "Epoch 162/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9823 - val_loss: 7.5755 - val_sparse_categorical_accuracy: 0.4918\n",
      "\n",
      "Epoch 00162: saving model to tmp/MobileNetV2_flowers_model_e_162.h5\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0492 - sparse_categorical_accuracy: 0.9837 - val_loss: 6.3972 - val_sparse_categorical_accuracy: 0.5227\n",
      "\n",
      "Epoch 00163: saving model to tmp/MobileNetV2_flowers_model_e_163.h5\n",
      "Epoch 164/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0355 - sparse_categorical_accuracy: 0.9839 - val_loss: 7.1084 - val_sparse_categorical_accuracy: 0.4719\n",
      "\n",
      "Epoch 00164: saving model to tmp/MobileNetV2_flowers_model_e_164.h5\n",
      "Epoch 165/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.1043 - sparse_categorical_accuracy: 0.9682 - val_loss: 6.9590 - val_sparse_categorical_accuracy: 0.5263\n",
      "\n",
      "Epoch 00165: saving model to tmp/MobileNetV2_flowers_model_e_165.h5\n",
      "Epoch 166/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9918 - val_loss: 5.3232 - val_sparse_categorical_accuracy: 0.5699\n",
      "\n",
      "Epoch 00166: saving model to tmp/MobileNetV2_flowers_model_e_166.h5\n",
      "Epoch 167/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9943 - val_loss: 5.9235 - val_sparse_categorical_accuracy: 0.5318\n",
      "\n",
      "Epoch 00167: saving model to tmp/MobileNetV2_flowers_model_e_167.h5\n",
      "Epoch 168/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9967 - val_loss: 4.4401 - val_sparse_categorical_accuracy: 0.5826\n",
      "\n",
      "Epoch 00168: saving model to tmp/MobileNetV2_flowers_model_e_168.h5\n",
      "Epoch 169/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9950 - val_loss: 4.1651 - val_sparse_categorical_accuracy: 0.6098\n",
      "\n",
      "Epoch 00169: saving model to tmp/MobileNetV2_flowers_model_e_169.h5\n",
      "Epoch 170/300\n",
      "81/81 [==============================] - 29s 228ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9970 - val_loss: 3.6924 - val_sparse_categorical_accuracy: 0.6116\n",
      "\n",
      "Epoch 00170: saving model to tmp/MobileNetV2_flowers_model_e_170.h5\n",
      "Epoch 171/300\n",
      "81/81 [==============================] - 30s 228ms/step - loss: 0.0091 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.2924 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00171: saving model to tmp/MobileNetV2_flowers_model_e_171.h5\n",
      "Epoch 172/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9973 - val_loss: 2.5386 - val_sparse_categorical_accuracy: 0.6425\n",
      "\n",
      "Epoch 00172: saving model to tmp/MobileNetV2_flowers_model_e_172.h5\n",
      "Epoch 173/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9968 - val_loss: 3.5447 - val_sparse_categorical_accuracy: 0.6352\n",
      "\n",
      "Epoch 00173: saving model to tmp/MobileNetV2_flowers_model_e_173.h5\n",
      "Epoch 174/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0108 - sparse_categorical_accuracy: 0.9952 - val_loss: 4.1659 - val_sparse_categorical_accuracy: 0.5717\n",
      "\n",
      "Epoch 00174: saving model to tmp/MobileNetV2_flowers_model_e_174.h5\n",
      "Epoch 175/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0382 - sparse_categorical_accuracy: 0.9863 - val_loss: 4.2099 - val_sparse_categorical_accuracy: 0.5917\n",
      "\n",
      "Epoch 00175: saving model to tmp/MobileNetV2_flowers_model_e_175.h5\n",
      "Epoch 176/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0714 - sparse_categorical_accuracy: 0.9757 - val_loss: 6.1102 - val_sparse_categorical_accuracy: 0.4537\n",
      "\n",
      "Epoch 00176: saving model to tmp/MobileNetV2_flowers_model_e_176.h5\n",
      "Epoch 177/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9852 - val_loss: 3.7386 - val_sparse_categorical_accuracy: 0.6134\n",
      "\n",
      "Epoch 00177: saving model to tmp/MobileNetV2_flowers_model_e_177.h5\n",
      "Epoch 178/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0305 - sparse_categorical_accuracy: 0.9891 - val_loss: 4.5852 - val_sparse_categorical_accuracy: 0.5844\n",
      "\n",
      "Epoch 00178: saving model to tmp/MobileNetV2_flowers_model_e_178.h5\n",
      "Epoch 179/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9882 - val_loss: 5.1948 - val_sparse_categorical_accuracy: 0.5463\n",
      "\n",
      "Epoch 00179: saving model to tmp/MobileNetV2_flowers_model_e_179.h5\n",
      "Epoch 180/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9934 - val_loss: 6.5559 - val_sparse_categorical_accuracy: 0.4809\n",
      "\n",
      "Epoch 00180: saving model to tmp/MobileNetV2_flowers_model_e_180.h5\n",
      "Epoch 181/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9909 - val_loss: 3.2484 - val_sparse_categorical_accuracy: 0.6570\n",
      "\n",
      "Epoch 00181: saving model to tmp/MobileNetV2_flowers_model_e_181.h5\n",
      "Epoch 182/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9934 - val_loss: 5.2572 - val_sparse_categorical_accuracy: 0.5517\n",
      "\n",
      "Epoch 00182: saving model to tmp/MobileNetV2_flowers_model_e_182.h5\n",
      "Epoch 183/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0491 - sparse_categorical_accuracy: 0.9817 - val_loss: 4.4168 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00183: saving model to tmp/MobileNetV2_flowers_model_e_183.h5\n",
      "Epoch 184/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0603 - sparse_categorical_accuracy: 0.9780 - val_loss: 9.2425 - val_sparse_categorical_accuracy: 0.4211\n",
      "\n",
      "Epoch 00184: saving model to tmp/MobileNetV2_flowers_model_e_184.h5\n",
      "Epoch 185/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9940 - val_loss: 3.9940 - val_sparse_categorical_accuracy: 0.6044\n",
      "\n",
      "Epoch 00185: saving model to tmp/MobileNetV2_flowers_model_e_185.h5\n",
      "Epoch 186/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9972 - val_loss: 8.4345 - val_sparse_categorical_accuracy: 0.4102\n",
      "\n",
      "Epoch 00186: saving model to tmp/MobileNetV2_flowers_model_e_186.h5\n",
      "Epoch 187/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9967 - val_loss: 5.2268 - val_sparse_categorical_accuracy: 0.5880\n",
      "\n",
      "Epoch 00187: saving model to tmp/MobileNetV2_flowers_model_e_187.h5\n",
      "Epoch 188/300\n",
      "81/81 [==============================] - 30s 229ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9902 - val_loss: 4.2935 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00188: saving model to tmp/MobileNetV2_flowers_model_e_188.h5\n",
      "Epoch 189/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9982 - val_loss: 3.5630 - val_sparse_categorical_accuracy: 0.6334\n",
      "\n",
      "Epoch 00189: saving model to tmp/MobileNetV2_flowers_model_e_189.h5\n",
      "Epoch 190/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9990 - val_loss: 3.8422 - val_sparse_categorical_accuracy: 0.5898\n",
      "\n",
      "Epoch 00190: saving model to tmp/MobileNetV2_flowers_model_e_190.h5\n",
      "Epoch 191/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0073 - sparse_categorical_accuracy: 0.9961 - val_loss: 3.2607 - val_sparse_categorical_accuracy: 0.6461\n",
      "\n",
      "Epoch 00191: saving model to tmp/MobileNetV2_flowers_model_e_191.h5\n",
      "Epoch 192/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9975 - val_loss: 3.2916 - val_sparse_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00192: saving model to tmp/MobileNetV2_flowers_model_e_192.h5\n",
      "Epoch 193/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9991 - val_loss: 3.4443 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00193: saving model to tmp/MobileNetV2_flowers_model_e_193.h5\n",
      "Epoch 194/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9932 - val_loss: 3.1487 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00194: saving model to tmp/MobileNetV2_flowers_model_e_194.h5\n",
      "Epoch 195/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9938 - val_loss: 3.1885 - val_sparse_categorical_accuracy: 0.6171\n",
      "\n",
      "Epoch 00195: saving model to tmp/MobileNetV2_flowers_model_e_195.h5\n",
      "Epoch 196/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9792 - val_loss: 6.2434 - val_sparse_categorical_accuracy: 0.4737\n",
      "\n",
      "Epoch 00196: saving model to tmp/MobileNetV2_flowers_model_e_196.h5\n",
      "Epoch 197/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0642 - sparse_categorical_accuracy: 0.9813 - val_loss: 8.6860 - val_sparse_categorical_accuracy: 0.4483\n",
      "\n",
      "Epoch 00197: saving model to tmp/MobileNetV2_flowers_model_e_197.h5\n",
      "Epoch 198/300\n",
      "81/81 [==============================] - 28s 226ms/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9826 - val_loss: 6.9607 - val_sparse_categorical_accuracy: 0.4229\n",
      "\n",
      "Epoch 00198: saving model to tmp/MobileNetV2_flowers_model_e_198.h5\n",
      "Epoch 199/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0555 - sparse_categorical_accuracy: 0.9820 - val_loss: 4.7542 - val_sparse_categorical_accuracy: 0.5898\n",
      "\n",
      "Epoch 00199: saving model to tmp/MobileNetV2_flowers_model_e_199.h5\n",
      "Epoch 200/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0213 - sparse_categorical_accuracy: 0.9920 - val_loss: 3.3830 - val_sparse_categorical_accuracy: 0.6298\n",
      "\n",
      "Epoch 00200: saving model to tmp/MobileNetV2_flowers_model_e_200.h5\n",
      "Epoch 201/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9921 - val_loss: 5.7477 - val_sparse_categorical_accuracy: 0.5354\n",
      "\n",
      "Epoch 00201: saving model to tmp/MobileNetV2_flowers_model_e_201.h5\n",
      "Epoch 202/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9865 - val_loss: 6.7911 - val_sparse_categorical_accuracy: 0.5372\n",
      "\n",
      "Epoch 00202: saving model to tmp/MobileNetV2_flowers_model_e_202.h5\n",
      "Epoch 203/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9936 - val_loss: 4.1950 - val_sparse_categorical_accuracy: 0.6134\n",
      "\n",
      "Epoch 00203: saving model to tmp/MobileNetV2_flowers_model_e_203.h5\n",
      "Epoch 204/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9951 - val_loss: 2.9211 - val_sparse_categorical_accuracy: 0.6806\n",
      "\n",
      "Epoch 00204: saving model to tmp/MobileNetV2_flowers_model_e_204.h5\n",
      "Epoch 205/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9964 - val_loss: 3.9358 - val_sparse_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00205: saving model to tmp/MobileNetV2_flowers_model_e_205.h5\n",
      "Epoch 206/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9912 - val_loss: 4.9254 - val_sparse_categorical_accuracy: 0.5971\n",
      "\n",
      "Epoch 00206: saving model to tmp/MobileNetV2_flowers_model_e_206.h5\n",
      "Epoch 207/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9936 - val_loss: 3.6592 - val_sparse_categorical_accuracy: 0.6207\n",
      "\n",
      "Epoch 00207: saving model to tmp/MobileNetV2_flowers_model_e_207.h5\n",
      "Epoch 208/300\n",
      "81/81 [==============================] - 29s 228ms/step - loss: 0.0111 - sparse_categorical_accuracy: 0.9964 - val_loss: 4.2932 - val_sparse_categorical_accuracy: 0.5735\n",
      "\n",
      "Epoch 00208: saving model to tmp/MobileNetV2_flowers_model_e_208.h5\n",
      "Epoch 209/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9912 - val_loss: 3.7973 - val_sparse_categorical_accuracy: 0.6225\n",
      "\n",
      "Epoch 00209: saving model to tmp/MobileNetV2_flowers_model_e_209.h5\n",
      "Epoch 210/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9959 - val_loss: 3.7957 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00210: saving model to tmp/MobileNetV2_flowers_model_e_210.h5\n",
      "Epoch 211/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9940 - val_loss: 2.9676 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00211: saving model to tmp/MobileNetV2_flowers_model_e_211.h5\n",
      "Epoch 212/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.9862 - val_loss: 3.3662 - val_sparse_categorical_accuracy: 0.5808\n",
      "\n",
      "Epoch 00212: saving model to tmp/MobileNetV2_flowers_model_e_212.h5\n",
      "Epoch 213/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0744 - sparse_categorical_accuracy: 0.9766 - val_loss: 6.5467 - val_sparse_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00213: saving model to tmp/MobileNetV2_flowers_model_e_213.h5\n",
      "Epoch 214/300\n",
      "81/81 [==============================] - 30s 222ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9865 - val_loss: 4.7692 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00214: saving model to tmp/MobileNetV2_flowers_model_e_214.h5\n",
      "Epoch 215/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0379 - sparse_categorical_accuracy: 0.9892 - val_loss: 4.1733 - val_sparse_categorical_accuracy: 0.5917\n",
      "\n",
      "Epoch 00215: saving model to tmp/MobileNetV2_flowers_model_e_215.h5\n",
      "Epoch 216/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9962 - val_loss: 4.6369 - val_sparse_categorical_accuracy: 0.5681\n",
      "\n",
      "Epoch 00216: saving model to tmp/MobileNetV2_flowers_model_e_216.h5\n",
      "Epoch 217/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9947 - val_loss: 5.3542 - val_sparse_categorical_accuracy: 0.5898\n",
      "\n",
      "Epoch 00217: saving model to tmp/MobileNetV2_flowers_model_e_217.h5\n",
      "Epoch 218/300\n",
      "81/81 [==============================] - 30s 227ms/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.5053 - val_sparse_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00218: saving model to tmp/MobileNetV2_flowers_model_e_218.h5\n",
      "Epoch 219/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.1454 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00219: saving model to tmp/MobileNetV2_flowers_model_e_219.h5\n",
      "Epoch 220/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9965 - val_loss: 2.7482 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00220: saving model to tmp/MobileNetV2_flowers_model_e_220.h5\n",
      "Epoch 221/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9989 - val_loss: 2.7575 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00221: saving model to tmp/MobileNetV2_flowers_model_e_221.h5\n",
      "Epoch 222/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0046 - sparse_categorical_accuracy: 0.9987 - val_loss: 3.3503 - val_sparse_categorical_accuracy: 0.6443\n",
      "\n",
      "Epoch 00222: saving model to tmp/MobileNetV2_flowers_model_e_222.h5\n",
      "Epoch 223/300\n",
      "81/81 [==============================] - 29s 221ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9968 - val_loss: 3.8841 - val_sparse_categorical_accuracy: 0.5753\n",
      "\n",
      "Epoch 00223: saving model to tmp/MobileNetV2_flowers_model_e_223.h5\n",
      "Epoch 224/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0085 - sparse_categorical_accuracy: 0.9969 - val_loss: 3.6186 - val_sparse_categorical_accuracy: 0.5971\n",
      "\n",
      "Epoch 00224: saving model to tmp/MobileNetV2_flowers_model_e_224.h5\n",
      "Epoch 225/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9919 - val_loss: 2.6418 - val_sparse_categorical_accuracy: 0.6679\n",
      "\n",
      "Epoch 00225: saving model to tmp/MobileNetV2_flowers_model_e_225.h5\n",
      "Epoch 226/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.1101 - sparse_categorical_accuracy: 0.9603 - val_loss: 7.6660 - val_sparse_categorical_accuracy: 0.4283\n",
      "\n",
      "Epoch 00226: saving model to tmp/MobileNetV2_flowers_model_e_226.h5\n",
      "Epoch 227/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0537 - sparse_categorical_accuracy: 0.9814 - val_loss: 8.8451 - val_sparse_categorical_accuracy: 0.4864\n",
      "\n",
      "Epoch 00227: saving model to tmp/MobileNetV2_flowers_model_e_227.h5\n",
      "Epoch 228/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9936 - val_loss: 5.4286 - val_sparse_categorical_accuracy: 0.5808\n",
      "\n",
      "Epoch 00228: saving model to tmp/MobileNetV2_flowers_model_e_228.h5\n",
      "Epoch 229/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0083 - sparse_categorical_accuracy: 0.9975 - val_loss: 7.2273 - val_sparse_categorical_accuracy: 0.5082\n",
      "\n",
      "Epoch 00229: saving model to tmp/MobileNetV2_flowers_model_e_229.h5\n",
      "Epoch 230/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9937 - val_loss: 6.6808 - val_sparse_categorical_accuracy: 0.4955\n",
      "\n",
      "Epoch 00230: saving model to tmp/MobileNetV2_flowers_model_e_230.h5\n",
      "Epoch 231/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9915 - val_loss: 6.5418 - val_sparse_categorical_accuracy: 0.5299\n",
      "\n",
      "Epoch 00231: saving model to tmp/MobileNetV2_flowers_model_e_231.h5\n",
      "Epoch 232/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9957 - val_loss: 5.1686 - val_sparse_categorical_accuracy: 0.5789\n",
      "\n",
      "Epoch 00232: saving model to tmp/MobileNetV2_flowers_model_e_232.h5\n",
      "Epoch 233/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9952 - val_loss: 4.4913 - val_sparse_categorical_accuracy: 0.5880\n",
      "\n",
      "Epoch 00233: saving model to tmp/MobileNetV2_flowers_model_e_233.h5\n",
      "Epoch 234/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9985 - val_loss: 3.8193 - val_sparse_categorical_accuracy: 0.5862\n",
      "\n",
      "Epoch 00234: saving model to tmp/MobileNetV2_flowers_model_e_234.h5\n",
      "Epoch 235/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0400 - sparse_categorical_accuracy: 0.9873 - val_loss: 6.8611 - val_sparse_categorical_accuracy: 0.4755\n",
      "\n",
      "Epoch 00235: saving model to tmp/MobileNetV2_flowers_model_e_235.h5\n",
      "Epoch 236/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9935 - val_loss: 8.2507 - val_sparse_categorical_accuracy: 0.4428\n",
      "\n",
      "Epoch 00236: saving model to tmp/MobileNetV2_flowers_model_e_236.h5\n",
      "Epoch 237/300\n",
      "81/81 [==============================] - 30s 222ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9888 - val_loss: 6.6370 - val_sparse_categorical_accuracy: 0.4864\n",
      "\n",
      "Epoch 00237: saving model to tmp/MobileNetV2_flowers_model_e_237.h5\n",
      "Epoch 238/300\n",
      "81/81 [==============================] - 29s 220ms/step - loss: 0.0091 - sparse_categorical_accuracy: 0.9973 - val_loss: 4.8254 - val_sparse_categorical_accuracy: 0.5390\n",
      "\n",
      "Epoch 00238: saving model to tmp/MobileNetV2_flowers_model_e_238.h5\n",
      "Epoch 239/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9973 - val_loss: 4.5436 - val_sparse_categorical_accuracy: 0.5481\n",
      "\n",
      "Epoch 00239: saving model to tmp/MobileNetV2_flowers_model_e_239.h5\n",
      "Epoch 240/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0089 - sparse_categorical_accuracy: 0.9969 - val_loss: 6.0377 - val_sparse_categorical_accuracy: 0.5227\n",
      "\n",
      "Epoch 00240: saving model to tmp/MobileNetV2_flowers_model_e_240.h5\n",
      "Epoch 241/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9928 - val_loss: 8.1808 - val_sparse_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00241: saving model to tmp/MobileNetV2_flowers_model_e_241.h5\n",
      "Epoch 242/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9916 - val_loss: 4.7249 - val_sparse_categorical_accuracy: 0.5590\n",
      "\n",
      "Epoch 00242: saving model to tmp/MobileNetV2_flowers_model_e_242.h5\n",
      "Epoch 243/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9947 - val_loss: 2.9751 - val_sparse_categorical_accuracy: 0.6570\n",
      "\n",
      "Epoch 00243: saving model to tmp/MobileNetV2_flowers_model_e_243.h5\n",
      "Epoch 244/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9965 - val_loss: 4.5355 - val_sparse_categorical_accuracy: 0.6171\n",
      "\n",
      "Epoch 00244: saving model to tmp/MobileNetV2_flowers_model_e_244.h5\n",
      "Epoch 245/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9927 - val_loss: 5.0165 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00245: saving model to tmp/MobileNetV2_flowers_model_e_245.h5\n",
      "Epoch 246/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9949 - val_loss: 2.8481 - val_sparse_categorical_accuracy: 0.6370\n",
      "\n",
      "Epoch 00246: saving model to tmp/MobileNetV2_flowers_model_e_246.h5\n",
      "Epoch 247/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0628 - sparse_categorical_accuracy: 0.9834 - val_loss: 5.4121 - val_sparse_categorical_accuracy: 0.5517\n",
      "\n",
      "Epoch 00247: saving model to tmp/MobileNetV2_flowers_model_e_247.h5\n",
      "Epoch 248/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9950 - val_loss: 6.3071 - val_sparse_categorical_accuracy: 0.5735\n",
      "\n",
      "Epoch 00248: saving model to tmp/MobileNetV2_flowers_model_e_248.h5\n",
      "Epoch 249/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9930 - val_loss: 3.0671 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00249: saving model to tmp/MobileNetV2_flowers_model_e_249.h5\n",
      "Epoch 250/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9968 - val_loss: 3.7903 - val_sparse_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00250: saving model to tmp/MobileNetV2_flowers_model_e_250.h5\n",
      "Epoch 251/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9948 - val_loss: 6.9096 - val_sparse_categorical_accuracy: 0.5009\n",
      "\n",
      "Epoch 00251: saving model to tmp/MobileNetV2_flowers_model_e_251.h5\n",
      "Epoch 252/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9981 - val_loss: 3.1679 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00252: saving model to tmp/MobileNetV2_flowers_model_e_252.h5\n",
      "Epoch 253/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9934 - val_loss: 4.3523 - val_sparse_categorical_accuracy: 0.6080\n",
      "\n",
      "Epoch 00253: saving model to tmp/MobileNetV2_flowers_model_e_253.h5\n",
      "Epoch 254/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9934 - val_loss: 4.3214 - val_sparse_categorical_accuracy: 0.5971\n",
      "\n",
      "Epoch 00254: saving model to tmp/MobileNetV2_flowers_model_e_254.h5\n",
      "Epoch 255/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9967 - val_loss: 4.0276 - val_sparse_categorical_accuracy: 0.6116\n",
      "\n",
      "Epoch 00255: saving model to tmp/MobileNetV2_flowers_model_e_255.h5\n",
      "Epoch 256/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9971 - val_loss: 5.4371 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00256: saving model to tmp/MobileNetV2_flowers_model_e_256.h5\n",
      "Epoch 257/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9961 - val_loss: 4.8085 - val_sparse_categorical_accuracy: 0.5390\n",
      "\n",
      "Epoch 00257: saving model to tmp/MobileNetV2_flowers_model_e_257.h5\n",
      "Epoch 258/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9957 - val_loss: 5.0938 - val_sparse_categorical_accuracy: 0.5463\n",
      "\n",
      "Epoch 00258: saving model to tmp/MobileNetV2_flowers_model_e_258.h5\n",
      "Epoch 259/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9785 - val_loss: 6.8501 - val_sparse_categorical_accuracy: 0.4682\n",
      "\n",
      "Epoch 00259: saving model to tmp/MobileNetV2_flowers_model_e_259.h5\n",
      "Epoch 260/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9855 - val_loss: 5.5320 - val_sparse_categorical_accuracy: 0.5771\n",
      "\n",
      "Epoch 00260: saving model to tmp/MobileNetV2_flowers_model_e_260.h5\n",
      "Epoch 261/300\n",
      "81/81 [==============================] - 28s 221ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9921 - val_loss: 3.4312 - val_sparse_categorical_accuracy: 0.6007\n",
      "\n",
      "Epoch 00261: saving model to tmp/MobileNetV2_flowers_model_e_261.h5\n",
      "Epoch 262/300\n",
      "81/81 [==============================] - 30s 222ms/step - loss: 0.0397 - sparse_categorical_accuracy: 0.9878 - val_loss: 11.6860 - val_sparse_categorical_accuracy: 0.4065\n",
      "\n",
      "Epoch 00262: saving model to tmp/MobileNetV2_flowers_model_e_262.h5\n",
      "Epoch 263/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9930 - val_loss: 3.4449 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00263: saving model to tmp/MobileNetV2_flowers_model_e_263.h5\n",
      "Epoch 264/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0094 - sparse_categorical_accuracy: 0.9959 - val_loss: 4.9175 - val_sparse_categorical_accuracy: 0.5554\n",
      "\n",
      "Epoch 00264: saving model to tmp/MobileNetV2_flowers_model_e_264.h5\n",
      "Epoch 265/300\n",
      "81/81 [==============================] - 29s 227ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9969 - val_loss: 3.5830 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00265: saving model to tmp/MobileNetV2_flowers_model_e_265.h5\n",
      "Epoch 266/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9939 - val_loss: 3.6894 - val_sparse_categorical_accuracy: 0.6171\n",
      "\n",
      "Epoch 00266: saving model to tmp/MobileNetV2_flowers_model_e_266.h5\n",
      "Epoch 267/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0128 - sparse_categorical_accuracy: 0.9947 - val_loss: 3.3123 - val_sparse_categorical_accuracy: 0.6352\n",
      "\n",
      "Epoch 00267: saving model to tmp/MobileNetV2_flowers_model_e_267.h5\n",
      "Epoch 268/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9994 - val_loss: 3.2056 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00268: saving model to tmp/MobileNetV2_flowers_model_e_268.h5\n",
      "Epoch 269/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9982 - val_loss: 2.7908 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00269: saving model to tmp/MobileNetV2_flowers_model_e_269.h5\n",
      "Epoch 270/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9996 - val_loss: 2.4716 - val_sparse_categorical_accuracy: 0.6897\n",
      "\n",
      "Epoch 00270: saving model to tmp/MobileNetV2_flowers_model_e_270.h5\n",
      "Epoch 271/300\n",
      "81/81 [==============================] - 28s 224ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9997 - val_loss: 2.6669 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00271: saving model to tmp/MobileNetV2_flowers_model_e_271.h5\n",
      "Epoch 272/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9989 - val_loss: 3.1132 - val_sparse_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00272: saving model to tmp/MobileNetV2_flowers_model_e_272.h5\n",
      "Epoch 273/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0083 - sparse_categorical_accuracy: 0.9974 - val_loss: 3.4448 - val_sparse_categorical_accuracy: 0.6007\n",
      "\n",
      "Epoch 00273: saving model to tmp/MobileNetV2_flowers_model_e_273.h5\n",
      "Epoch 274/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9895 - val_loss: 3.1788 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00274: saving model to tmp/MobileNetV2_flowers_model_e_274.h5\n",
      "Epoch 275/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0345 - sparse_categorical_accuracy: 0.9914 - val_loss: 3.3142 - val_sparse_categorical_accuracy: 0.6080\n",
      "\n",
      "Epoch 00275: saving model to tmp/MobileNetV2_flowers_model_e_275.h5\n",
      "Epoch 276/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9950 - val_loss: 3.4763 - val_sparse_categorical_accuracy: 0.5753\n",
      "\n",
      "Epoch 00276: saving model to tmp/MobileNetV2_flowers_model_e_276.h5\n",
      "Epoch 277/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9950 - val_loss: 2.7977 - val_sparse_categorical_accuracy: 0.6261\n",
      "\n",
      "Epoch 00277: saving model to tmp/MobileNetV2_flowers_model_e_277.h5\n",
      "Epoch 278/300\n",
      "81/81 [==============================] - 30s 226ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9963 - val_loss: 3.5207 - val_sparse_categorical_accuracy: 0.6261\n",
      "\n",
      "Epoch 00278: saving model to tmp/MobileNetV2_flowers_model_e_278.h5\n",
      "Epoch 279/300\n",
      "81/81 [==============================] - 30s 224ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.9895 - val_loss: 3.4963 - val_sparse_categorical_accuracy: 0.6025\n",
      "\n",
      "Epoch 00279: saving model to tmp/MobileNetV2_flowers_model_e_279.h5\n",
      "Epoch 280/300\n",
      "81/81 [==============================] - 28s 225ms/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9913 - val_loss: 3.3008 - val_sparse_categorical_accuracy: 0.6298\n",
      "\n",
      "Epoch 00280: saving model to tmp/MobileNetV2_flowers_model_e_280.h5\n",
      "Epoch 281/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9944 - val_loss: 3.2077 - val_sparse_categorical_accuracy: 0.6316\n",
      "\n",
      "Epoch 00281: saving model to tmp/MobileNetV2_flowers_model_e_281.h5\n",
      "Epoch 282/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9961 - val_loss: 5.5653 - val_sparse_categorical_accuracy: 0.5481\n",
      "\n",
      "Epoch 00282: saving model to tmp/MobileNetV2_flowers_model_e_282.h5\n",
      "Epoch 283/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9946 - val_loss: 2.4217 - val_sparse_categorical_accuracy: 0.6788\n",
      "\n",
      "Epoch 00283: saving model to tmp/MobileNetV2_flowers_model_e_283.h5\n",
      "Epoch 284/300\n",
      "81/81 [==============================] - 30s 223ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9960 - val_loss: 2.4717 - val_sparse_categorical_accuracy: 0.6860\n",
      "\n",
      "Epoch 00284: saving model to tmp/MobileNetV2_flowers_model_e_284.h5\n",
      "Epoch 285/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9969 - val_loss: 2.4679 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00285: saving model to tmp/MobileNetV2_flowers_model_e_285.h5\n",
      "Epoch 286/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9841 - val_loss: 3.5499 - val_sparse_categorical_accuracy: 0.5917\n",
      "\n",
      "Epoch 00286: saving model to tmp/MobileNetV2_flowers_model_e_286.h5\n",
      "Epoch 287/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9926 - val_loss: 4.2101 - val_sparse_categorical_accuracy: 0.5898\n",
      "\n",
      "Epoch 00287: saving model to tmp/MobileNetV2_flowers_model_e_287.h5\n",
      "Epoch 288/300\n",
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9945 - val_loss: 5.3257 - val_sparse_categorical_accuracy: 0.5953\n",
      "\n",
      "Epoch 00288: saving model to tmp/MobileNetV2_flowers_model_e_288.h5\n",
      "Epoch 289/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9933 - val_loss: 2.9432 - val_sparse_categorical_accuracy: 0.6461\n",
      "\n",
      "Epoch 00289: saving model to tmp/MobileNetV2_flowers_model_e_289.h5\n",
      "Epoch 290/300\n",
      "81/81 [==============================] - 29s 226ms/step - loss: 0.0633 - sparse_categorical_accuracy: 0.9842 - val_loss: 4.9749 - val_sparse_categorical_accuracy: 0.5699\n",
      "\n",
      "Epoch 00290: saving model to tmp/MobileNetV2_flowers_model_e_290.h5\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 30s 225ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9960 - val_loss: 4.6347 - val_sparse_categorical_accuracy: 0.5408\n",
      "\n",
      "Epoch 00291: saving model to tmp/MobileNetV2_flowers_model_e_291.h5\n",
      "Epoch 292/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0108 - sparse_categorical_accuracy: 0.9968 - val_loss: 4.6621 - val_sparse_categorical_accuracy: 0.5445\n",
      "\n",
      "Epoch 00292: saving model to tmp/MobileNetV2_flowers_model_e_292.h5\n",
      "Epoch 293/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9984 - val_loss: 3.5522 - val_sparse_categorical_accuracy: 0.6044\n",
      "\n",
      "Epoch 00293: saving model to tmp/MobileNetV2_flowers_model_e_293.h5\n",
      "Epoch 294/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9973 - val_loss: 3.1229 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00294: saving model to tmp/MobileNetV2_flowers_model_e_294.h5\n",
      "Epoch 295/300\n",
      "81/81 [==============================] - 29s 224ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9970 - val_loss: 4.7767 - val_sparse_categorical_accuracy: 0.5517\n",
      "\n",
      "Epoch 00295: saving model to tmp/MobileNetV2_flowers_model_e_295.h5\n",
      "Epoch 296/300\n",
      "81/81 [==============================] - 29s 228ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9975 - val_loss: 3.0605 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00296: saving model to tmp/MobileNetV2_flowers_model_e_296.h5\n",
      "Epoch 297/300\n",
      "81/81 [==============================] - 28s 224ms/step - loss: 0.0072 - sparse_categorical_accuracy: 0.9978 - val_loss: 4.0620 - val_sparse_categorical_accuracy: 0.6025\n",
      "\n",
      "Epoch 00297: saving model to tmp/MobileNetV2_flowers_model_e_297.h5\n",
      "Epoch 298/300\n",
      "81/81 [==============================] - 29s 225ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9976 - val_loss: 2.3086 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00298: saving model to tmp/MobileNetV2_flowers_model_e_298.h5\n",
      "Epoch 299/300\n",
      "81/81 [==============================] - 29s 223ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9964 - val_loss: 2.6378 - val_sparse_categorical_accuracy: 0.6425\n",
      "\n",
      "Epoch 00299: saving model to tmp/MobileNetV2_flowers_model_e_299.h5\n",
      "Epoch 300/300\n",
      "81/81 [==============================] - 29s 222ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.3607 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00300: saving model to tmp/MobileNetV2_flowers_model_e_300.h5\n",
      "Saved to: MobileNetV2_flowers_model.h5\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_flowers(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNets - B0, B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'EfficentNetB0_flowers_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "81/81 [==============================] - 43s 335ms/step - loss: 3.0777 - sparse_categorical_accuracy: 0.2806 - val_loss: 2.6566 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00001: saving model to tmp/EfficentNetB0_flowers_model_e_01.h5\n",
      "Epoch 2/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 1.3022 - sparse_categorical_accuracy: 0.4645 - val_loss: 2.1797 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00002: saving model to tmp/EfficentNetB0_flowers_model_e_02.h5\n",
      "Epoch 3/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 1.0181 - sparse_categorical_accuracy: 0.5835 - val_loss: 2.1402 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00003: saving model to tmp/EfficentNetB0_flowers_model_e_03.h5\n",
      "Epoch 4/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.9478 - sparse_categorical_accuracy: 0.6399 - val_loss: 2.1931 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00004: saving model to tmp/EfficentNetB0_flowers_model_e_04.h5\n",
      "Epoch 5/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.8289 - sparse_categorical_accuracy: 0.6857 - val_loss: 2.9261 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00005: saving model to tmp/EfficentNetB0_flowers_model_e_05.h5\n",
      "Epoch 6/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.6986 - sparse_categorical_accuracy: 0.7367 - val_loss: 2.3612 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00006: saving model to tmp/EfficentNetB0_flowers_model_e_06.h5\n",
      "Epoch 7/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.5585 - sparse_categorical_accuracy: 0.7820 - val_loss: 2.6503 - val_sparse_categorical_accuracy: 0.2668\n",
      "\n",
      "Epoch 00007: saving model to tmp/EfficentNetB0_flowers_model_e_07.h5\n",
      "Epoch 8/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.4768 - sparse_categorical_accuracy: 0.8256 - val_loss: 1.6634 - val_sparse_categorical_accuracy: 0.3557\n",
      "\n",
      "Epoch 00008: saving model to tmp/EfficentNetB0_flowers_model_e_08.h5\n",
      "Epoch 9/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.4333 - sparse_categorical_accuracy: 0.8389 - val_loss: 2.0571 - val_sparse_categorical_accuracy: 0.4519\n",
      "\n",
      "Epoch 00009: saving model to tmp/EfficentNetB0_flowers_model_e_09.h5\n",
      "Epoch 10/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.3634 - sparse_categorical_accuracy: 0.8696 - val_loss: 1.7956 - val_sparse_categorical_accuracy: 0.5699\n",
      "\n",
      "Epoch 00010: saving model to tmp/EfficentNetB0_flowers_model_e_10.h5\n",
      "Epoch 11/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.2449 - sparse_categorical_accuracy: 0.9165 - val_loss: 1.7138 - val_sparse_categorical_accuracy: 0.6044\n",
      "\n",
      "Epoch 00011: saving model to tmp/EfficentNetB0_flowers_model_e_11.h5\n",
      "Epoch 12/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.2564 - sparse_categorical_accuracy: 0.9203 - val_loss: 1.4351 - val_sparse_categorical_accuracy: 0.6134\n",
      "\n",
      "Epoch 00012: saving model to tmp/EfficentNetB0_flowers_model_e_12.h5\n",
      "Epoch 13/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.2837 - sparse_categorical_accuracy: 0.9016 - val_loss: 1.6215 - val_sparse_categorical_accuracy: 0.6443\n",
      "\n",
      "Epoch 00013: saving model to tmp/EfficentNetB0_flowers_model_e_13.h5\n",
      "Epoch 14/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.2043 - sparse_categorical_accuracy: 0.9256 - val_loss: 1.8424 - val_sparse_categorical_accuracy: 0.6116\n",
      "\n",
      "Epoch 00014: saving model to tmp/EfficentNetB0_flowers_model_e_14.h5\n",
      "Epoch 15/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.1927 - sparse_categorical_accuracy: 0.9281 - val_loss: 1.5358 - val_sparse_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00015: saving model to tmp/EfficentNetB0_flowers_model_e_15.h5\n",
      "Epoch 16/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.1683 - sparse_categorical_accuracy: 0.9433 - val_loss: 1.8485 - val_sparse_categorical_accuracy: 0.5753\n",
      "\n",
      "Epoch 00016: saving model to tmp/EfficentNetB0_flowers_model_e_16.h5\n",
      "Epoch 17/300\n",
      "81/81 [==============================] - 36s 320ms/step - loss: 0.2076 - sparse_categorical_accuracy: 0.9335 - val_loss: 1.5081 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00017: saving model to tmp/EfficentNetB0_flowers_model_e_17.h5\n",
      "Epoch 18/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.2210 - sparse_categorical_accuracy: 0.9289 - val_loss: 1.4200 - val_sparse_categorical_accuracy: 0.6443\n",
      "\n",
      "Epoch 00018: saving model to tmp/EfficentNetB0_flowers_model_e_18.h5\n",
      "Epoch 19/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.1646 - sparse_categorical_accuracy: 0.9411 - val_loss: 1.6208 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00019: saving model to tmp/EfficentNetB0_flowers_model_e_19.h5\n",
      "Epoch 20/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.1049 - sparse_categorical_accuracy: 0.9645 - val_loss: 1.8182 - val_sparse_categorical_accuracy: 0.6425\n",
      "\n",
      "Epoch 00020: saving model to tmp/EfficentNetB0_flowers_model_e_20.h5\n",
      "Epoch 21/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.1668 - sparse_categorical_accuracy: 0.9466 - val_loss: 1.5528 - val_sparse_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00021: saving model to tmp/EfficentNetB0_flowers_model_e_21.h5\n",
      "Epoch 22/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9570 - val_loss: 1.5516 - val_sparse_categorical_accuracy: 0.6134\n",
      "\n",
      "Epoch 00022: saving model to tmp/EfficentNetB0_flowers_model_e_22.h5\n",
      "Epoch 23/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.1236 - sparse_categorical_accuracy: 0.9566 - val_loss: 1.4031 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00023: saving model to tmp/EfficentNetB0_flowers_model_e_23.h5\n",
      "Epoch 24/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0924 - sparse_categorical_accuracy: 0.9697 - val_loss: 1.7464 - val_sparse_categorical_accuracy: 0.6207\n",
      "\n",
      "Epoch 00024: saving model to tmp/EfficentNetB0_flowers_model_e_24.h5\n",
      "Epoch 25/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.1362 - sparse_categorical_accuracy: 0.9535 - val_loss: 1.5204 - val_sparse_categorical_accuracy: 0.6679\n",
      "\n",
      "Epoch 00025: saving model to tmp/EfficentNetB0_flowers_model_e_25.h5\n",
      "Epoch 26/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.1077 - sparse_categorical_accuracy: 0.9608 - val_loss: 1.6983 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00026: saving model to tmp/EfficentNetB0_flowers_model_e_26.h5\n",
      "Epoch 27/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.1439 - sparse_categorical_accuracy: 0.9482 - val_loss: 1.5436 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00027: saving model to tmp/EfficentNetB0_flowers_model_e_27.h5\n",
      "Epoch 28/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0909 - sparse_categorical_accuracy: 0.9728 - val_loss: 1.7114 - val_sparse_categorical_accuracy: 0.6443\n",
      "\n",
      "Epoch 00028: saving model to tmp/EfficentNetB0_flowers_model_e_28.h5\n",
      "Epoch 29/300\n",
      "81/81 [==============================] - 36s 320ms/step - loss: 0.0963 - sparse_categorical_accuracy: 0.9677 - val_loss: 1.4747 - val_sparse_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00029: saving model to tmp/EfficentNetB0_flowers_model_e_29.h5\n",
      "Epoch 30/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0946 - sparse_categorical_accuracy: 0.9722 - val_loss: 1.5184 - val_sparse_categorical_accuracy: 0.6388\n",
      "\n",
      "Epoch 00030: saving model to tmp/EfficentNetB0_flowers_model_e_30.h5\n",
      "Epoch 31/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.1280 - sparse_categorical_accuracy: 0.9619 - val_loss: 1.6599 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00031: saving model to tmp/EfficentNetB0_flowers_model_e_31.h5\n",
      "Epoch 32/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0948 - sparse_categorical_accuracy: 0.9663 - val_loss: 1.4745 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00032: saving model to tmp/EfficentNetB0_flowers_model_e_32.h5\n",
      "Epoch 33/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0970 - sparse_categorical_accuracy: 0.9685 - val_loss: 1.6560 - val_sparse_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00033: saving model to tmp/EfficentNetB0_flowers_model_e_33.h5\n",
      "Epoch 34/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.1074 - sparse_categorical_accuracy: 0.9642 - val_loss: 1.7653 - val_sparse_categorical_accuracy: 0.6171\n",
      "\n",
      "Epoch 00034: saving model to tmp/EfficentNetB0_flowers_model_e_34.h5\n",
      "Epoch 35/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9691 - val_loss: 1.6630 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00035: saving model to tmp/EfficentNetB0_flowers_model_e_35.h5\n",
      "Epoch 36/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.1295 - sparse_categorical_accuracy: 0.9525 - val_loss: 2.5273 - val_sparse_categorical_accuracy: 0.5935\n",
      "\n",
      "Epoch 00036: saving model to tmp/EfficentNetB0_flowers_model_e_36.h5\n",
      "Epoch 37/300\n",
      "81/81 [==============================] - 36s 318ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9745 - val_loss: 2.0798 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00037: saving model to tmp/EfficentNetB0_flowers_model_e_37.h5\n",
      "Epoch 38/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9745 - val_loss: 1.5986 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00038: saving model to tmp/EfficentNetB0_flowers_model_e_38.h5\n",
      "Epoch 39/300\n",
      "81/81 [==============================] - 36s 320ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9683 - val_loss: 1.8503 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00039: saving model to tmp/EfficentNetB0_flowers_model_e_39.h5\n",
      "Epoch 40/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.0951 - sparse_categorical_accuracy: 0.9704 - val_loss: 1.5395 - val_sparse_categorical_accuracy: 0.6715\n",
      "\n",
      "Epoch 00040: saving model to tmp/EfficentNetB0_flowers_model_e_40.h5\n",
      "Epoch 41/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0824 - sparse_categorical_accuracy: 0.9719 - val_loss: 1.8363 - val_sparse_categorical_accuracy: 0.6225\n",
      "\n",
      "Epoch 00041: saving model to tmp/EfficentNetB0_flowers_model_e_41.h5\n",
      "Epoch 42/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0691 - sparse_categorical_accuracy: 0.9776 - val_loss: 1.6950 - val_sparse_categorical_accuracy: 0.6352\n",
      "\n",
      "Epoch 00042: saving model to tmp/EfficentNetB0_flowers_model_e_42.h5\n",
      "Epoch 43/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9800 - val_loss: 1.7873 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00043: saving model to tmp/EfficentNetB0_flowers_model_e_43.h5\n",
      "Epoch 44/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.1017 - sparse_categorical_accuracy: 0.9700 - val_loss: 1.7347 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00044: saving model to tmp/EfficentNetB0_flowers_model_e_44.h5\n",
      "Epoch 45/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0784 - sparse_categorical_accuracy: 0.9729 - val_loss: 1.7322 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00045: saving model to tmp/EfficentNetB0_flowers_model_e_45.h5\n",
      "Epoch 46/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9775 - val_loss: 1.7166 - val_sparse_categorical_accuracy: 0.6788\n",
      "\n",
      "Epoch 00046: saving model to tmp/EfficentNetB0_flowers_model_e_46.h5\n",
      "Epoch 47/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0615 - sparse_categorical_accuracy: 0.9830 - val_loss: 1.7589 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00047: saving model to tmp/EfficentNetB0_flowers_model_e_47.h5\n",
      "Epoch 48/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0881 - sparse_categorical_accuracy: 0.9729 - val_loss: 2.2633 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00048: saving model to tmp/EfficentNetB0_flowers_model_e_48.h5\n",
      "Epoch 49/300\n",
      "81/81 [==============================] - 37s 317ms/step - loss: 0.0649 - sparse_categorical_accuracy: 0.9765 - val_loss: 1.5833 - val_sparse_categorical_accuracy: 0.6388\n",
      "\n",
      "Epoch 00049: saving model to tmp/EfficentNetB0_flowers_model_e_49.h5\n",
      "Epoch 50/300\n",
      "81/81 [==============================] - 37s 323ms/step - loss: 0.0479 - sparse_categorical_accuracy: 0.9833 - val_loss: 1.7537 - val_sparse_categorical_accuracy: 0.6298\n",
      "\n",
      "Epoch 00050: saving model to tmp/EfficentNetB0_flowers_model_e_50.h5\n",
      "Epoch 51/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9822 - val_loss: 2.0355 - val_sparse_categorical_accuracy: 0.6098\n",
      "\n",
      "Epoch 00051: saving model to tmp/EfficentNetB0_flowers_model_e_51.h5\n",
      "Epoch 52/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.8532 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00052: saving model to tmp/EfficentNetB0_flowers_model_e_52.h5\n",
      "Epoch 53/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0547 - sparse_categorical_accuracy: 0.9845 - val_loss: 1.5170 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00053: saving model to tmp/EfficentNetB0_flowers_model_e_53.h5\n",
      "Epoch 54/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0556 - sparse_categorical_accuracy: 0.9776 - val_loss: 2.2336 - val_sparse_categorical_accuracy: 0.6261\n",
      "\n",
      "Epoch 00054: saving model to tmp/EfficentNetB0_flowers_model_e_54.h5\n",
      "Epoch 55/300\n",
      "81/81 [==============================] - 37s 323ms/step - loss: 0.0834 - sparse_categorical_accuracy: 0.9713 - val_loss: 1.8513 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00055: saving model to tmp/EfficentNetB0_flowers_model_e_55.h5\n",
      "Epoch 56/300\n",
      "81/81 [==============================] - 36s 320ms/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9726 - val_loss: 1.6050 - val_sparse_categorical_accuracy: 0.6642\n",
      "\n",
      "Epoch 00056: saving model to tmp/EfficentNetB0_flowers_model_e_56.h5\n",
      "Epoch 57/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9824 - val_loss: 1.6401 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00057: saving model to tmp/EfficentNetB0_flowers_model_e_57.h5\n",
      "Epoch 58/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9833 - val_loss: 1.8843 - val_sparse_categorical_accuracy: 0.6171\n",
      "\n",
      "Epoch 00058: saving model to tmp/EfficentNetB0_flowers_model_e_58.h5\n",
      "Epoch 59/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0804 - sparse_categorical_accuracy: 0.9809 - val_loss: 1.6468 - val_sparse_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00059: saving model to tmp/EfficentNetB0_flowers_model_e_59.h5\n",
      "Epoch 60/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0588 - sparse_categorical_accuracy: 0.9842 - val_loss: 1.9870 - val_sparse_categorical_accuracy: 0.6461\n",
      "\n",
      "Epoch 00060: saving model to tmp/EfficentNetB0_flowers_model_e_60.h5\n",
      "Epoch 61/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0452 - sparse_categorical_accuracy: 0.9852 - val_loss: 1.6353 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00061: saving model to tmp/EfficentNetB0_flowers_model_e_61.h5\n",
      "Epoch 62/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0458 - sparse_categorical_accuracy: 0.9875 - val_loss: 2.1716 - val_sparse_categorical_accuracy: 0.5717\n",
      "\n",
      "Epoch 00062: saving model to tmp/EfficentNetB0_flowers_model_e_62.h5\n",
      "Epoch 63/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0537 - sparse_categorical_accuracy: 0.9804 - val_loss: 1.8244 - val_sparse_categorical_accuracy: 0.6570\n",
      "\n",
      "Epoch 00063: saving model to tmp/EfficentNetB0_flowers_model_e_63.h5\n",
      "Epoch 64/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9831 - val_loss: 1.5983 - val_sparse_categorical_accuracy: 0.6515\n",
      "\n",
      "Epoch 00064: saving model to tmp/EfficentNetB0_flowers_model_e_64.h5\n",
      "Epoch 65/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9849 - val_loss: 1.6245 - val_sparse_categorical_accuracy: 0.6642\n",
      "\n",
      "Epoch 00065: saving model to tmp/EfficentNetB0_flowers_model_e_65.h5\n",
      "Epoch 66/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9834 - val_loss: 1.4426 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00066: saving model to tmp/EfficentNetB0_flowers_model_e_66.h5\n",
      "Epoch 67/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0817 - sparse_categorical_accuracy: 0.9792 - val_loss: 1.8542 - val_sparse_categorical_accuracy: 0.6515\n",
      "\n",
      "Epoch 00067: saving model to tmp/EfficentNetB0_flowers_model_e_67.h5\n",
      "Epoch 68/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0709 - sparse_categorical_accuracy: 0.9769 - val_loss: 1.7268 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00068: saving model to tmp/EfficentNetB0_flowers_model_e_68.h5\n",
      "Epoch 69/300\n",
      "81/81 [==============================] - 38s 323ms/step - loss: 0.0871 - sparse_categorical_accuracy: 0.9692 - val_loss: 1.7081 - val_sparse_categorical_accuracy: 0.6788\n",
      "\n",
      "Epoch 00069: saving model to tmp/EfficentNetB0_flowers_model_e_69.h5\n",
      "Epoch 70/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0348 - sparse_categorical_accuracy: 0.9873 - val_loss: 1.5019 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00070: saving model to tmp/EfficentNetB0_flowers_model_e_70.h5\n",
      "Epoch 71/300\n",
      "81/81 [==============================] - 38s 321ms/step - loss: 0.0647 - sparse_categorical_accuracy: 0.9807 - val_loss: 1.6552 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00071: saving model to tmp/EfficentNetB0_flowers_model_e_71.h5\n",
      "Epoch 72/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9855 - val_loss: 1.5222 - val_sparse_categorical_accuracy: 0.6788\n",
      "\n",
      "Epoch 00072: saving model to tmp/EfficentNetB0_flowers_model_e_72.h5\n",
      "Epoch 73/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9874 - val_loss: 1.6913 - val_sparse_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00073: saving model to tmp/EfficentNetB0_flowers_model_e_73.h5\n",
      "Epoch 74/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.7405 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00074: saving model to tmp/EfficentNetB0_flowers_model_e_74.h5\n",
      "Epoch 75/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0725 - sparse_categorical_accuracy: 0.9753 - val_loss: 1.6717 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00075: saving model to tmp/EfficentNetB0_flowers_model_e_75.h5\n",
      "Epoch 76/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0568 - sparse_categorical_accuracy: 0.9762 - val_loss: 1.7617 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00076: saving model to tmp/EfficentNetB0_flowers_model_e_76.h5\n",
      "Epoch 77/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0675 - sparse_categorical_accuracy: 0.9805 - val_loss: 2.0622 - val_sparse_categorical_accuracy: 0.6243\n",
      "\n",
      "Epoch 00077: saving model to tmp/EfficentNetB0_flowers_model_e_77.h5\n",
      "Epoch 78/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0655 - sparse_categorical_accuracy: 0.9781 - val_loss: 2.1552 - val_sparse_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00078: saving model to tmp/EfficentNetB0_flowers_model_e_78.h5\n",
      "Epoch 79/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9909 - val_loss: 1.6376 - val_sparse_categorical_accuracy: 0.6515\n",
      "\n",
      "Epoch 00079: saving model to tmp/EfficentNetB0_flowers_model_e_79.h5\n",
      "Epoch 80/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0526 - sparse_categorical_accuracy: 0.9798 - val_loss: 1.5966 - val_sparse_categorical_accuracy: 0.6606\n",
      "\n",
      "Epoch 00080: saving model to tmp/EfficentNetB0_flowers_model_e_80.h5\n",
      "Epoch 81/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9872 - val_loss: 1.4941 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00081: saving model to tmp/EfficentNetB0_flowers_model_e_81.h5\n",
      "Epoch 82/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9895 - val_loss: 2.0029 - val_sparse_categorical_accuracy: 0.6334\n",
      "\n",
      "Epoch 00082: saving model to tmp/EfficentNetB0_flowers_model_e_82.h5\n",
      "Epoch 83/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.9881 - val_loss: 1.9075 - val_sparse_categorical_accuracy: 0.6443\n",
      "\n",
      "Epoch 00083: saving model to tmp/EfficentNetB0_flowers_model_e_83.h5\n",
      "Epoch 84/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9780 - val_loss: 2.0079 - val_sparse_categorical_accuracy: 0.6261\n",
      "\n",
      "Epoch 00084: saving model to tmp/EfficentNetB0_flowers_model_e_84.h5\n",
      "Epoch 85/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9871 - val_loss: 1.7466 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00085: saving model to tmp/EfficentNetB0_flowers_model_e_85.h5\n",
      "Epoch 86/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9759 - val_loss: 1.7874 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00086: saving model to tmp/EfficentNetB0_flowers_model_e_86.h5\n",
      "Epoch 87/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9857 - val_loss: 1.6197 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00087: saving model to tmp/EfficentNetB0_flowers_model_e_87.h5\n",
      "Epoch 88/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9864 - val_loss: 1.5604 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00088: saving model to tmp/EfficentNetB0_flowers_model_e_88.h5\n",
      "Epoch 89/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0796 - sparse_categorical_accuracy: 0.9766 - val_loss: 1.9390 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00089: saving model to tmp/EfficentNetB0_flowers_model_e_89.h5\n",
      "Epoch 90/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9874 - val_loss: 1.6901 - val_sparse_categorical_accuracy: 0.6715\n",
      "\n",
      "Epoch 00090: saving model to tmp/EfficentNetB0_flowers_model_e_90.h5\n",
      "Epoch 91/300\n",
      "81/81 [==============================] - 36s 318ms/step - loss: 0.0700 - sparse_categorical_accuracy: 0.9725 - val_loss: 1.7422 - val_sparse_categorical_accuracy: 0.6461\n",
      "\n",
      "Epoch 00091: saving model to tmp/EfficentNetB0_flowers_model_e_91.h5\n",
      "Epoch 92/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.0387 - sparse_categorical_accuracy: 0.9890 - val_loss: 1.4525 - val_sparse_categorical_accuracy: 0.6897\n",
      "\n",
      "Epoch 00092: saving model to tmp/EfficentNetB0_flowers_model_e_92.h5\n",
      "Epoch 93/300\n",
      "81/81 [==============================] - 36s 321ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9878 - val_loss: 1.3697 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00093: saving model to tmp/EfficentNetB0_flowers_model_e_93.h5\n",
      "Epoch 94/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9906 - val_loss: 1.6497 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00094: saving model to tmp/EfficentNetB0_flowers_model_e_94.h5\n",
      "Epoch 95/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.0240 - sparse_categorical_accuracy: 0.9913 - val_loss: 1.8077 - val_sparse_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00095: saving model to tmp/EfficentNetB0_flowers_model_e_95.h5\n",
      "Epoch 96/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9921 - val_loss: 1.8977 - val_sparse_categorical_accuracy: 0.6388\n",
      "\n",
      "Epoch 00096: saving model to tmp/EfficentNetB0_flowers_model_e_96.h5\n",
      "Epoch 97/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0453 - sparse_categorical_accuracy: 0.9856 - val_loss: 1.8084 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00097: saving model to tmp/EfficentNetB0_flowers_model_e_97.h5\n",
      "Epoch 98/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9890 - val_loss: 1.7663 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00098: saving model to tmp/EfficentNetB0_flowers_model_e_98.h5\n",
      "Epoch 99/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9919 - val_loss: 1.6043 - val_sparse_categorical_accuracy: 0.6878\n",
      "\n",
      "Epoch 00099: saving model to tmp/EfficentNetB0_flowers_model_e_99.h5\n",
      "Epoch 100/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0591 - sparse_categorical_accuracy: 0.9792 - val_loss: 2.5218 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00100: saving model to tmp/EfficentNetB0_flowers_model_e_100.h5\n",
      "Epoch 101/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9766 - val_loss: 2.2578 - val_sparse_categorical_accuracy: 0.5953\n",
      "\n",
      "Epoch 00101: saving model to tmp/EfficentNetB0_flowers_model_e_101.h5\n",
      "Epoch 102/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0561 - sparse_categorical_accuracy: 0.9812 - val_loss: 1.5418 - val_sparse_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00102: saving model to tmp/EfficentNetB0_flowers_model_e_102.h5\n",
      "Epoch 103/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0626 - sparse_categorical_accuracy: 0.9797 - val_loss: 1.4976 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00103: saving model to tmp/EfficentNetB0_flowers_model_e_103.h5\n",
      "Epoch 104/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9928 - val_loss: 1.5564 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00104: saving model to tmp/EfficentNetB0_flowers_model_e_104.h5\n",
      "Epoch 105/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9940 - val_loss: 1.4086 - val_sparse_categorical_accuracy: 0.7114\n",
      "\n",
      "Epoch 00105: saving model to tmp/EfficentNetB0_flowers_model_e_105.h5\n",
      "Epoch 106/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0357 - sparse_categorical_accuracy: 0.9880 - val_loss: 1.7703 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00106: saving model to tmp/EfficentNetB0_flowers_model_e_106.h5\n",
      "Epoch 107/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9889 - val_loss: 1.6575 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00107: saving model to tmp/EfficentNetB0_flowers_model_e_107.h5\n",
      "Epoch 108/300\n",
      "81/81 [==============================] - 36s 318ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9927 - val_loss: 1.4659 - val_sparse_categorical_accuracy: 0.7241\n",
      "\n",
      "Epoch 00108: saving model to tmp/EfficentNetB0_flowers_model_e_108.h5\n",
      "Epoch 109/300\n",
      "81/81 [==============================] - 36s 320ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9905 - val_loss: 1.8213 - val_sparse_categorical_accuracy: 0.6298\n",
      "\n",
      "Epoch 00109: saving model to tmp/EfficentNetB0_flowers_model_e_109.h5\n",
      "Epoch 110/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9920 - val_loss: 1.6625 - val_sparse_categorical_accuracy: 0.6878\n",
      "\n",
      "Epoch 00110: saving model to tmp/EfficentNetB0_flowers_model_e_110.h5\n",
      "Epoch 111/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0672 - sparse_categorical_accuracy: 0.9798 - val_loss: 1.7776 - val_sparse_categorical_accuracy: 0.6642\n",
      "\n",
      "Epoch 00111: saving model to tmp/EfficentNetB0_flowers_model_e_111.h5\n",
      "Epoch 112/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0350 - sparse_categorical_accuracy: 0.9909 - val_loss: 1.6343 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00112: saving model to tmp/EfficentNetB0_flowers_model_e_112.h5\n",
      "Epoch 113/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9901 - val_loss: 1.7833 - val_sparse_categorical_accuracy: 0.6606\n",
      "\n",
      "Epoch 00113: saving model to tmp/EfficentNetB0_flowers_model_e_113.h5\n",
      "Epoch 114/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9878 - val_loss: 1.7687 - val_sparse_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00114: saving model to tmp/EfficentNetB0_flowers_model_e_114.h5\n",
      "Epoch 115/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9925 - val_loss: 1.6223 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00115: saving model to tmp/EfficentNetB0_flowers_model_e_115.h5\n",
      "Epoch 116/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0286 - sparse_categorical_accuracy: 0.9887 - val_loss: 1.7232 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00116: saving model to tmp/EfficentNetB0_flowers_model_e_116.h5\n",
      "Epoch 117/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.5863 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00117: saving model to tmp/EfficentNetB0_flowers_model_e_117.h5\n",
      "Epoch 118/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9943 - val_loss: 1.6386 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00118: saving model to tmp/EfficentNetB0_flowers_model_e_118.h5\n",
      "Epoch 119/300\n",
      "81/81 [==============================] - 38s 325ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9897 - val_loss: 1.7166 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00119: saving model to tmp/EfficentNetB0_flowers_model_e_119.h5\n",
      "Epoch 120/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9748 - val_loss: 2.0100 - val_sparse_categorical_accuracy: 0.6080\n",
      "\n",
      "Epoch 00120: saving model to tmp/EfficentNetB0_flowers_model_e_120.h5\n",
      "Epoch 121/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9928 - val_loss: 1.6094 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00121: saving model to tmp/EfficentNetB0_flowers_model_e_121.h5\n",
      "Epoch 122/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9928 - val_loss: 1.6473 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00122: saving model to tmp/EfficentNetB0_flowers_model_e_122.h5\n",
      "Epoch 123/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9917 - val_loss: 1.6791 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00123: saving model to tmp/EfficentNetB0_flowers_model_e_123.h5\n",
      "Epoch 124/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9936 - val_loss: 1.7786 - val_sparse_categorical_accuracy: 0.6606\n",
      "\n",
      "Epoch 00124: saving model to tmp/EfficentNetB0_flowers_model_e_124.h5\n",
      "Epoch 125/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.8993 - val_sparse_categorical_accuracy: 0.6715\n",
      "\n",
      "Epoch 00125: saving model to tmp/EfficentNetB0_flowers_model_e_125.h5\n",
      "Epoch 126/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9970 - val_loss: 1.8549 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00126: saving model to tmp/EfficentNetB0_flowers_model_e_126.h5\n",
      "Epoch 127/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9886 - val_loss: 1.7231 - val_sparse_categorical_accuracy: 0.6606\n",
      "\n",
      "Epoch 00127: saving model to tmp/EfficentNetB0_flowers_model_e_127.h5\n",
      "Epoch 128/300\n",
      "81/81 [==============================] - 37s 317ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9749 - val_loss: 1.8848 - val_sparse_categorical_accuracy: 0.6642\n",
      "\n",
      "Epoch 00128: saving model to tmp/EfficentNetB0_flowers_model_e_128.h5\n",
      "Epoch 129/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0320 - sparse_categorical_accuracy: 0.9891 - val_loss: 2.2201 - val_sparse_categorical_accuracy: 0.5898\n",
      "\n",
      "Epoch 00129: saving model to tmp/EfficentNetB0_flowers_model_e_129.h5\n",
      "Epoch 130/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9898 - val_loss: 2.3235 - val_sparse_categorical_accuracy: 0.6279\n",
      "\n",
      "Epoch 00130: saving model to tmp/EfficentNetB0_flowers_model_e_130.h5\n",
      "Epoch 131/300\n",
      "81/81 [==============================] - 36s 321ms/step - loss: 0.0225 - sparse_categorical_accuracy: 0.9924 - val_loss: 1.6633 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00131: saving model to tmp/EfficentNetB0_flowers_model_e_131.h5\n",
      "Epoch 132/300\n",
      "81/81 [==============================] - 36s 318ms/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9917 - val_loss: 2.4038 - val_sparse_categorical_accuracy: 0.6025\n",
      "\n",
      "Epoch 00132: saving model to tmp/EfficentNetB0_flowers_model_e_132.h5\n",
      "Epoch 133/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9900 - val_loss: 1.6154 - val_sparse_categorical_accuracy: 0.6897\n",
      "\n",
      "Epoch 00133: saving model to tmp/EfficentNetB0_flowers_model_e_133.h5\n",
      "Epoch 134/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0292 - sparse_categorical_accuracy: 0.9898 - val_loss: 1.6147 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00134: saving model to tmp/EfficentNetB0_flowers_model_e_134.h5\n",
      "Epoch 135/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9949 - val_loss: 1.7274 - val_sparse_categorical_accuracy: 0.6860\n",
      "\n",
      "Epoch 00135: saving model to tmp/EfficentNetB0_flowers_model_e_135.h5\n",
      "Epoch 136/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9947 - val_loss: 1.4169 - val_sparse_categorical_accuracy: 0.7060\n",
      "\n",
      "Epoch 00136: saving model to tmp/EfficentNetB0_flowers_model_e_136.h5\n",
      "Epoch 137/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9920 - val_loss: 1.7611 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00137: saving model to tmp/EfficentNetB0_flowers_model_e_137.h5\n",
      "Epoch 138/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9940 - val_loss: 1.6825 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00138: saving model to tmp/EfficentNetB0_flowers_model_e_138.h5\n",
      "Epoch 139/300\n",
      "81/81 [==============================] - 36s 322ms/step - loss: 0.0475 - sparse_categorical_accuracy: 0.9852 - val_loss: 2.3353 - val_sparse_categorical_accuracy: 0.6044\n",
      "\n",
      "Epoch 00139: saving model to tmp/EfficentNetB0_flowers_model_e_139.h5\n",
      "Epoch 140/300\n",
      "81/81 [==============================] - 37s 317ms/step - loss: 0.0330 - sparse_categorical_accuracy: 0.9878 - val_loss: 1.9935 - val_sparse_categorical_accuracy: 0.5935\n",
      "\n",
      "Epoch 00140: saving model to tmp/EfficentNetB0_flowers_model_e_140.h5\n",
      "Epoch 141/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9852 - val_loss: 1.3980 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00141: saving model to tmp/EfficentNetB0_flowers_model_e_141.h5\n",
      "Epoch 142/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9921 - val_loss: 1.5922 - val_sparse_categorical_accuracy: 0.6987\n",
      "\n",
      "Epoch 00142: saving model to tmp/EfficentNetB0_flowers_model_e_142.h5\n",
      "Epoch 143/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.0443 - sparse_categorical_accuracy: 0.9815 - val_loss: 1.6783 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00143: saving model to tmp/EfficentNetB0_flowers_model_e_143.h5\n",
      "Epoch 144/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9865 - val_loss: 2.0977 - val_sparse_categorical_accuracy: 0.6534\n",
      "\n",
      "Epoch 00144: saving model to tmp/EfficentNetB0_flowers_model_e_144.h5\n",
      "Epoch 145/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9940 - val_loss: 1.4463 - val_sparse_categorical_accuracy: 0.7151\n",
      "\n",
      "Epoch 00145: saving model to tmp/EfficentNetB0_flowers_model_e_145.h5\n",
      "Epoch 146/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9987 - val_loss: 1.6313 - val_sparse_categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00146: saving model to tmp/EfficentNetB0_flowers_model_e_146.h5\n",
      "Epoch 147/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9944 - val_loss: 1.5300 - val_sparse_categorical_accuracy: 0.7024\n",
      "\n",
      "Epoch 00147: saving model to tmp/EfficentNetB0_flowers_model_e_147.h5\n",
      "Epoch 148/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9965 - val_loss: 1.5396 - val_sparse_categorical_accuracy: 0.6951\n",
      "\n",
      "Epoch 00148: saving model to tmp/EfficentNetB0_flowers_model_e_148.h5\n",
      "Epoch 149/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9959 - val_loss: 1.9051 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00149: saving model to tmp/EfficentNetB0_flowers_model_e_149.h5\n",
      "Epoch 150/300\n",
      "81/81 [==============================] - 37s 316ms/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9819 - val_loss: 1.7146 - val_sparse_categorical_accuracy: 0.6751\n",
      "\n",
      "Epoch 00150: saving model to tmp/EfficentNetB0_flowers_model_e_150.h5\n",
      "Epoch 151/300\n",
      "81/81 [==============================] - 36s 322ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9896 - val_loss: 1.8326 - val_sparse_categorical_accuracy: 0.6878\n",
      "\n",
      "Epoch 00151: saving model to tmp/EfficentNetB0_flowers_model_e_151.h5\n",
      "Epoch 152/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9978 - val_loss: 1.7281 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00152: saving model to tmp/EfficentNetB0_flowers_model_e_152.h5\n",
      "Epoch 153/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9937 - val_loss: 1.7163 - val_sparse_categorical_accuracy: 0.6987\n",
      "\n",
      "Epoch 00153: saving model to tmp/EfficentNetB0_flowers_model_e_153.h5\n",
      "Epoch 154/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9952 - val_loss: 1.6823 - val_sparse_categorical_accuracy: 0.7042\n",
      "\n",
      "Epoch 00154: saving model to tmp/EfficentNetB0_flowers_model_e_154.h5\n",
      "Epoch 155/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9965 - val_loss: 1.4971 - val_sparse_categorical_accuracy: 0.7078\n",
      "\n",
      "Epoch 00155: saving model to tmp/EfficentNetB0_flowers_model_e_155.h5\n",
      "Epoch 156/300\n",
      "81/81 [==============================] - 36s 321ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9948 - val_loss: 1.6366 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00156: saving model to tmp/EfficentNetB0_flowers_model_e_156.h5\n",
      "Epoch 157/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0243 - sparse_categorical_accuracy: 0.9948 - val_loss: 1.7266 - val_sparse_categorical_accuracy: 0.6642\n",
      "\n",
      "Epoch 00157: saving model to tmp/EfficentNetB0_flowers_model_e_157.h5\n",
      "Epoch 158/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9897 - val_loss: 1.5993 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00158: saving model to tmp/EfficentNetB0_flowers_model_e_158.h5\n",
      "Epoch 159/300\n",
      "81/81 [==============================] - 37s 314ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9920 - val_loss: 1.5981 - val_sparse_categorical_accuracy: 0.6860\n",
      "\n",
      "Epoch 00159: saving model to tmp/EfficentNetB0_flowers_model_e_159.h5\n",
      "Epoch 160/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9924 - val_loss: 1.7652 - val_sparse_categorical_accuracy: 0.6715\n",
      "\n",
      "Epoch 00160: saving model to tmp/EfficentNetB0_flowers_model_e_160.h5\n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9938 - val_loss: 1.7747 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00161: saving model to tmp/EfficentNetB0_flowers_model_e_161.h5\n",
      "Epoch 162/300\n",
      "81/81 [==============================] - 36s 322ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9934 - val_loss: 1.7907 - val_sparse_categorical_accuracy: 0.6806\n",
      "\n",
      "Epoch 00162: saving model to tmp/EfficentNetB0_flowers_model_e_162.h5\n",
      "Epoch 163/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9971 - val_loss: 1.6565 - val_sparse_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00163: saving model to tmp/EfficentNetB0_flowers_model_e_163.h5\n",
      "Epoch 164/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9946 - val_loss: 1.6309 - val_sparse_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00164: saving model to tmp/EfficentNetB0_flowers_model_e_164.h5\n",
      "Epoch 165/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9927 - val_loss: 3.4696 - val_sparse_categorical_accuracy: 0.5662\n",
      "\n",
      "Epoch 00165: saving model to tmp/EfficentNetB0_flowers_model_e_165.h5\n",
      "Epoch 166/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0261 - sparse_categorical_accuracy: 0.9903 - val_loss: 2.1369 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00166: saving model to tmp/EfficentNetB0_flowers_model_e_166.h5\n",
      "Epoch 167/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9901 - val_loss: 1.9840 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00167: saving model to tmp/EfficentNetB0_flowers_model_e_167.h5\n",
      "Epoch 168/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9795 - val_sparse_categorical_accuracy: 0.6298\n",
      "\n",
      "Epoch 00168: saving model to tmp/EfficentNetB0_flowers_model_e_168.h5\n",
      "Epoch 169/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9916 - val_loss: 2.1254 - val_sparse_categorical_accuracy: 0.6443\n",
      "\n",
      "Epoch 00169: saving model to tmp/EfficentNetB0_flowers_model_e_169.h5\n",
      "Epoch 170/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9903 - val_loss: 1.5006 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00170: saving model to tmp/EfficentNetB0_flowers_model_e_170.h5\n",
      "Epoch 171/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9941 - val_loss: 1.7186 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00171: saving model to tmp/EfficentNetB0_flowers_model_e_171.h5\n",
      "Epoch 172/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9934 - val_loss: 1.8132 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00172: saving model to tmp/EfficentNetB0_flowers_model_e_172.h5\n",
      "Epoch 173/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9877 - val_loss: 1.5169 - val_sparse_categorical_accuracy: 0.6951\n",
      "\n",
      "Epoch 00173: saving model to tmp/EfficentNetB0_flowers_model_e_173.h5\n",
      "Epoch 174/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9918 - val_loss: 2.1676 - val_sparse_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00174: saving model to tmp/EfficentNetB0_flowers_model_e_174.h5\n",
      "Epoch 175/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9851 - val_loss: 1.8983 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00175: saving model to tmp/EfficentNetB0_flowers_model_e_175.h5\n",
      "Epoch 176/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9889 - val_loss: 1.5682 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00176: saving model to tmp/EfficentNetB0_flowers_model_e_176.h5\n",
      "Epoch 177/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5893 - val_sparse_categorical_accuracy: 0.7024\n",
      "\n",
      "Epoch 00177: saving model to tmp/EfficentNetB0_flowers_model_e_177.h5\n",
      "Epoch 178/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7056 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00178: saving model to tmp/EfficentNetB0_flowers_model_e_178.h5\n",
      "Epoch 179/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9972 - val_loss: 1.5433 - val_sparse_categorical_accuracy: 0.6951\n",
      "\n",
      "Epoch 00179: saving model to tmp/EfficentNetB0_flowers_model_e_179.h5\n",
      "Epoch 180/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9957 - val_loss: 1.5426 - val_sparse_categorical_accuracy: 0.7078\n",
      "\n",
      "Epoch 00180: saving model to tmp/EfficentNetB0_flowers_model_e_180.h5\n",
      "Epoch 181/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9947 - val_loss: 1.5103 - val_sparse_categorical_accuracy: 0.6824\n",
      "\n",
      "Epoch 00181: saving model to tmp/EfficentNetB0_flowers_model_e_181.h5\n",
      "Epoch 182/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9944 - val_loss: 1.7147 - val_sparse_categorical_accuracy: 0.6697\n",
      "\n",
      "Epoch 00182: saving model to tmp/EfficentNetB0_flowers_model_e_182.h5\n",
      "Epoch 183/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9935 - val_loss: 1.8950 - val_sparse_categorical_accuracy: 0.6860\n",
      "\n",
      "Epoch 00183: saving model to tmp/EfficentNetB0_flowers_model_e_183.h5\n",
      "Epoch 184/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9948 - val_loss: 1.8336 - val_sparse_categorical_accuracy: 0.6479\n",
      "\n",
      "Epoch 00184: saving model to tmp/EfficentNetB0_flowers_model_e_184.h5\n",
      "Epoch 185/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9831 - val_loss: 2.2174 - val_sparse_categorical_accuracy: 0.6407\n",
      "\n",
      "Epoch 00185: saving model to tmp/EfficentNetB0_flowers_model_e_185.h5\n",
      "Epoch 186/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0520 - sparse_categorical_accuracy: 0.9869 - val_loss: 1.7300 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00186: saving model to tmp/EfficentNetB0_flowers_model_e_186.h5\n",
      "Epoch 187/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9970 - val_loss: 1.6219 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00187: saving model to tmp/EfficentNetB0_flowers_model_e_187.h5\n",
      "Epoch 188/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9934 - val_loss: 1.8848 - val_sparse_categorical_accuracy: 0.6715\n",
      "\n",
      "Epoch 00188: saving model to tmp/EfficentNetB0_flowers_model_e_188.h5\n",
      "Epoch 189/300\n",
      "81/81 [==============================] - 36s 318ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9944 - val_loss: 1.5995 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00189: saving model to tmp/EfficentNetB0_flowers_model_e_189.h5\n",
      "Epoch 190/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0079 - sparse_categorical_accuracy: 0.9976 - val_loss: 1.5025 - val_sparse_categorical_accuracy: 0.6987\n",
      "\n",
      "Epoch 00190: saving model to tmp/EfficentNetB0_flowers_model_e_190.h5\n",
      "Epoch 191/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0064 - sparse_categorical_accuracy: 0.9984 - val_loss: 1.8350 - val_sparse_categorical_accuracy: 0.6878\n",
      "\n",
      "Epoch 00191: saving model to tmp/EfficentNetB0_flowers_model_e_191.h5\n",
      "Epoch 192/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.7390 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00192: saving model to tmp/EfficentNetB0_flowers_model_e_192.h5\n",
      "Epoch 193/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.0727 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00193: saving model to tmp/EfficentNetB0_flowers_model_e_193.h5\n",
      "Epoch 194/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0358 - sparse_categorical_accuracy: 0.9902 - val_loss: 2.6305 - val_sparse_categorical_accuracy: 0.5971\n",
      "\n",
      "Epoch 00194: saving model to tmp/EfficentNetB0_flowers_model_e_194.h5\n",
      "Epoch 195/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0408 - sparse_categorical_accuracy: 0.9845 - val_loss: 2.1359 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00195: saving model to tmp/EfficentNetB0_flowers_model_e_195.h5\n",
      "Epoch 196/300\n",
      "81/81 [==============================] - 37s 323ms/step - loss: 0.0306 - sparse_categorical_accuracy: 0.9903 - val_loss: 1.7453 - val_sparse_categorical_accuracy: 0.6897\n",
      "\n",
      "Epoch 00196: saving model to tmp/EfficentNetB0_flowers_model_e_196.h5\n",
      "Epoch 197/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9962 - val_loss: 1.9696 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00197: saving model to tmp/EfficentNetB0_flowers_model_e_197.h5\n",
      "Epoch 198/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9959 - val_loss: 1.8806 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00198: saving model to tmp/EfficentNetB0_flowers_model_e_198.h5\n",
      "Epoch 199/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9865 - val_loss: 1.7237 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00199: saving model to tmp/EfficentNetB0_flowers_model_e_199.h5\n",
      "Epoch 200/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9906 - val_loss: 1.6005 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00200: saving model to tmp/EfficentNetB0_flowers_model_e_200.h5\n",
      "Epoch 201/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9929 - val_loss: 1.7585 - val_sparse_categorical_accuracy: 0.6951\n",
      "\n",
      "Epoch 00201: saving model to tmp/EfficentNetB0_flowers_model_e_201.h5\n",
      "Epoch 202/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9929 - val_loss: 1.4029 - val_sparse_categorical_accuracy: 0.7278\n",
      "\n",
      "Epoch 00202: saving model to tmp/EfficentNetB0_flowers_model_e_202.h5\n",
      "Epoch 203/300\n",
      "81/81 [==============================] - 36s 320ms/step - loss: 0.0602 - sparse_categorical_accuracy: 0.9802 - val_loss: 1.6396 - val_sparse_categorical_accuracy: 0.7024\n",
      "\n",
      "Epoch 00203: saving model to tmp/EfficentNetB0_flowers_model_e_203.h5\n",
      "Epoch 204/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9888 - val_loss: 1.5928 - val_sparse_categorical_accuracy: 0.7114\n",
      "\n",
      "Epoch 00204: saving model to tmp/EfficentNetB0_flowers_model_e_204.h5\n",
      "Epoch 205/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9896 - val_loss: 1.6451 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00205: saving model to tmp/EfficentNetB0_flowers_model_e_205.h5\n",
      "Epoch 206/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0494 - sparse_categorical_accuracy: 0.9874 - val_loss: 1.7069 - val_sparse_categorical_accuracy: 0.6642\n",
      "\n",
      "Epoch 00206: saving model to tmp/EfficentNetB0_flowers_model_e_206.h5\n",
      "Epoch 207/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5676 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00207: saving model to tmp/EfficentNetB0_flowers_model_e_207.h5\n",
      "Epoch 208/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6836 - val_sparse_categorical_accuracy: 0.6679\n",
      "\n",
      "Epoch 00208: saving model to tmp/EfficentNetB0_flowers_model_e_208.h5\n",
      "Epoch 209/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9951 - val_loss: 1.5776 - val_sparse_categorical_accuracy: 0.7005\n",
      "\n",
      "Epoch 00209: saving model to tmp/EfficentNetB0_flowers_model_e_209.h5\n",
      "Epoch 210/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.6691 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00210: saving model to tmp/EfficentNetB0_flowers_model_e_210.h5\n",
      "Epoch 211/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9981 - val_loss: 1.7023 - val_sparse_categorical_accuracy: 0.7042\n",
      "\n",
      "Epoch 00211: saving model to tmp/EfficentNetB0_flowers_model_e_211.h5\n",
      "Epoch 212/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7484 - val_sparse_categorical_accuracy: 0.7024\n",
      "\n",
      "Epoch 00212: saving model to tmp/EfficentNetB0_flowers_model_e_212.h5\n",
      "Epoch 213/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9976 - val_loss: 1.7530 - val_sparse_categorical_accuracy: 0.7042\n",
      "\n",
      "Epoch 00213: saving model to tmp/EfficentNetB0_flowers_model_e_213.h5\n",
      "Epoch 214/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9970 - val_loss: 1.8299 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00214: saving model to tmp/EfficentNetB0_flowers_model_e_214.h5\n",
      "Epoch 215/300\n",
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9989 - val_loss: 1.7176 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00215: saving model to tmp/EfficentNetB0_flowers_model_e_215.h5\n",
      "Epoch 216/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9958 - val_loss: 1.8785 - val_sparse_categorical_accuracy: 0.6969\n",
      "\n",
      "Epoch 00216: saving model to tmp/EfficentNetB0_flowers_model_e_216.h5\n",
      "Epoch 217/300\n",
      "81/81 [==============================] - 38s 322ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7873 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00217: saving model to tmp/EfficentNetB0_flowers_model_e_217.h5\n",
      "Epoch 218/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9967 - val_loss: 1.7306 - val_sparse_categorical_accuracy: 0.7060\n",
      "\n",
      "Epoch 00218: saving model to tmp/EfficentNetB0_flowers_model_e_218.h5\n",
      "Epoch 219/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9972 - val_loss: 1.9830 - val_sparse_categorical_accuracy: 0.6588\n",
      "\n",
      "Epoch 00219: saving model to tmp/EfficentNetB0_flowers_model_e_219.h5\n",
      "Epoch 220/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9964 - val_loss: 1.6953 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00220: saving model to tmp/EfficentNetB0_flowers_model_e_220.h5\n",
      "Epoch 221/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9990 - val_loss: 1.5916 - val_sparse_categorical_accuracy: 0.6788\n",
      "\n",
      "Epoch 00221: saving model to tmp/EfficentNetB0_flowers_model_e_221.h5\n",
      "Epoch 222/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9978 - val_loss: 1.7486 - val_sparse_categorical_accuracy: 0.6733\n",
      "\n",
      "Epoch 00222: saving model to tmp/EfficentNetB0_flowers_model_e_222.h5\n",
      "Epoch 223/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0094 - sparse_categorical_accuracy: 0.9969 - val_loss: 2.0177 - val_sparse_categorical_accuracy: 0.6770\n",
      "\n",
      "Epoch 00223: saving model to tmp/EfficentNetB0_flowers_model_e_223.h5\n",
      "Epoch 224/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9948 - val_loss: 2.1933 - val_sparse_categorical_accuracy: 0.6497\n",
      "\n",
      "Epoch 00224: saving model to tmp/EfficentNetB0_flowers_model_e_224.h5\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 36s 319ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9944 - val_loss: 1.8471 - val_sparse_categorical_accuracy: 0.6661\n",
      "\n",
      "Epoch 00225: saving model to tmp/EfficentNetB0_flowers_model_e_225.h5\n",
      "Epoch 226/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0321 - sparse_categorical_accuracy: 0.9913 - val_loss: 2.2367 - val_sparse_categorical_accuracy: 0.6552\n",
      "\n",
      "Epoch 00226: saving model to tmp/EfficentNetB0_flowers_model_e_226.h5\n",
      "Epoch 227/300\n",
      "81/81 [==============================] - 38s 320ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9957 - val_loss: 1.7300 - val_sparse_categorical_accuracy: 0.7223\n",
      "\n",
      "Epoch 00227: saving model to tmp/EfficentNetB0_flowers_model_e_227.h5\n",
      "Epoch 228/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9934 - val_loss: 2.0916 - val_sparse_categorical_accuracy: 0.6679\n",
      "\n",
      "Epoch 00228: saving model to tmp/EfficentNetB0_flowers_model_e_228.h5\n",
      "Epoch 229/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9957 - val_loss: 1.8910 - val_sparse_categorical_accuracy: 0.6933\n",
      "\n",
      "Epoch 00229: saving model to tmp/EfficentNetB0_flowers_model_e_229.h5\n",
      "Epoch 230/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8765 - val_sparse_categorical_accuracy: 0.6897\n",
      "\n",
      "Epoch 00230: saving model to tmp/EfficentNetB0_flowers_model_e_230.h5\n",
      "Epoch 231/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9898 - val_loss: 1.9001 - val_sparse_categorical_accuracy: 0.6860\n",
      "\n",
      "Epoch 00231: saving model to tmp/EfficentNetB0_flowers_model_e_231.h5\n",
      "Epoch 232/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9946 - val_loss: 1.6297 - val_sparse_categorical_accuracy: 0.6878\n",
      "\n",
      "Epoch 00232: saving model to tmp/EfficentNetB0_flowers_model_e_232.h5\n",
      "Epoch 233/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9943 - val_loss: 1.5065 - val_sparse_categorical_accuracy: 0.6987\n",
      "\n",
      "Epoch 00233: saving model to tmp/EfficentNetB0_flowers_model_e_233.h5\n",
      "Epoch 234/300\n",
      "81/81 [==============================] - 37s 320ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9915 - val_loss: 1.6565 - val_sparse_categorical_accuracy: 0.7024\n",
      "\n",
      "Epoch 00234: saving model to tmp/EfficentNetB0_flowers_model_e_234.h5\n",
      "Epoch 235/300\n",
      "81/81 [==============================] - 37s 318ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9969 - val_loss: 1.6862 - val_sparse_categorical_accuracy: 0.6897\n",
      "\n",
      "Epoch 00235: saving model to tmp/EfficentNetB0_flowers_model_e_235.h5\n",
      "Epoch 236/300\n",
      "81/81 [==============================] - 37s 322ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9961 - val_loss: 1.7158 - val_sparse_categorical_accuracy: 0.6915\n",
      "\n",
      "Epoch 00236: saving model to tmp/EfficentNetB0_flowers_model_e_236.h5\n",
      "Epoch 237/300\n",
      "81/81 [==============================] - 37s 319ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9979 - val_loss: 1.7626 - val_sparse_categorical_accuracy: 0.7078\n",
      "\n",
      "Epoch 00237: saving model to tmp/EfficentNetB0_flowers_model_e_237.h5\n",
      "Epoch 238/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9984 - val_loss: 1.8622 - val_sparse_categorical_accuracy: 0.6878\n",
      "\n",
      "Epoch 00238: saving model to tmp/EfficentNetB0_flowers_model_e_238.h5\n",
      "Epoch 239/300\n",
      "81/81 [==============================] - 37s 321ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9952 - val_loss: 2.0344 - val_sparse_categorical_accuracy: 0.6624\n",
      "\n",
      "Epoch 00239: saving model to tmp/EfficentNetB0_flowers_model_e_239.h5\n",
      "Epoch 240/300\n",
      "34/81 [===========>..................] - ETA: 14s - loss: 0.0123 - sparse_categorical_accuracy: 0.9959"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB0(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_flowers(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = './flowers_models/EfficentNetB4_flowers_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB4(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
