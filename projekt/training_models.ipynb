{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "from os import path\n",
    "%load_ext tensorboard\n",
    "\n",
    "def normalize(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo change this to 100+\n",
    "\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, ds_train, ds_validation, model_name, batch_size=64):\n",
    "#    if path.exists(model_path):\n",
    "#        print(\"Model is already trained and saved here: \" + model_path)\n",
    "#        return\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    \n",
    "    save = callbacks.ModelCheckpoint(\n",
    "        os.path.join('tmp', model_name + \"_e_{epoch:02d}.h5\"),\n",
    "        monitor='loss',\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        mode='auto')\n",
    "\n",
    "    early = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                    min_delta=0,\n",
    "                                    patience=30,\n",
    "                                    verbose=1,\n",
    "                                    mode='auto')\n",
    "\n",
    "    hist = model.fit(ds_train, \n",
    "                     epochs=NUM_EPOCHS, \n",
    "                     validation_data=ds_validation,\n",
    "                     callbacks=[save]\n",
    "                    )\n",
    "    \n",
    "    # save model\n",
    "    model.save(model_name + \".h5\")\n",
    "    print('Saved to: ' + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ds_to_tensors(ds):\n",
    "    \"\"\"returns tuple of train_X, train_y\"\"\"\n",
    "    a = ds.map(lambda a, b: a)\n",
    "    tf_list = []\n",
    "    for i in a:\n",
    "        tf_list.append(i)\n",
    "    train_X = tf.stack(tf_list, axis=0)\n",
    "    \n",
    "    b = ds.map(lambda a, b: b)\n",
    "    tf_list = []\n",
    "    for i in b:\n",
    "        tf_list.append([i.numpy()])\n",
    "    train_y = np.array(tf_list, dtype=np.uint8)\n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beans dataset\n",
    "\n",
    "https://www.tensorflow.org/datasets/catalog/beans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_beans_train(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)    \n",
    "    return image, label\n",
    "\n",
    "def preprocess_beans_test_and_val(image, label):\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrap model for beans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model_for_beans(base_model, num_classes):\n",
    "    inputs = base_model.inputs\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "INPUT_SHAPE = (500, 500, 3)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_beans_datasets():\n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
    "        'beans',\n",
    "        split=['train', 'validation', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    \n",
    "    ds_train = ds_train.map(normalize)\n",
    "    #ds_train = ds_train.map(preprocess_beans_train)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples).batch(BATCH_SIZE)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize)\n",
    "    #ds_validation = ds_validation.map(preprocess_beans_test_and_val)\n",
    "    ds_validation = ds_validation.batch(BATCH_SIZE)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize)\n",
    "    #ds_test = ds_test.map(preprocess_beans_test_and_val)\n",
    "    ds_test = ds_test.batch(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_validation, ds_test = load_beans_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'MobileNetV2_beans_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNets - B0, B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"EfficentNetB0_beans_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'EfficentNetB4_beans_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB4(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flowers dataset\n",
    "\n",
    "https://www.tensorflow.org/datasets/catalog/oxford_flowers102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        image, size=[256, 256, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_flowers_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    return image, label\n",
    "\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess_flowers(image, label):\n",
    "    image = tf.image.resize(image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrap model for flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_model_for_flowers(base_model, num_classes):\n",
    "    inputs = base_model.inputs\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 102\n",
    "INPUT_SHAPE = (256, 256, 3)\n",
    "BATCH_SIZE = 32\n",
    "RESIZE_DIMENSION = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flowers_dataset():  \n",
    "    (ds_train, ds_validation, ds_test), ds_info = tfds.load(name=\"oxford_flowers102\", \n",
    "                                             with_info=True,\n",
    "                                             split=['train', 'validation', 'test'],  #70/15/15 split\n",
    "                                             as_supervised=True)\n",
    "\n",
    "    ds_train = ds_train.map(normalize, \n",
    "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)    \n",
    "    ds_train = ds_train.map(preprocess_flowers)\n",
    "    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE)\n",
    "    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    ds_validation = ds_validation.map(normalize, \n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_validation = ds_validation.map(preprocess_flowers)\n",
    "    ds_validation = ds_validation.batch(BATCH_SIZE)\n",
    "    ds_validation = ds_validation.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    ds_test = ds_test.map(normalize, \n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds_test = ds_test.map(preprocess_flowers)\n",
    "    ds_test = ds_test.batch(BATCH_SIZE)\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds_train, ds_validation, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_validation, ds_test = load_flowers_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'MobileNetV2_flowers_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 19s 353ms/step - loss: 4.7947 - sparse_categorical_accuracy: 0.0126 - val_loss: 4.6250 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00001: saving model to tmp/MobileNetV2_flowers_model_e_01.h5\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 4.4562 - sparse_categorical_accuracy: 0.0230 - val_loss: 4.6253 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00002: saving model to tmp/MobileNetV2_flowers_model_e_02.h5\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 4.1323 - sparse_categorical_accuracy: 0.0635 - val_loss: 4.6261 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00003: saving model to tmp/MobileNetV2_flowers_model_e_03.h5\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 15s 328ms/step - loss: 3.7212 - sparse_categorical_accuracy: 0.1038 - val_loss: 4.6277 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00004: saving model to tmp/MobileNetV2_flowers_model_e_04.h5\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 3.4269 - sparse_categorical_accuracy: 0.1514 - val_loss: 4.6305 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00005: saving model to tmp/MobileNetV2_flowers_model_e_05.h5\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 3.1716 - sparse_categorical_accuracy: 0.2019 - val_loss: 4.6346 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00006: saving model to tmp/MobileNetV2_flowers_model_e_06.h5\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 2.9763 - sparse_categorical_accuracy: 0.2505 - val_loss: 4.6396 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00007: saving model to tmp/MobileNetV2_flowers_model_e_07.h5\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 2.7303 - sparse_categorical_accuracy: 0.3011 - val_loss: 4.6481 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00008: saving model to tmp/MobileNetV2_flowers_model_e_08.h5\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 2.5311 - sparse_categorical_accuracy: 0.3511 - val_loss: 4.6590 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00009: saving model to tmp/MobileNetV2_flowers_model_e_09.h5\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 2.3179 - sparse_categorical_accuracy: 0.3917 - val_loss: 4.6713 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00010: saving model to tmp/MobileNetV2_flowers_model_e_10.h5\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 2.1847 - sparse_categorical_accuracy: 0.4179 - val_loss: 4.6832 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00011: saving model to tmp/MobileNetV2_flowers_model_e_11.h5\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 2.0761 - sparse_categorical_accuracy: 0.4284 - val_loss: 4.7029 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00012: saving model to tmp/MobileNetV2_flowers_model_e_12.h5\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 1.7404 - sparse_categorical_accuracy: 0.5028 - val_loss: 4.7241 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00013: saving model to tmp/MobileNetV2_flowers_model_e_13.h5\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 1.5670 - sparse_categorical_accuracy: 0.5882 - val_loss: 4.7480 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00014: saving model to tmp/MobileNetV2_flowers_model_e_14.h5\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 1.4150 - sparse_categorical_accuracy: 0.6532 - val_loss: 4.7738 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00015: saving model to tmp/MobileNetV2_flowers_model_e_15.h5\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 1.1682 - sparse_categorical_accuracy: 0.7296 - val_loss: 4.8040 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00016: saving model to tmp/MobileNetV2_flowers_model_e_16.h5\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 15s 314ms/step - loss: 1.2799 - sparse_categorical_accuracy: 0.6592 - val_loss: 4.8379 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00017: saving model to tmp/MobileNetV2_flowers_model_e_17.h5\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 1.0063 - sparse_categorical_accuracy: 0.7757 - val_loss: 4.8690 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00018: saving model to tmp/MobileNetV2_flowers_model_e_18.h5\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 0.8927 - sparse_categorical_accuracy: 0.7772 - val_loss: 4.9170 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00019: saving model to tmp/MobileNetV2_flowers_model_e_19.h5\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 15s 326ms/step - loss: 0.7310 - sparse_categorical_accuracy: 0.8459 - val_loss: 4.9509 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00020: saving model to tmp/MobileNetV2_flowers_model_e_20.h5\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.8724 - val_loss: 4.9958 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00021: saving model to tmp/MobileNetV2_flowers_model_e_21.h5\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 0.4961 - sparse_categorical_accuracy: 0.8956 - val_loss: 5.0530 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00022: saving model to tmp/MobileNetV2_flowers_model_e_22.h5\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 14s 310ms/step - loss: 0.4502 - sparse_categorical_accuracy: 0.9231 - val_loss: 5.0881 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00023: saving model to tmp/MobileNetV2_flowers_model_e_23.h5\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 0.4228 - sparse_categorical_accuracy: 0.9192 - val_loss: 5.1342 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00024: saving model to tmp/MobileNetV2_flowers_model_e_24.h5\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.2862 - sparse_categorical_accuracy: 0.9560 - val_loss: 5.1843 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00025: saving model to tmp/MobileNetV2_flowers_model_e_25.h5\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.2944 - sparse_categorical_accuracy: 0.9557 - val_loss: 5.2569 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00026: saving model to tmp/MobileNetV2_flowers_model_e_26.h5\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.2037 - sparse_categorical_accuracy: 0.9733 - val_loss: 5.2967 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00027: saving model to tmp/MobileNetV2_flowers_model_e_27.h5\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.2028 - sparse_categorical_accuracy: 0.9685 - val_loss: 5.3667 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00028: saving model to tmp/MobileNetV2_flowers_model_e_28.h5\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.1570 - sparse_categorical_accuracy: 0.9883 - val_loss: 5.4027 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00029: saving model to tmp/MobileNetV2_flowers_model_e_29.h5\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9951 - val_loss: 5.4740 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00030: saving model to tmp/MobileNetV2_flowers_model_e_30.h5\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0863 - sparse_categorical_accuracy: 0.9934 - val_loss: 5.5243 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00031: saving model to tmp/MobileNetV2_flowers_model_e_31.h5\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 0.1184 - sparse_categorical_accuracy: 0.9914 - val_loss: 5.5435 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00032: saving model to tmp/MobileNetV2_flowers_model_e_32.h5\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.1587 - sparse_categorical_accuracy: 0.9753 - val_loss: 5.6338 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00033: saving model to tmp/MobileNetV2_flowers_model_e_33.h5\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 0.1908 - sparse_categorical_accuracy: 0.9600 - val_loss: 5.6181 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00034: saving model to tmp/MobileNetV2_flowers_model_e_34.h5\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.2012 - sparse_categorical_accuracy: 0.9654 - val_loss: 5.7292 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00035: saving model to tmp/MobileNetV2_flowers_model_e_35.h5\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.1776 - sparse_categorical_accuracy: 0.9711 - val_loss: 5.7513 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00036: saving model to tmp/MobileNetV2_flowers_model_e_36.h5\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 14s 319ms/step - loss: 0.1654 - sparse_categorical_accuracy: 0.9703 - val_loss: 5.8941 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00037: saving model to tmp/MobileNetV2_flowers_model_e_37.h5\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.1973 - sparse_categorical_accuracy: 0.9560 - val_loss: 5.9927 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00038: saving model to tmp/MobileNetV2_flowers_model_e_38.h5\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.1994 - sparse_categorical_accuracy: 0.9584 - val_loss: 5.9392 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00039: saving model to tmp/MobileNetV2_flowers_model_e_39.h5\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.1755 - sparse_categorical_accuracy: 0.9680 - val_loss: 6.1791 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00040: saving model to tmp/MobileNetV2_flowers_model_e_40.h5\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.1597 - sparse_categorical_accuracy: 0.9647 - val_loss: 6.0627 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00041: saving model to tmp/MobileNetV2_flowers_model_e_41.h5\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.1474 - sparse_categorical_accuracy: 0.9827 - val_loss: 6.2029 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00042: saving model to tmp/MobileNetV2_flowers_model_e_42.h5\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 14s 316ms/step - loss: 0.1210 - sparse_categorical_accuracy: 0.9730 - val_loss: 6.2596 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00043: saving model to tmp/MobileNetV2_flowers_model_e_43.h5\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.1221 - sparse_categorical_accuracy: 0.9799 - val_loss: 6.3364 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00044: saving model to tmp/MobileNetV2_flowers_model_e_44.h5\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 14s 319ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.9866 - val_loss: 6.4391 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00045: saving model to tmp/MobileNetV2_flowers_model_e_45.h5\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 0.0987 - sparse_categorical_accuracy: 0.9795 - val_loss: 6.3310 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00046: saving model to tmp/MobileNetV2_flowers_model_e_46.h5\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0773 - sparse_categorical_accuracy: 0.9853 - val_loss: 6.4984 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00047: saving model to tmp/MobileNetV2_flowers_model_e_47.h5\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.0710 - sparse_categorical_accuracy: 0.9845 - val_loss: 6.5228 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00048: saving model to tmp/MobileNetV2_flowers_model_e_48.h5\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0780 - sparse_categorical_accuracy: 0.9858 - val_loss: 6.5110 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00049: saving model to tmp/MobileNetV2_flowers_model_e_49.h5\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 0.0708 - sparse_categorical_accuracy: 0.9906 - val_loss: 6.5409 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00050: saving model to tmp/MobileNetV2_flowers_model_e_50.h5\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 14s 320ms/step - loss: 0.0575 - sparse_categorical_accuracy: 0.9925 - val_loss: 6.6988 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00051: saving model to tmp/MobileNetV2_flowers_model_e_51.h5\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0400 - sparse_categorical_accuracy: 0.9950 - val_loss: 6.8265 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00052: saving model to tmp/MobileNetV2_flowers_model_e_52.h5\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 15s 330ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9943 - val_loss: 6.8915 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00053: saving model to tmp/MobileNetV2_flowers_model_e_53.h5\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9976 - val_loss: 6.8002 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00054: saving model to tmp/MobileNetV2_flowers_model_e_54.h5\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9986 - val_loss: 6.8754 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00055: saving model to tmp/MobileNetV2_flowers_model_e_55.h5\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9928 - val_loss: 6.8522 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00056: saving model to tmp/MobileNetV2_flowers_model_e_56.h5\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0717 - sparse_categorical_accuracy: 0.9875 - val_loss: 7.0185 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00057: saving model to tmp/MobileNetV2_flowers_model_e_57.h5\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 0.0527 - sparse_categorical_accuracy: 0.9935 - val_loss: 6.8093 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00058: saving model to tmp/MobileNetV2_flowers_model_e_58.h5\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 14s 311ms/step - loss: 0.0530 - sparse_categorical_accuracy: 0.9961 - val_loss: 7.0590 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00059: saving model to tmp/MobileNetV2_flowers_model_e_59.h5\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0542 - sparse_categorical_accuracy: 0.9916 - val_loss: 6.9799 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00060: saving model to tmp/MobileNetV2_flowers_model_e_60.h5\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0725 - sparse_categorical_accuracy: 0.9830 - val_loss: 7.3092 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00061: saving model to tmp/MobileNetV2_flowers_model_e_61.h5\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0501 - sparse_categorical_accuracy: 0.9961 - val_loss: 7.2209 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00062: saving model to tmp/MobileNetV2_flowers_model_e_62.h5\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9931 - val_loss: 7.3512 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00063: saving model to tmp/MobileNetV2_flowers_model_e_63.h5\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0712 - sparse_categorical_accuracy: 0.9892 - val_loss: 7.4299 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00064: saving model to tmp/MobileNetV2_flowers_model_e_64.h5\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0629 - sparse_categorical_accuracy: 0.9855 - val_loss: 7.2120 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00065: saving model to tmp/MobileNetV2_flowers_model_e_65.h5\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.1097 - sparse_categorical_accuracy: 0.9814 - val_loss: 7.3488 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00066: saving model to tmp/MobileNetV2_flowers_model_e_66.h5\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 0.1160 - sparse_categorical_accuracy: 0.9711 - val_loss: 7.3221 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00067: saving model to tmp/MobileNetV2_flowers_model_e_67.h5\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.9770 - val_loss: 7.1828 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00068: saving model to tmp/MobileNetV2_flowers_model_e_68.h5\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 0.0929 - sparse_categorical_accuracy: 0.9782 - val_loss: 7.3274 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00069: saving model to tmp/MobileNetV2_flowers_model_e_69.h5\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.1184 - sparse_categorical_accuracy: 0.9717 - val_loss: 7.5852 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00070: saving model to tmp/MobileNetV2_flowers_model_e_70.h5\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.1883 - sparse_categorical_accuracy: 0.9438 - val_loss: 7.9200 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00071: saving model to tmp/MobileNetV2_flowers_model_e_71.h5\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.1825 - sparse_categorical_accuracy: 0.9480 - val_loss: 7.9673 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00072: saving model to tmp/MobileNetV2_flowers_model_e_72.h5\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.2293 - sparse_categorical_accuracy: 0.9397 - val_loss: 8.3296 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00073: saving model to tmp/MobileNetV2_flowers_model_e_73.h5\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.1785 - sparse_categorical_accuracy: 0.9489 - val_loss: 8.0866 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00074: saving model to tmp/MobileNetV2_flowers_model_e_74.h5\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 15s 329ms/step - loss: 0.1364 - sparse_categorical_accuracy: 0.9690 - val_loss: 8.4155 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00075: saving model to tmp/MobileNetV2_flowers_model_e_75.h5\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9682 - val_loss: 8.0579 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00076: saving model to tmp/MobileNetV2_flowers_model_e_76.h5\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 14s 316ms/step - loss: 0.0893 - sparse_categorical_accuracy: 0.9856 - val_loss: 8.6299 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00077: saving model to tmp/MobileNetV2_flowers_model_e_77.h5\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0816 - sparse_categorical_accuracy: 0.9849 - val_loss: 8.6617 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00078: saving model to tmp/MobileNetV2_flowers_model_e_78.h5\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0477 - sparse_categorical_accuracy: 0.9901 - val_loss: 8.2915 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00079: saving model to tmp/MobileNetV2_flowers_model_e_79.h5\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0610 - sparse_categorical_accuracy: 0.9867 - val_loss: 8.4287 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00080: saving model to tmp/MobileNetV2_flowers_model_e_80.h5\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0586 - sparse_categorical_accuracy: 0.9913 - val_loss: 8.2594 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00081: saving model to tmp/MobileNetV2_flowers_model_e_81.h5\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0409 - sparse_categorical_accuracy: 0.9937 - val_loss: 8.6106 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00082: saving model to tmp/MobileNetV2_flowers_model_e_82.h5\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9921 - val_loss: 8.3983 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00083: saving model to tmp/MobileNetV2_flowers_model_e_83.h5\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 15s 326ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9961 - val_loss: 8.4721 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00084: saving model to tmp/MobileNetV2_flowers_model_e_84.h5\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9977 - val_loss: 8.8266 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00085: saving model to tmp/MobileNetV2_flowers_model_e_85.h5\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9982 - val_loss: 8.5569 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00086: saving model to tmp/MobileNetV2_flowers_model_e_86.h5\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9990 - val_loss: 8.3262 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00087: saving model to tmp/MobileNetV2_flowers_model_e_87.h5\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9995 - val_loss: 8.3446 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00088: saving model to tmp/MobileNetV2_flowers_model_e_88.h5\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 14s 310ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9991 - val_loss: 8.5234 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00089: saving model to tmp/MobileNetV2_flowers_model_e_89.h5\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 14s 316ms/step - loss: 0.0077 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.6724 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00090: saving model to tmp/MobileNetV2_flowers_model_e_90.h5\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0079 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.6031 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00091: saving model to tmp/MobileNetV2_flowers_model_e_91.h5\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.5765 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00092: saving model to tmp/MobileNetV2_flowers_model_e_92.h5\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.6246 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00093: saving model to tmp/MobileNetV2_flowers_model_e_93.h5\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9992 - val_loss: 8.6414 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00094: saving model to tmp/MobileNetV2_flowers_model_e_94.h5\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9983 - val_loss: 8.7354 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00095: saving model to tmp/MobileNetV2_flowers_model_e_95.h5\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 14s 319ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9927 - val_loss: 8.5119 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00096: saving model to tmp/MobileNetV2_flowers_model_e_96.h5\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9953 - val_loss: 8.5502 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00097: saving model to tmp/MobileNetV2_flowers_model_e_97.h5\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9975 - val_loss: 8.5977 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00098: saving model to tmp/MobileNetV2_flowers_model_e_98.h5\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9996 - val_loss: 8.8273 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00099: saving model to tmp/MobileNetV2_flowers_model_e_99.h5\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9976 - val_loss: 9.3378 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00100: saving model to tmp/MobileNetV2_flowers_model_e_100.h5\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0157 - sparse_categorical_accuracy: 0.9980 - val_loss: 8.8333 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00101: saving model to tmp/MobileNetV2_flowers_model_e_101.h5\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9944 - val_loss: 8.5819 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00102: saving model to tmp/MobileNetV2_flowers_model_e_102.h5\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9928 - val_loss: 9.2666 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00103: saving model to tmp/MobileNetV2_flowers_model_e_103.h5\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 0.0304 - sparse_categorical_accuracy: 0.9900 - val_loss: 8.8488 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00104: saving model to tmp/MobileNetV2_flowers_model_e_104.h5\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 14s 323ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9883 - val_loss: 9.3019 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00105: saving model to tmp/MobileNetV2_flowers_model_e_105.h5\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 15s 328ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9824 - val_loss: 9.1084 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00106: saving model to tmp/MobileNetV2_flowers_model_e_106.h5\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 0.1293 - sparse_categorical_accuracy: 0.9657 - val_loss: 9.6991 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00107: saving model to tmp/MobileNetV2_flowers_model_e_107.h5\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 15s 314ms/step - loss: 0.1896 - sparse_categorical_accuracy: 0.9414 - val_loss: 10.0334 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00108: saving model to tmp/MobileNetV2_flowers_model_e_108.h5\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 14s 319ms/step - loss: 0.3776 - sparse_categorical_accuracy: 0.8812 - val_loss: 11.0084 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00109: saving model to tmp/MobileNetV2_flowers_model_e_109.h5\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.3909 - sparse_categorical_accuracy: 0.8784 - val_loss: 10.1703 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00110: saving model to tmp/MobileNetV2_flowers_model_e_110.h5\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.4302 - sparse_categorical_accuracy: 0.8809 - val_loss: 10.1993 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00111: saving model to tmp/MobileNetV2_flowers_model_e_111.h5\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.2986 - sparse_categorical_accuracy: 0.9023 - val_loss: 9.7652 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00112: saving model to tmp/MobileNetV2_flowers_model_e_112.h5\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.2168 - sparse_categorical_accuracy: 0.9505 - val_loss: 11.4302 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00113: saving model to tmp/MobileNetV2_flowers_model_e_113.h5\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.1444 - sparse_categorical_accuracy: 0.9724 - val_loss: 9.7530 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00114: saving model to tmp/MobileNetV2_flowers_model_e_114.h5\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0646 - sparse_categorical_accuracy: 0.9872 - val_loss: 9.7181 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00115: saving model to tmp/MobileNetV2_flowers_model_e_115.h5\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0560 - sparse_categorical_accuracy: 0.9888 - val_loss: 10.0107 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00116: saving model to tmp/MobileNetV2_flowers_model_e_116.h5\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0530 - sparse_categorical_accuracy: 0.9956 - val_loss: 10.2031 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00117: saving model to tmp/MobileNetV2_flowers_model_e_117.h5\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 14s 316ms/step - loss: 0.0316 - sparse_categorical_accuracy: 0.9929 - val_loss: 9.7534 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00118: saving model to tmp/MobileNetV2_flowers_model_e_118.h5\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9981 - val_loss: 9.6000 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00119: saving model to tmp/MobileNetV2_flowers_model_e_119.h5\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9995 - val_loss: 9.5410 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00120: saving model to tmp/MobileNetV2_flowers_model_e_120.h5\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0064 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.6226 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00121: saving model to tmp/MobileNetV2_flowers_model_e_121.h5\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9996 - val_loss: 10.0820 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00122: saving model to tmp/MobileNetV2_flowers_model_e_122.h5\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9971 - val_loss: 10.0776 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00123: saving model to tmp/MobileNetV2_flowers_model_e_123.h5\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0091 - sparse_categorical_accuracy: 0.9985 - val_loss: 9.7952 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00124: saving model to tmp/MobileNetV2_flowers_model_e_124.h5\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9988 - val_loss: 9.6333 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00125: saving model to tmp/MobileNetV2_flowers_model_e_125.h5\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0042 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.6160 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00126: saving model to tmp/MobileNetV2_flowers_model_e_126.h5\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.6051 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00127: saving model to tmp/MobileNetV2_flowers_model_e_127.h5\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.5625 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00128: saving model to tmp/MobileNetV2_flowers_model_e_128.h5\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0037 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.3920 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00129: saving model to tmp/MobileNetV2_flowers_model_e_129.h5\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.3270 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00130: saving model to tmp/MobileNetV2_flowers_model_e_130.h5\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0027 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.2487 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00131: saving model to tmp/MobileNetV2_flowers_model_e_131.h5\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9995 - val_loss: 8.9395 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00132: saving model to tmp/MobileNetV2_flowers_model_e_132.h5\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9961 - val_loss: 9.1491 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00133: saving model to tmp/MobileNetV2_flowers_model_e_133.h5\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0061 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.7257 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00134: saving model to tmp/MobileNetV2_flowers_model_e_134.h5\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.7483 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00135: saving model to tmp/MobileNetV2_flowers_model_e_135.h5\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0044 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.8327 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00136: saving model to tmp/MobileNetV2_flowers_model_e_136.h5\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 9.2397 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00137: saving model to tmp/MobileNetV2_flowers_model_e_137.h5\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0054 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.7448 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00138: saving model to tmp/MobileNetV2_flowers_model_e_138.h5\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.6322 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00139: saving model to tmp/MobileNetV2_flowers_model_e_139.h5\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.5887 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00140: saving model to tmp/MobileNetV2_flowers_model_e_140.h5\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9999 - val_loss: 8.4898 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00141: saving model to tmp/MobileNetV2_flowers_model_e_141.h5\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9988 - val_loss: 8.5447 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00142: saving model to tmp/MobileNetV2_flowers_model_e_142.h5\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 14s 311ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9963 - val_loss: 8.0834 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00143: saving model to tmp/MobileNetV2_flowers_model_e_143.h5\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9964 - val_loss: 8.2162 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00144: saving model to tmp/MobileNetV2_flowers_model_e_144.h5\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9928 - val_loss: 8.1588 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00145: saving model to tmp/MobileNetV2_flowers_model_e_145.h5\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9981 - val_loss: 7.7740 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00146: saving model to tmp/MobileNetV2_flowers_model_e_146.h5\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9965 - val_loss: 7.8776 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00147: saving model to tmp/MobileNetV2_flowers_model_e_147.h5\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0276 - sparse_categorical_accuracy: 0.9928 - val_loss: 8.2105 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00148: saving model to tmp/MobileNetV2_flowers_model_e_148.h5\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9881 - val_loss: 7.8248 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00149: saving model to tmp/MobileNetV2_flowers_model_e_149.h5\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0650 - sparse_categorical_accuracy: 0.9841 - val_loss: 8.7853 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00150: saving model to tmp/MobileNetV2_flowers_model_e_150.h5\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0574 - sparse_categorical_accuracy: 0.9884 - val_loss: 7.7788 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00151: saving model to tmp/MobileNetV2_flowers_model_e_151.h5\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0845 - sparse_categorical_accuracy: 0.9811 - val_loss: 8.2182 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00152: saving model to tmp/MobileNetV2_flowers_model_e_152.h5\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.1076 - sparse_categorical_accuracy: 0.9755 - val_loss: 9.5382 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00153: saving model to tmp/MobileNetV2_flowers_model_e_153.h5\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.1710 - sparse_categorical_accuracy: 0.9435 - val_loss: 9.6219 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00154: saving model to tmp/MobileNetV2_flowers_model_e_154.h5\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.2039 - sparse_categorical_accuracy: 0.9458 - val_loss: 8.5762 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00155: saving model to tmp/MobileNetV2_flowers_model_e_155.h5\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.3090 - sparse_categorical_accuracy: 0.9036 - val_loss: 7.7682 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00156: saving model to tmp/MobileNetV2_flowers_model_e_156.h5\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.2624 - sparse_categorical_accuracy: 0.9330 - val_loss: 7.3654 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00157: saving model to tmp/MobileNetV2_flowers_model_e_157.h5\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.2202 - sparse_categorical_accuracy: 0.9348 - val_loss: 7.1151 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00158: saving model to tmp/MobileNetV2_flowers_model_e_158.h5\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.1286 - sparse_categorical_accuracy: 0.9674 - val_loss: 6.8927 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00159: saving model to tmp/MobileNetV2_flowers_model_e_159.h5\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.1012 - sparse_categorical_accuracy: 0.9722 - val_loss: 7.9327 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00160: saving model to tmp/MobileNetV2_flowers_model_e_160.h5\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9897 - val_loss: 7.1643 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00161: saving model to tmp/MobileNetV2_flowers_model_e_161.h5\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9903 - val_loss: 6.9292 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00162: saving model to tmp/MobileNetV2_flowers_model_e_162.h5\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9955 - val_loss: 7.2596 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00163: saving model to tmp/MobileNetV2_flowers_model_e_163.h5\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9966 - val_loss: 6.6824 - val_sparse_categorical_accuracy: 0.0157\n",
      "\n",
      "Epoch 00164: saving model to tmp/MobileNetV2_flowers_model_e_164.h5\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0062 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.6862 - val_sparse_categorical_accuracy: 0.0176\n",
      "\n",
      "Epoch 00165: saving model to tmp/MobileNetV2_flowers_model_e_165.h5\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9986 - val_loss: 6.6160 - val_sparse_categorical_accuracy: 0.0167\n",
      "\n",
      "Epoch 00166: saving model to tmp/MobileNetV2_flowers_model_e_166.h5\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9991 - val_loss: 6.7653 - val_sparse_categorical_accuracy: 0.0118\n",
      "\n",
      "Epoch 00167: saving model to tmp/MobileNetV2_flowers_model_e_167.h5\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 15s 328ms/step - loss: 0.0037 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7207 - val_sparse_categorical_accuracy: 0.0118\n",
      "\n",
      "Epoch 00168: saving model to tmp/MobileNetV2_flowers_model_e_168.h5\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.6791 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00169: saving model to tmp/MobileNetV2_flowers_model_e_169.h5\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.0048 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7096 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00170: saving model to tmp/MobileNetV2_flowers_model_e_170.h5\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0035 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7205 - val_sparse_categorical_accuracy: 0.0137\n",
      "\n",
      "Epoch 00171: saving model to tmp/MobileNetV2_flowers_model_e_171.h5\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.6278 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00172: saving model to tmp/MobileNetV2_flowers_model_e_172.h5\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0017 - sparse_categorical_accuracy: 0.9996 - val_loss: 6.6036 - val_sparse_categorical_accuracy: 0.0137\n",
      "\n",
      "Epoch 00173: saving model to tmp/MobileNetV2_flowers_model_e_173.h5\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5694 - val_sparse_categorical_accuracy: 0.0137\n",
      "\n",
      "Epoch 00174: saving model to tmp/MobileNetV2_flowers_model_e_174.h5\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5481 - val_sparse_categorical_accuracy: 0.0167\n",
      "\n",
      "Epoch 00175: saving model to tmp/MobileNetV2_flowers_model_e_175.h5\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5289 - val_sparse_categorical_accuracy: 0.0137\n",
      "\n",
      "Epoch 00176: saving model to tmp/MobileNetV2_flowers_model_e_176.h5\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5180 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00177: saving model to tmp/MobileNetV2_flowers_model_e_177.h5\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4633 - val_sparse_categorical_accuracy: 0.0147\n",
      "\n",
      "Epoch 00178: saving model to tmp/MobileNetV2_flowers_model_e_178.h5\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4847 - val_sparse_categorical_accuracy: 0.0167\n",
      "\n",
      "Epoch 00179: saving model to tmp/MobileNetV2_flowers_model_e_179.h5\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4816 - val_sparse_categorical_accuracy: 0.0176\n",
      "\n",
      "Epoch 00180: saving model to tmp/MobileNetV2_flowers_model_e_180.h5\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4628 - val_sparse_categorical_accuracy: 0.0186\n",
      "\n",
      "Epoch 00181: saving model to tmp/MobileNetV2_flowers_model_e_181.h5\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4813 - val_sparse_categorical_accuracy: 0.0186\n",
      "\n",
      "Epoch 00182: saving model to tmp/MobileNetV2_flowers_model_e_182.h5\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4890 - val_sparse_categorical_accuracy: 0.0206\n",
      "\n",
      "Epoch 00183: saving model to tmp/MobileNetV2_flowers_model_e_183.h5\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 9.7893e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5042 - val_sparse_categorical_accuracy: 0.0186\n",
      "\n",
      "Epoch 00184: saving model to tmp/MobileNetV2_flowers_model_e_184.h5\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5149 - val_sparse_categorical_accuracy: 0.0196\n",
      "\n",
      "Epoch 00185: saving model to tmp/MobileNetV2_flowers_model_e_185.h5\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5494 - val_sparse_categorical_accuracy: 0.0196\n",
      "\n",
      "Epoch 00186: saving model to tmp/MobileNetV2_flowers_model_e_186.h5\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5453 - val_sparse_categorical_accuracy: 0.0206\n",
      "\n",
      "Epoch 00187: saving model to tmp/MobileNetV2_flowers_model_e_187.h5\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5517 - val_sparse_categorical_accuracy: 0.0196\n",
      "\n",
      "Epoch 00188: saving model to tmp/MobileNetV2_flowers_model_e_188.h5\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 9.4086e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5774 - val_sparse_categorical_accuracy: 0.0176\n",
      "\n",
      "Epoch 00189: saving model to tmp/MobileNetV2_flowers_model_e_189.h5\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5614 - val_sparse_categorical_accuracy: 0.0196\n",
      "\n",
      "Epoch 00190: saving model to tmp/MobileNetV2_flowers_model_e_190.h5\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5380 - val_sparse_categorical_accuracy: 0.0206\n",
      "\n",
      "Epoch 00191: saving model to tmp/MobileNetV2_flowers_model_e_191.h5\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5397 - val_sparse_categorical_accuracy: 0.0225\n",
      "\n",
      "Epoch 00192: saving model to tmp/MobileNetV2_flowers_model_e_192.h5\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 15s 316ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4944 - val_sparse_categorical_accuracy: 0.0255\n",
      "\n",
      "Epoch 00193: saving model to tmp/MobileNetV2_flowers_model_e_193.h5\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 14s 317ms/step - loss: 6.2853e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4874 - val_sparse_categorical_accuracy: 0.0265\n",
      "\n",
      "Epoch 00194: saving model to tmp/MobileNetV2_flowers_model_e_194.h5\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 14s 312ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9988 - val_loss: 6.5847 - val_sparse_categorical_accuracy: 0.0225\n",
      "\n",
      "Epoch 00195: saving model to tmp/MobileNetV2_flowers_model_e_195.h5\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0027 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5618 - val_sparse_categorical_accuracy: 0.0206\n",
      "\n",
      "Epoch 00196: saving model to tmp/MobileNetV2_flowers_model_e_196.h5\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5485 - val_sparse_categorical_accuracy: 0.0255\n",
      "\n",
      "Epoch 00197: saving model to tmp/MobileNetV2_flowers_model_e_197.h5\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7739 - val_sparse_categorical_accuracy: 0.0216\n",
      "\n",
      "Epoch 00198: saving model to tmp/MobileNetV2_flowers_model_e_198.h5\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7510 - val_sparse_categorical_accuracy: 0.0216\n",
      "\n",
      "Epoch 00199: saving model to tmp/MobileNetV2_flowers_model_e_199.h5\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.6521 - val_sparse_categorical_accuracy: 0.0314\n",
      "\n",
      "Epoch 00200: saving model to tmp/MobileNetV2_flowers_model_e_200.h5\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5993 - val_sparse_categorical_accuracy: 0.0294\n",
      "\n",
      "Epoch 00201: saving model to tmp/MobileNetV2_flowers_model_e_201.h5\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9998 - val_loss: 6.6655 - val_sparse_categorical_accuracy: 0.0314\n",
      "\n",
      "Epoch 00202: saving model to tmp/MobileNetV2_flowers_model_e_202.h5\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9972 - val_loss: 6.9812 - val_sparse_categorical_accuracy: 0.0275\n",
      "\n",
      "Epoch 00203: saving model to tmp/MobileNetV2_flowers_model_e_203.h5\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 14s 318ms/step - loss: 0.0086 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.5000 - val_sparse_categorical_accuracy: 0.0343\n",
      "\n",
      "Epoch 00204: saving model to tmp/MobileNetV2_flowers_model_e_204.h5\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9935 - val_loss: 7.4872 - val_sparse_categorical_accuracy: 0.0275\n",
      "\n",
      "Epoch 00205: saving model to tmp/MobileNetV2_flowers_model_e_205.h5\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 15s 328ms/step - loss: 0.1173 - sparse_categorical_accuracy: 0.9685 - val_loss: 9.0498 - val_sparse_categorical_accuracy: 0.0167\n",
      "\n",
      "Epoch 00206: saving model to tmp/MobileNetV2_flowers_model_e_206.h5\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.3483 - sparse_categorical_accuracy: 0.9072 - val_loss: 10.6896 - val_sparse_categorical_accuracy: 0.0412\n",
      "\n",
      "Epoch 00207: saving model to tmp/MobileNetV2_flowers_model_e_207.h5\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 14s 312ms/step - loss: 0.5319 - sparse_categorical_accuracy: 0.8327 - val_loss: 13.7266 - val_sparse_categorical_accuracy: 0.0765\n",
      "\n",
      "Epoch 00208: saving model to tmp/MobileNetV2_flowers_model_e_208.h5\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.6082 - sparse_categorical_accuracy: 0.8140 - val_loss: 13.2392 - val_sparse_categorical_accuracy: 0.1118\n",
      "\n",
      "Epoch 00209: saving model to tmp/MobileNetV2_flowers_model_e_209.h5\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.2786 - sparse_categorical_accuracy: 0.9039 - val_loss: 12.5124 - val_sparse_categorical_accuracy: 0.1167\n",
      "\n",
      "Epoch 00210: saving model to tmp/MobileNetV2_flowers_model_e_210.h5\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.1103 - sparse_categorical_accuracy: 0.9699 - val_loss: 10.8483 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00211: saving model to tmp/MobileNetV2_flowers_model_e_211.h5\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0483 - sparse_categorical_accuracy: 0.9916 - val_loss: 10.0428 - val_sparse_categorical_accuracy: 0.1676\n",
      "\n",
      "Epoch 00212: saving model to tmp/MobileNetV2_flowers_model_e_212.h5\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 0.0268 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.9803 - val_sparse_categorical_accuracy: 0.1588\n",
      "\n",
      "Epoch 00213: saving model to tmp/MobileNetV2_flowers_model_e_213.h5\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9982 - val_loss: 8.3818 - val_sparse_categorical_accuracy: 0.1725\n",
      "\n",
      "Epoch 00214: saving model to tmp/MobileNetV2_flowers_model_e_214.h5\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0084 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.7366 - val_sparse_categorical_accuracy: 0.1608\n",
      "\n",
      "Epoch 00215: saving model to tmp/MobileNetV2_flowers_model_e_215.h5\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9977 - val_loss: 8.4405 - val_sparse_categorical_accuracy: 0.1716\n",
      "\n",
      "Epoch 00216: saving model to tmp/MobileNetV2_flowers_model_e_216.h5\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 14s 320ms/step - loss: 0.0039 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0461 - val_sparse_categorical_accuracy: 0.1686\n",
      "\n",
      "Epoch 00217: saving model to tmp/MobileNetV2_flowers_model_e_217.h5\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0042 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.5125 - val_sparse_categorical_accuracy: 0.1814\n",
      "\n",
      "Epoch 00218: saving model to tmp/MobileNetV2_flowers_model_e_218.h5\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.1047 - val_sparse_categorical_accuracy: 0.1931\n",
      "\n",
      "Epoch 00219: saving model to tmp/MobileNetV2_flowers_model_e_219.h5\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.7957 - val_sparse_categorical_accuracy: 0.1961\n",
      "\n",
      "Epoch 00220: saving model to tmp/MobileNetV2_flowers_model_e_220.h5\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.4095 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00221: saving model to tmp/MobileNetV2_flowers_model_e_221.h5\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.2504 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00222: saving model to tmp/MobileNetV2_flowers_model_e_222.h5\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.1508 - val_sparse_categorical_accuracy: 0.2039\n",
      "\n",
      "Epoch 00223: saving model to tmp/MobileNetV2_flowers_model_e_223.h5\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.0209 - val_sparse_categorical_accuracy: 0.2157\n",
      "\n",
      "Epoch 00224: saving model to tmp/MobileNetV2_flowers_model_e_224.h5\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 15s 326ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.0060 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00225: saving model to tmp/MobileNetV2_flowers_model_e_225.h5\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.9389 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00226: saving model to tmp/MobileNetV2_flowers_model_e_226.h5\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.7946 - val_sparse_categorical_accuracy: 0.2176\n",
      "\n",
      "Epoch 00227: saving model to tmp/MobileNetV2_flowers_model_e_227.h5\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 15s 314ms/step - loss: 0.0021 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.5866 - val_sparse_categorical_accuracy: 0.2265\n",
      "\n",
      "Epoch 00228: saving model to tmp/MobileNetV2_flowers_model_e_228.h5\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.0014 - sparse_categorical_accuracy: 0.9999 - val_loss: 5.4944 - val_sparse_categorical_accuracy: 0.2294\n",
      "\n",
      "Epoch 00229: saving model to tmp/MobileNetV2_flowers_model_e_229.h5\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0018 - sparse_categorical_accuracy: 0.9998 - val_loss: 5.5205 - val_sparse_categorical_accuracy: 0.2304\n",
      "\n",
      "Epoch 00230: saving model to tmp/MobileNetV2_flowers_model_e_230.h5\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.4847 - val_sparse_categorical_accuracy: 0.2314\n",
      "\n",
      "Epoch 00231: saving model to tmp/MobileNetV2_flowers_model_e_231.h5\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.3411 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00232: saving model to tmp/MobileNetV2_flowers_model_e_232.h5\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.1975 - val_sparse_categorical_accuracy: 0.2343\n",
      "\n",
      "Epoch 00233: saving model to tmp/MobileNetV2_flowers_model_e_233.h5\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 9.9156e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.0860 - val_sparse_categorical_accuracy: 0.2402\n",
      "\n",
      "Epoch 00234: saving model to tmp/MobileNetV2_flowers_model_e_234.h5\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 7.1344e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 5.0241 - val_sparse_categorical_accuracy: 0.2363\n",
      "\n",
      "Epoch 00235: saving model to tmp/MobileNetV2_flowers_model_e_235.h5\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 14s 316ms/step - loss: 9.5478e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.9445 - val_sparse_categorical_accuracy: 0.2402\n",
      "\n",
      "Epoch 00236: saving model to tmp/MobileNetV2_flowers_model_e_236.h5\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.0012 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.9586 - val_sparse_categorical_accuracy: 0.2412\n",
      "\n",
      "Epoch 00237: saving model to tmp/MobileNetV2_flowers_model_e_237.h5\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.9376 - val_sparse_categorical_accuracy: 0.2363\n",
      "\n",
      "Epoch 00238: saving model to tmp/MobileNetV2_flowers_model_e_238.h5\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 9.3229e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.9100 - val_sparse_categorical_accuracy: 0.2343\n",
      "\n",
      "Epoch 00239: saving model to tmp/MobileNetV2_flowers_model_e_239.h5\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.8678 - val_sparse_categorical_accuracy: 0.2402\n",
      "\n",
      "Epoch 00240: saving model to tmp/MobileNetV2_flowers_model_e_240.h5\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.8453 - val_sparse_categorical_accuracy: 0.2441\n",
      "\n",
      "Epoch 00241: saving model to tmp/MobileNetV2_flowers_model_e_241.h5\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.7594 - val_sparse_categorical_accuracy: 0.2441\n",
      "\n",
      "Epoch 00242: saving model to tmp/MobileNetV2_flowers_model_e_242.h5\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 8.6746e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.7329 - val_sparse_categorical_accuracy: 0.2431\n",
      "\n",
      "Epoch 00243: saving model to tmp/MobileNetV2_flowers_model_e_243.h5\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 7.0158e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.7054 - val_sparse_categorical_accuracy: 0.2578\n",
      "\n",
      "Epoch 00244: saving model to tmp/MobileNetV2_flowers_model_e_244.h5\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.8365 - val_sparse_categorical_accuracy: 0.2588\n",
      "\n",
      "Epoch 00245: saving model to tmp/MobileNetV2_flowers_model_e_245.h5\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 7.1145e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.7524 - val_sparse_categorical_accuracy: 0.2657\n",
      "\n",
      "Epoch 00246: saving model to tmp/MobileNetV2_flowers_model_e_246.h5\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 7.5601e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.6817 - val_sparse_categorical_accuracy: 0.2676\n",
      "\n",
      "Epoch 00247: saving model to tmp/MobileNetV2_flowers_model_e_247.h5\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 15s 326ms/step - loss: 5.9269e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.6067 - val_sparse_categorical_accuracy: 0.2716\n",
      "\n",
      "Epoch 00248: saving model to tmp/MobileNetV2_flowers_model_e_248.h5\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 8.3163e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.6131 - val_sparse_categorical_accuracy: 0.2676\n",
      "\n",
      "Epoch 00249: saving model to tmp/MobileNetV2_flowers_model_e_249.h5\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 6.9081e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.5641 - val_sparse_categorical_accuracy: 0.2775\n",
      "\n",
      "Epoch 00250: saving model to tmp/MobileNetV2_flowers_model_e_250.h5\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 14s 314ms/step - loss: 7.5054e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.5413 - val_sparse_categorical_accuracy: 0.2745\n",
      "\n",
      "Epoch 00251: saving model to tmp/MobileNetV2_flowers_model_e_251.h5\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.4848 - val_sparse_categorical_accuracy: 0.2804\n",
      "\n",
      "Epoch 00252: saving model to tmp/MobileNetV2_flowers_model_e_252.h5\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 4.0875e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.4382 - val_sparse_categorical_accuracy: 0.2853\n",
      "\n",
      "Epoch 00253: saving model to tmp/MobileNetV2_flowers_model_e_253.h5\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 5.9544e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.4163 - val_sparse_categorical_accuracy: 0.2863\n",
      "\n",
      "Epoch 00254: saving model to tmp/MobileNetV2_flowers_model_e_254.h5\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 15s 317ms/step - loss: 3.9174e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.3562 - val_sparse_categorical_accuracy: 0.2912\n",
      "\n",
      "Epoch 00255: saving model to tmp/MobileNetV2_flowers_model_e_255.h5\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 6.0617e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.3141 - val_sparse_categorical_accuracy: 0.2902\n",
      "\n",
      "Epoch 00256: saving model to tmp/MobileNetV2_flowers_model_e_256.h5\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 15s 329ms/step - loss: 5.0950e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2888 - val_sparse_categorical_accuracy: 0.2922\n",
      "\n",
      "Epoch 00257: saving model to tmp/MobileNetV2_flowers_model_e_257.h5\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 2.8426e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2671 - val_sparse_categorical_accuracy: 0.2971\n",
      "\n",
      "Epoch 00258: saving model to tmp/MobileNetV2_flowers_model_e_258.h5\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 4.2507e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2603 - val_sparse_categorical_accuracy: 0.2961\n",
      "\n",
      "Epoch 00259: saving model to tmp/MobileNetV2_flowers_model_e_259.h5\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 4.4982e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2652 - val_sparse_categorical_accuracy: 0.2922\n",
      "\n",
      "Epoch 00260: saving model to tmp/MobileNetV2_flowers_model_e_260.h5\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 3.6972e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2587 - val_sparse_categorical_accuracy: 0.2931\n",
      "\n",
      "Epoch 00261: saving model to tmp/MobileNetV2_flowers_model_e_261.h5\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 14s 319ms/step - loss: 4.7640e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2651 - val_sparse_categorical_accuracy: 0.2892\n",
      "\n",
      "Epoch 00262: saving model to tmp/MobileNetV2_flowers_model_e_262.h5\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 3.0430e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2463 - val_sparse_categorical_accuracy: 0.2892\n",
      "\n",
      "Epoch 00263: saving model to tmp/MobileNetV2_flowers_model_e_263.h5\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 4.9953e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2304 - val_sparse_categorical_accuracy: 0.2912\n",
      "\n",
      "Epoch 00264: saving model to tmp/MobileNetV2_flowers_model_e_264.h5\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 4.0664e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2158 - val_sparse_categorical_accuracy: 0.2902\n",
      "\n",
      "Epoch 00265: saving model to tmp/MobileNetV2_flowers_model_e_265.h5\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 14s 315ms/step - loss: 0.0011 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2489 - val_sparse_categorical_accuracy: 0.2725\n",
      "\n",
      "Epoch 00266: saving model to tmp/MobileNetV2_flowers_model_e_266.h5\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 8.4703e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.2241 - val_sparse_categorical_accuracy: 0.2892\n",
      "\n",
      "Epoch 00267: saving model to tmp/MobileNetV2_flowers_model_e_267.h5\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 4.1385e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1923 - val_sparse_categorical_accuracy: 0.2922\n",
      "\n",
      "Epoch 00268: saving model to tmp/MobileNetV2_flowers_model_e_268.h5\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 4.9632e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1941 - val_sparse_categorical_accuracy: 0.2912\n",
      "\n",
      "Epoch 00269: saving model to tmp/MobileNetV2_flowers_model_e_269.h5\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 3.5629e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1786 - val_sparse_categorical_accuracy: 0.2882\n",
      "\n",
      "Epoch 00270: saving model to tmp/MobileNetV2_flowers_model_e_270.h5\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 5.3379e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1499 - val_sparse_categorical_accuracy: 0.2912\n",
      "\n",
      "Epoch 00271: saving model to tmp/MobileNetV2_flowers_model_e_271.h5\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 14s 313ms/step - loss: 3.7653e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1382 - val_sparse_categorical_accuracy: 0.2941\n",
      "\n",
      "Epoch 00272: saving model to tmp/MobileNetV2_flowers_model_e_272.h5\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 2.7207e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1333 - val_sparse_categorical_accuracy: 0.2951\n",
      "\n",
      "Epoch 00273: saving model to tmp/MobileNetV2_flowers_model_e_273.h5\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 3.8098e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1157 - val_sparse_categorical_accuracy: 0.3020\n",
      "\n",
      "Epoch 00274: saving model to tmp/MobileNetV2_flowers_model_e_274.h5\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 15s 326ms/step - loss: 4.1075e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1088 - val_sparse_categorical_accuracy: 0.3020\n",
      "\n",
      "Epoch 00275: saving model to tmp/MobileNetV2_flowers_model_e_275.h5\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 14s 319ms/step - loss: 3.0889e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0879 - val_sparse_categorical_accuracy: 0.3029\n",
      "\n",
      "Epoch 00276: saving model to tmp/MobileNetV2_flowers_model_e_276.h5\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 3.4339e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0744 - val_sparse_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00277: saving model to tmp/MobileNetV2_flowers_model_e_277.h5\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 14s 320ms/step - loss: 3.9489e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0882 - val_sparse_categorical_accuracy: 0.2990\n",
      "\n",
      "Epoch 00278: saving model to tmp/MobileNetV2_flowers_model_e_278.h5\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 2.4100e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0783 - val_sparse_categorical_accuracy: 0.3010\n",
      "\n",
      "Epoch 00279: saving model to tmp/MobileNetV2_flowers_model_e_279.h5\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 3.9827e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0452 - val_sparse_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00280: saving model to tmp/MobileNetV2_flowers_model_e_280.h5\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 3.1282e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0839 - val_sparse_categorical_accuracy: 0.3078\n",
      "\n",
      "Epoch 00281: saving model to tmp/MobileNetV2_flowers_model_e_281.h5\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 15s 327ms/step - loss: 4.7972e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1089 - val_sparse_categorical_accuracy: 0.2961\n",
      "\n",
      "Epoch 00282: saving model to tmp/MobileNetV2_flowers_model_e_282.h5\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 2.9897e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1130 - val_sparse_categorical_accuracy: 0.2941\n",
      "\n",
      "Epoch 00283: saving model to tmp/MobileNetV2_flowers_model_e_283.h5\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 3.3641e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0952 - val_sparse_categorical_accuracy: 0.2912\n",
      "\n",
      "Epoch 00284: saving model to tmp/MobileNetV2_flowers_model_e_284.h5\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 4.2720e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0731 - val_sparse_categorical_accuracy: 0.3010\n",
      "\n",
      "Epoch 00285: saving model to tmp/MobileNetV2_flowers_model_e_285.h5\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 15s 324ms/step - loss: 2.5658e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0696 - val_sparse_categorical_accuracy: 0.2990\n",
      "\n",
      "Epoch 00286: saving model to tmp/MobileNetV2_flowers_model_e_286.h5\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 3.8373e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0497 - val_sparse_categorical_accuracy: 0.2980\n",
      "\n",
      "Epoch 00287: saving model to tmp/MobileNetV2_flowers_model_e_287.h5\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 2.7669e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0384 - val_sparse_categorical_accuracy: 0.2990\n",
      "\n",
      "Epoch 00288: saving model to tmp/MobileNetV2_flowers_model_e_288.h5\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 3.2538e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.0672 - val_sparse_categorical_accuracy: 0.3010\n",
      "\n",
      "Epoch 00289: saving model to tmp/MobileNetV2_flowers_model_e_289.h5\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 15s 325ms/step - loss: 3.9101e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 4.1918 - val_sparse_categorical_accuracy: 0.2951\n",
      "\n",
      "Epoch 00290: saving model to tmp/MobileNetV2_flowers_model_e_290.h5\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 15s 315ms/step - loss: 0.0042 - sparse_categorical_accuracy: 0.9992 - val_loss: 9.5029 - val_sparse_categorical_accuracy: 0.0971\n",
      "\n",
      "Epoch 00291: saving model to tmp/MobileNetV2_flowers_model_e_291.h5\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 15s 323ms/step - loss: 0.0511 - sparse_categorical_accuracy: 0.9863 - val_loss: 15.1891 - val_sparse_categorical_accuracy: 0.0490\n",
      "\n",
      "Epoch 00292: saving model to tmp/MobileNetV2_flowers_model_e_292.h5\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 1.0635 - sparse_categorical_accuracy: 0.7233 - val_loss: 42.6164 - val_sparse_categorical_accuracy: 0.0176\n",
      "\n",
      "Epoch 00293: saving model to tmp/MobileNetV2_flowers_model_e_293.h5\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 15s 321ms/step - loss: 1.3281 - sparse_categorical_accuracy: 0.6461 - val_loss: 31.3319 - val_sparse_categorical_accuracy: 0.0333\n",
      "\n",
      "Epoch 00294: saving model to tmp/MobileNetV2_flowers_model_e_294.h5\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 15s 328ms/step - loss: 0.4238 - sparse_categorical_accuracy: 0.8781 - val_loss: 31.2420 - val_sparse_categorical_accuracy: 0.0363\n",
      "\n",
      "Epoch 00295: saving model to tmp/MobileNetV2_flowers_model_e_295.h5\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 15s 322ms/step - loss: 0.1509 - sparse_categorical_accuracy: 0.9542 - val_loss: 31.9588 - val_sparse_categorical_accuracy: 0.0314\n",
      "\n",
      "Epoch 00296: saving model to tmp/MobileNetV2_flowers_model_e_296.h5\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0662 - sparse_categorical_accuracy: 0.9896 - val_loss: 31.0737 - val_sparse_categorical_accuracy: 0.0353\n",
      "\n",
      "Epoch 00297: saving model to tmp/MobileNetV2_flowers_model_e_297.h5\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 15s 318ms/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9960 - val_loss: 26.3737 - val_sparse_categorical_accuracy: 0.0686\n",
      "\n",
      "Epoch 00298: saving model to tmp/MobileNetV2_flowers_model_e_298.h5\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 15s 320ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9975 - val_loss: 23.7359 - val_sparse_categorical_accuracy: 0.0637\n",
      "\n",
      "Epoch 00299: saving model to tmp/MobileNetV2_flowers_model_e_299.h5\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 15s 319ms/step - loss: 0.0128 - sparse_categorical_accuracy: 0.9999 - val_loss: 22.4566 - val_sparse_categorical_accuracy: 0.0794\n",
      "\n",
      "Epoch 00300: saving model to tmp/MobileNetV2_flowers_model_e_300.h5\n",
      "Saved to: MobileNetV2_flowers_model.h5\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV2(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_flowers(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNets - B0, B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'EfficentNetB0_flowers_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 24s 453ms/step - loss: 4.7327 - sparse_categorical_accuracy: 0.0158 - val_loss: 4.6297 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00001: saving model to tmp/EfficentNetB0_flowers_model_e_01.h5\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 4.5204 - sparse_categorical_accuracy: 0.0333 - val_loss: 4.6582 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00002: saving model to tmp/EfficentNetB0_flowers_model_e_02.h5\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 17s 414ms/step - loss: 4.3994 - sparse_categorical_accuracy: 0.0584 - val_loss: 4.6947 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00003: saving model to tmp/EfficentNetB0_flowers_model_e_03.h5\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 4.1438 - sparse_categorical_accuracy: 0.0601 - val_loss: 4.8437 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00004: saving model to tmp/EfficentNetB0_flowers_model_e_04.h5\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 3.9142 - sparse_categorical_accuracy: 0.1000 - val_loss: 5.0846 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00005: saving model to tmp/EfficentNetB0_flowers_model_e_05.h5\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 3.6764 - sparse_categorical_accuracy: 0.1557 - val_loss: 5.3094 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00006: saving model to tmp/EfficentNetB0_flowers_model_e_06.h5\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 3.3776 - sparse_categorical_accuracy: 0.2147 - val_loss: 5.4205 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00007: saving model to tmp/EfficentNetB0_flowers_model_e_07.h5\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 17s 420ms/step - loss: 3.0778 - sparse_categorical_accuracy: 0.2698 - val_loss: 5.8970 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00008: saving model to tmp/EfficentNetB0_flowers_model_e_08.h5\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 2.7231 - sparse_categorical_accuracy: 0.3746 - val_loss: 6.1339 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00009: saving model to tmp/EfficentNetB0_flowers_model_e_09.h5\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 18s 412ms/step - loss: 2.3034 - sparse_categorical_accuracy: 0.4802 - val_loss: 5.7656 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00010: saving model to tmp/EfficentNetB0_flowers_model_e_10.h5\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 2.0121 - sparse_categorical_accuracy: 0.5513 - val_loss: 6.0252 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00011: saving model to tmp/EfficentNetB0_flowers_model_e_11.h5\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 1.8484 - sparse_categorical_accuracy: 0.5958 - val_loss: 6.1879 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00012: saving model to tmp/EfficentNetB0_flowers_model_e_12.h5\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 18s 410ms/step - loss: 1.5875 - sparse_categorical_accuracy: 0.6372 - val_loss: 6.6259 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00013: saving model to tmp/EfficentNetB0_flowers_model_e_13.h5\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 1.2808 - sparse_categorical_accuracy: 0.7474 - val_loss: 6.3140 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00014: saving model to tmp/EfficentNetB0_flowers_model_e_14.h5\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 1.2024 - sparse_categorical_accuracy: 0.7377 - val_loss: 7.0943 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00015: saving model to tmp/EfficentNetB0_flowers_model_e_15.h5\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.9336 - sparse_categorical_accuracy: 0.8101 - val_loss: 6.0957 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00016: saving model to tmp/EfficentNetB0_flowers_model_e_16.h5\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.8253 - sparse_categorical_accuracy: 0.8446 - val_loss: 6.3702 - val_sparse_categorical_accuracy: 0.0127\n",
      "\n",
      "Epoch 00017: saving model to tmp/EfficentNetB0_flowers_model_e_17.h5\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.7583 - sparse_categorical_accuracy: 0.8310 - val_loss: 7.6358 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00018: saving model to tmp/EfficentNetB0_flowers_model_e_18.h5\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 18s 412ms/step - loss: 0.6528 - sparse_categorical_accuracy: 0.8731 - val_loss: 6.8766 - val_sparse_categorical_accuracy: 0.0157\n",
      "\n",
      "Epoch 00019: saving model to tmp/EfficentNetB0_flowers_model_e_19.h5\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.6581 - sparse_categorical_accuracy: 0.8563 - val_loss: 6.8354 - val_sparse_categorical_accuracy: 0.0196\n",
      "\n",
      "Epoch 00020: saving model to tmp/EfficentNetB0_flowers_model_e_20.h5\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.6143 - sparse_categorical_accuracy: 0.8642 - val_loss: 5.8633 - val_sparse_categorical_accuracy: 0.0314\n",
      "\n",
      "Epoch 00021: saving model to tmp/EfficentNetB0_flowers_model_e_21.h5\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.5196 - sparse_categorical_accuracy: 0.8943 - val_loss: 6.5851 - val_sparse_categorical_accuracy: 0.0441\n",
      "\n",
      "Epoch 00022: saving model to tmp/EfficentNetB0_flowers_model_e_22.h5\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.4171 - sparse_categorical_accuracy: 0.9098 - val_loss: 5.8255 - val_sparse_categorical_accuracy: 0.0686\n",
      "\n",
      "Epoch 00023: saving model to tmp/EfficentNetB0_flowers_model_e_23.h5\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.5088 - sparse_categorical_accuracy: 0.8750 - val_loss: 5.6346 - val_sparse_categorical_accuracy: 0.0745\n",
      "\n",
      "Epoch 00024: saving model to tmp/EfficentNetB0_flowers_model_e_24.h5\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.4613 - sparse_categorical_accuracy: 0.8904 - val_loss: 5.1593 - val_sparse_categorical_accuracy: 0.1020\n",
      "\n",
      "Epoch 00025: saving model to tmp/EfficentNetB0_flowers_model_e_25.h5\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.5241 - sparse_categorical_accuracy: 0.8817 - val_loss: 5.2679 - val_sparse_categorical_accuracy: 0.1157\n",
      "\n",
      "Epoch 00026: saving model to tmp/EfficentNetB0_flowers_model_e_26.h5\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.4624 - sparse_categorical_accuracy: 0.8920 - val_loss: 5.3741 - val_sparse_categorical_accuracy: 0.1127\n",
      "\n",
      "Epoch 00027: saving model to tmp/EfficentNetB0_flowers_model_e_27.h5\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.3817 - sparse_categorical_accuracy: 0.9015 - val_loss: 5.0482 - val_sparse_categorical_accuracy: 0.1314\n",
      "\n",
      "Epoch 00028: saving model to tmp/EfficentNetB0_flowers_model_e_28.h5\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.3503 - sparse_categorical_accuracy: 0.9265 - val_loss: 5.7960 - val_sparse_categorical_accuracy: 0.1000\n",
      "\n",
      "Epoch 00029: saving model to tmp/EfficentNetB0_flowers_model_e_29.h5\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.3115 - sparse_categorical_accuracy: 0.9143 - val_loss: 5.2657 - val_sparse_categorical_accuracy: 0.1284\n",
      "\n",
      "Epoch 00030: saving model to tmp/EfficentNetB0_flowers_model_e_30.h5\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.3177 - sparse_categorical_accuracy: 0.9346 - val_loss: 5.1961 - val_sparse_categorical_accuracy: 0.1304\n",
      "\n",
      "Epoch 00031: saving model to tmp/EfficentNetB0_flowers_model_e_31.h5\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 18s 413ms/step - loss: 0.3618 - sparse_categorical_accuracy: 0.9148 - val_loss: 4.8593 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00032: saving model to tmp/EfficentNetB0_flowers_model_e_32.h5\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.9241 - val_loss: 6.3004 - val_sparse_categorical_accuracy: 0.1078\n",
      "\n",
      "Epoch 00033: saving model to tmp/EfficentNetB0_flowers_model_e_33.h5\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.3007 - sparse_categorical_accuracy: 0.9347 - val_loss: 5.7870 - val_sparse_categorical_accuracy: 0.1039\n",
      "\n",
      "Epoch 00034: saving model to tmp/EfficentNetB0_flowers_model_e_34.h5\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.2457 - sparse_categorical_accuracy: 0.9404 - val_loss: 5.4220 - val_sparse_categorical_accuracy: 0.1206\n",
      "\n",
      "Epoch 00035: saving model to tmp/EfficentNetB0_flowers_model_e_35.h5\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.3244 - sparse_categorical_accuracy: 0.9275 - val_loss: 5.2789 - val_sparse_categorical_accuracy: 0.1245\n",
      "\n",
      "Epoch 00036: saving model to tmp/EfficentNetB0_flowers_model_e_36.h5\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.2357 - sparse_categorical_accuracy: 0.9496 - val_loss: 5.3456 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00037: saving model to tmp/EfficentNetB0_flowers_model_e_37.h5\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.2448 - sparse_categorical_accuracy: 0.9482 - val_loss: 5.3623 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00038: saving model to tmp/EfficentNetB0_flowers_model_e_38.h5\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.3098 - sparse_categorical_accuracy: 0.9273 - val_loss: 5.5599 - val_sparse_categorical_accuracy: 0.1402\n",
      "\n",
      "Epoch 00039: saving model to tmp/EfficentNetB0_flowers_model_e_39.h5\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.2616 - sparse_categorical_accuracy: 0.9377 - val_loss: 5.4445 - val_sparse_categorical_accuracy: 0.1324\n",
      "\n",
      "Epoch 00040: saving model to tmp/EfficentNetB0_flowers_model_e_40.h5\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.2459 - sparse_categorical_accuracy: 0.9363 - val_loss: 5.4591 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00041: saving model to tmp/EfficentNetB0_flowers_model_e_41.h5\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.1888 - sparse_categorical_accuracy: 0.9596 - val_loss: 6.0997 - val_sparse_categorical_accuracy: 0.0961\n",
      "\n",
      "Epoch 00042: saving model to tmp/EfficentNetB0_flowers_model_e_42.h5\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 17s 412ms/step - loss: 0.2458 - sparse_categorical_accuracy: 0.9382 - val_loss: 5.3883 - val_sparse_categorical_accuracy: 0.1265\n",
      "\n",
      "Epoch 00043: saving model to tmp/EfficentNetB0_flowers_model_e_43.h5\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.2047 - sparse_categorical_accuracy: 0.9525 - val_loss: 5.6827 - val_sparse_categorical_accuracy: 0.1314\n",
      "\n",
      "Epoch 00044: saving model to tmp/EfficentNetB0_flowers_model_e_44.h5\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.2327 - sparse_categorical_accuracy: 0.9417 - val_loss: 5.6611 - val_sparse_categorical_accuracy: 0.1196\n",
      "\n",
      "Epoch 00045: saving model to tmp/EfficentNetB0_flowers_model_e_45.h5\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.2061 - sparse_categorical_accuracy: 0.9587 - val_loss: 5.3656 - val_sparse_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00046: saving model to tmp/EfficentNetB0_flowers_model_e_46.h5\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.1719 - sparse_categorical_accuracy: 0.9515 - val_loss: 5.8326 - val_sparse_categorical_accuracy: 0.1245\n",
      "\n",
      "Epoch 00047: saving model to tmp/EfficentNetB0_flowers_model_e_47.h5\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.2307 - sparse_categorical_accuracy: 0.9435 - val_loss: 5.8333 - val_sparse_categorical_accuracy: 0.1167\n",
      "\n",
      "Epoch 00048: saving model to tmp/EfficentNetB0_flowers_model_e_48.h5\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.2287 - sparse_categorical_accuracy: 0.9448 - val_loss: 6.2981 - val_sparse_categorical_accuracy: 0.1176\n",
      "\n",
      "Epoch 00049: saving model to tmp/EfficentNetB0_flowers_model_e_49.h5\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.2251 - sparse_categorical_accuracy: 0.9440 - val_loss: 5.5473 - val_sparse_categorical_accuracy: 0.1480\n",
      "\n",
      "Epoch 00050: saving model to tmp/EfficentNetB0_flowers_model_e_50.h5\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 17s 415ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9555 - val_loss: 6.0340 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00051: saving model to tmp/EfficentNetB0_flowers_model_e_51.h5\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.1812 - sparse_categorical_accuracy: 0.9592 - val_loss: 6.6174 - val_sparse_categorical_accuracy: 0.1069\n",
      "\n",
      "Epoch 00052: saving model to tmp/EfficentNetB0_flowers_model_e_52.h5\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.1591 - sparse_categorical_accuracy: 0.9700 - val_loss: 5.6721 - val_sparse_categorical_accuracy: 0.1451\n",
      "\n",
      "Epoch 00053: saving model to tmp/EfficentNetB0_flowers_model_e_53.h5\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.1595 - sparse_categorical_accuracy: 0.9572 - val_loss: 6.0676 - val_sparse_categorical_accuracy: 0.1127\n",
      "\n",
      "Epoch 00054: saving model to tmp/EfficentNetB0_flowers_model_e_54.h5\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.1712 - sparse_categorical_accuracy: 0.9577 - val_loss: 5.8535 - val_sparse_categorical_accuracy: 0.1088\n",
      "\n",
      "Epoch 00055: saving model to tmp/EfficentNetB0_flowers_model_e_55.h5\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9403 - val_loss: 5.9242 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00056: saving model to tmp/EfficentNetB0_flowers_model_e_56.h5\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.1987 - sparse_categorical_accuracy: 0.9610 - val_loss: 5.8771 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00057: saving model to tmp/EfficentNetB0_flowers_model_e_57.h5\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9564 - val_loss: 5.8608 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00058: saving model to tmp/EfficentNetB0_flowers_model_e_58.h5\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 18s 412ms/step - loss: 0.1100 - sparse_categorical_accuracy: 0.9786 - val_loss: 5.6309 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00059: saving model to tmp/EfficentNetB0_flowers_model_e_59.h5\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.1613 - sparse_categorical_accuracy: 0.9682 - val_loss: 5.7415 - val_sparse_categorical_accuracy: 0.1549\n",
      "\n",
      "Epoch 00060: saving model to tmp/EfficentNetB0_flowers_model_e_60.h5\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.1527 - sparse_categorical_accuracy: 0.9638 - val_loss: 6.1262 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00061: saving model to tmp/EfficentNetB0_flowers_model_e_61.h5\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.2031 - sparse_categorical_accuracy: 0.9462 - val_loss: 5.9587 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00062: saving model to tmp/EfficentNetB0_flowers_model_e_62.h5\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.1432 - sparse_categorical_accuracy: 0.9685 - val_loss: 6.0612 - val_sparse_categorical_accuracy: 0.1245\n",
      "\n",
      "Epoch 00063: saving model to tmp/EfficentNetB0_flowers_model_e_63.h5\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.1110 - sparse_categorical_accuracy: 0.9758 - val_loss: 6.2206 - val_sparse_categorical_accuracy: 0.0990\n",
      "\n",
      "Epoch 00064: saving model to tmp/EfficentNetB0_flowers_model_e_64.h5\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.1415 - sparse_categorical_accuracy: 0.9663 - val_loss: 5.6235 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00065: saving model to tmp/EfficentNetB0_flowers_model_e_65.h5\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.1066 - sparse_categorical_accuracy: 0.9740 - val_loss: 5.8292 - val_sparse_categorical_accuracy: 0.1539\n",
      "\n",
      "Epoch 00066: saving model to tmp/EfficentNetB0_flowers_model_e_66.h5\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9681 - val_loss: 5.7165 - val_sparse_categorical_accuracy: 0.1569\n",
      "\n",
      "Epoch 00067: saving model to tmp/EfficentNetB0_flowers_model_e_67.h5\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.1164 - sparse_categorical_accuracy: 0.9740 - val_loss: 5.9039 - val_sparse_categorical_accuracy: 0.1176\n",
      "\n",
      "Epoch 00068: saving model to tmp/EfficentNetB0_flowers_model_e_68.h5\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.1126 - sparse_categorical_accuracy: 0.9713 - val_loss: 6.4617 - val_sparse_categorical_accuracy: 0.1225\n",
      "\n",
      "Epoch 00069: saving model to tmp/EfficentNetB0_flowers_model_e_69.h5\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.1601 - sparse_categorical_accuracy: 0.9610 - val_loss: 6.0931 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00070: saving model to tmp/EfficentNetB0_flowers_model_e_70.h5\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9654 - val_loss: 5.8684 - val_sparse_categorical_accuracy: 0.1284\n",
      "\n",
      "Epoch 00071: saving model to tmp/EfficentNetB0_flowers_model_e_71.h5\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.1690 - sparse_categorical_accuracy: 0.9524 - val_loss: 6.2308 - val_sparse_categorical_accuracy: 0.1402\n",
      "\n",
      "Epoch 00072: saving model to tmp/EfficentNetB0_flowers_model_e_72.h5\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0873 - sparse_categorical_accuracy: 0.9783 - val_loss: 6.2659 - val_sparse_categorical_accuracy: 0.1422\n",
      "\n",
      "Epoch 00073: saving model to tmp/EfficentNetB0_flowers_model_e_73.h5\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 17s 413ms/step - loss: 0.1062 - sparse_categorical_accuracy: 0.9701 - val_loss: 5.7876 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00074: saving model to tmp/EfficentNetB0_flowers_model_e_74.h5\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0723 - sparse_categorical_accuracy: 0.9828 - val_loss: 6.0651 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00075: saving model to tmp/EfficentNetB0_flowers_model_e_75.h5\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 18s 426ms/step - loss: 0.1119 - sparse_categorical_accuracy: 0.9770 - val_loss: 5.9010 - val_sparse_categorical_accuracy: 0.1284\n",
      "\n",
      "Epoch 00076: saving model to tmp/EfficentNetB0_flowers_model_e_76.h5\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0990 - sparse_categorical_accuracy: 0.9697 - val_loss: 6.1153 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00077: saving model to tmp/EfficentNetB0_flowers_model_e_77.h5\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9732 - val_loss: 5.6727 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00078: saving model to tmp/EfficentNetB0_flowers_model_e_78.h5\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0589 - sparse_categorical_accuracy: 0.9806 - val_loss: 5.6570 - val_sparse_categorical_accuracy: 0.1510\n",
      "\n",
      "Epoch 00079: saving model to tmp/EfficentNetB0_flowers_model_e_79.h5\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0823 - sparse_categorical_accuracy: 0.9778 - val_loss: 5.8543 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00080: saving model to tmp/EfficentNetB0_flowers_model_e_80.h5\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0817 - sparse_categorical_accuracy: 0.9802 - val_loss: 5.7589 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00081: saving model to tmp/EfficentNetB0_flowers_model_e_81.h5\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.1131 - sparse_categorical_accuracy: 0.9735 - val_loss: 5.9208 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00082: saving model to tmp/EfficentNetB0_flowers_model_e_82.h5\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0891 - sparse_categorical_accuracy: 0.9816 - val_loss: 6.0527 - val_sparse_categorical_accuracy: 0.1324\n",
      "\n",
      "Epoch 00083: saving model to tmp/EfficentNetB0_flowers_model_e_83.h5\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0976 - sparse_categorical_accuracy: 0.9746 - val_loss: 6.1074 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00084: saving model to tmp/EfficentNetB0_flowers_model_e_84.h5\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0750 - sparse_categorical_accuracy: 0.9818 - val_loss: 6.3708 - val_sparse_categorical_accuracy: 0.1147\n",
      "\n",
      "Epoch 00085: saving model to tmp/EfficentNetB0_flowers_model_e_85.h5\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 18s 413ms/step - loss: 0.0954 - sparse_categorical_accuracy: 0.9736 - val_loss: 6.3535 - val_sparse_categorical_accuracy: 0.1402\n",
      "\n",
      "Epoch 00086: saving model to tmp/EfficentNetB0_flowers_model_e_86.h5\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9862 - val_loss: 6.1197 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00087: saving model to tmp/EfficentNetB0_flowers_model_e_87.h5\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9852 - val_loss: 5.8715 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00088: saving model to tmp/EfficentNetB0_flowers_model_e_88.h5\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9816 - val_loss: 6.3094 - val_sparse_categorical_accuracy: 0.1196\n",
      "\n",
      "Epoch 00089: saving model to tmp/EfficentNetB0_flowers_model_e_89.h5\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9770 - val_loss: 6.1420 - val_sparse_categorical_accuracy: 0.1422\n",
      "\n",
      "Epoch 00090: saving model to tmp/EfficentNetB0_flowers_model_e_90.h5\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0918 - sparse_categorical_accuracy: 0.9785 - val_loss: 6.3731 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00091: saving model to tmp/EfficentNetB0_flowers_model_e_91.h5\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0672 - sparse_categorical_accuracy: 0.9811 - val_loss: 6.0387 - val_sparse_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00092: saving model to tmp/EfficentNetB0_flowers_model_e_92.h5\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0849 - sparse_categorical_accuracy: 0.9761 - val_loss: 6.3890 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00093: saving model to tmp/EfficentNetB0_flowers_model_e_93.h5\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 17s 409ms/step - loss: 0.1092 - sparse_categorical_accuracy: 0.9749 - val_loss: 6.8335 - val_sparse_categorical_accuracy: 0.1176\n",
      "\n",
      "Epoch 00094: saving model to tmp/EfficentNetB0_flowers_model_e_94.h5\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9800 - val_loss: 6.5043 - val_sparse_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00095: saving model to tmp/EfficentNetB0_flowers_model_e_95.h5\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.1163 - sparse_categorical_accuracy: 0.9638 - val_loss: 6.1374 - val_sparse_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00096: saving model to tmp/EfficentNetB0_flowers_model_e_96.h5\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0901 - sparse_categorical_accuracy: 0.9792 - val_loss: 6.7205 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00097: saving model to tmp/EfficentNetB0_flowers_model_e_97.h5\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9861 - val_loss: 6.1114 - val_sparse_categorical_accuracy: 0.1569\n",
      "\n",
      "Epoch 00098: saving model to tmp/EfficentNetB0_flowers_model_e_98.h5\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0595 - sparse_categorical_accuracy: 0.9873 - val_loss: 6.1278 - val_sparse_categorical_accuracy: 0.1402\n",
      "\n",
      "Epoch 00099: saving model to tmp/EfficentNetB0_flowers_model_e_99.h5\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0763 - sparse_categorical_accuracy: 0.9818 - val_loss: 6.1628 - val_sparse_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00100: saving model to tmp/EfficentNetB0_flowers_model_e_100.h5\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 17s 410ms/step - loss: 0.0703 - sparse_categorical_accuracy: 0.9820 - val_loss: 6.1350 - val_sparse_categorical_accuracy: 0.1451\n",
      "\n",
      "Epoch 00101: saving model to tmp/EfficentNetB0_flowers_model_e_101.h5\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9805 - val_loss: 6.2514 - val_sparse_categorical_accuracy: 0.1353\n",
      "\n",
      "Epoch 00102: saving model to tmp/EfficentNetB0_flowers_model_e_102.h5\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0933 - sparse_categorical_accuracy: 0.9741 - val_loss: 6.3609 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00103: saving model to tmp/EfficentNetB0_flowers_model_e_103.h5\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.1051 - sparse_categorical_accuracy: 0.9752 - val_loss: 6.8654 - val_sparse_categorical_accuracy: 0.1343\n",
      "\n",
      "Epoch 00104: saving model to tmp/EfficentNetB0_flowers_model_e_104.h5\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0946 - sparse_categorical_accuracy: 0.9763 - val_loss: 6.2329 - val_sparse_categorical_accuracy: 0.1480\n",
      "\n",
      "Epoch 00105: saving model to tmp/EfficentNetB0_flowers_model_e_105.h5\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0659 - sparse_categorical_accuracy: 0.9839 - val_loss: 6.0738 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00106: saving model to tmp/EfficentNetB0_flowers_model_e_106.h5\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0805 - sparse_categorical_accuracy: 0.9800 - val_loss: 6.1304 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00107: saving model to tmp/EfficentNetB0_flowers_model_e_107.h5\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0581 - sparse_categorical_accuracy: 0.9868 - val_loss: 6.4778 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00108: saving model to tmp/EfficentNetB0_flowers_model_e_108.h5\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0610 - sparse_categorical_accuracy: 0.9848 - val_loss: 6.3148 - val_sparse_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00109: saving model to tmp/EfficentNetB0_flowers_model_e_109.h5\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.1035 - sparse_categorical_accuracy: 0.9749 - val_loss: 6.8767 - val_sparse_categorical_accuracy: 0.1176\n",
      "\n",
      "Epoch 00110: saving model to tmp/EfficentNetB0_flowers_model_e_110.h5\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.1419 - sparse_categorical_accuracy: 0.9624 - val_loss: 6.8246 - val_sparse_categorical_accuracy: 0.1333\n",
      "\n",
      "Epoch 00111: saving model to tmp/EfficentNetB0_flowers_model_e_111.h5\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0824 - sparse_categorical_accuracy: 0.9800 - val_loss: 6.3017 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00112: saving model to tmp/EfficentNetB0_flowers_model_e_112.h5\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0440 - sparse_categorical_accuracy: 0.9925 - val_loss: 6.0170 - val_sparse_categorical_accuracy: 0.1588\n",
      "\n",
      "Epoch 00113: saving model to tmp/EfficentNetB0_flowers_model_e_113.h5\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9853 - val_loss: 5.9980 - val_sparse_categorical_accuracy: 0.1422\n",
      "\n",
      "Epoch 00114: saving model to tmp/EfficentNetB0_flowers_model_e_114.h5\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9852 - val_loss: 6.4410 - val_sparse_categorical_accuracy: 0.1304\n",
      "\n",
      "Epoch 00115: saving model to tmp/EfficentNetB0_flowers_model_e_115.h5\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0670 - sparse_categorical_accuracy: 0.9788 - val_loss: 6.2007 - val_sparse_categorical_accuracy: 0.1422\n",
      "\n",
      "Epoch 00116: saving model to tmp/EfficentNetB0_flowers_model_e_116.h5\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9848 - val_loss: 5.8409 - val_sparse_categorical_accuracy: 0.1598\n",
      "\n",
      "Epoch 00117: saving model to tmp/EfficentNetB0_flowers_model_e_117.h5\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9975 - val_loss: 5.5591 - val_sparse_categorical_accuracy: 0.1804\n",
      "\n",
      "Epoch 00118: saving model to tmp/EfficentNetB0_flowers_model_e_118.h5\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9950 - val_loss: 5.6266 - val_sparse_categorical_accuracy: 0.1765\n",
      "\n",
      "Epoch 00119: saving model to tmp/EfficentNetB0_flowers_model_e_119.h5\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9839 - val_loss: 6.1282 - val_sparse_categorical_accuracy: 0.1441\n",
      "\n",
      "Epoch 00120: saving model to tmp/EfficentNetB0_flowers_model_e_120.h5\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9864 - val_loss: 6.6061 - val_sparse_categorical_accuracy: 0.1196\n",
      "\n",
      "Epoch 00121: saving model to tmp/EfficentNetB0_flowers_model_e_121.h5\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0865 - sparse_categorical_accuracy: 0.9706 - val_loss: 6.3703 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00122: saving model to tmp/EfficentNetB0_flowers_model_e_122.h5\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0604 - sparse_categorical_accuracy: 0.9823 - val_loss: 6.8006 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00123: saving model to tmp/EfficentNetB0_flowers_model_e_123.h5\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0794 - sparse_categorical_accuracy: 0.9750 - val_loss: 6.5849 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00124: saving model to tmp/EfficentNetB0_flowers_model_e_124.h5\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0443 - sparse_categorical_accuracy: 0.9850 - val_loss: 6.1115 - val_sparse_categorical_accuracy: 0.1588\n",
      "\n",
      "Epoch 00125: saving model to tmp/EfficentNetB0_flowers_model_e_125.h5\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.0725 - sparse_categorical_accuracy: 0.9842 - val_loss: 6.2278 - val_sparse_categorical_accuracy: 0.1559\n",
      "\n",
      "Epoch 00126: saving model to tmp/EfficentNetB0_flowers_model_e_126.h5\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9865 - val_loss: 6.0840 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00127: saving model to tmp/EfficentNetB0_flowers_model_e_127.h5\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0624 - sparse_categorical_accuracy: 0.9918 - val_loss: 6.1535 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00128: saving model to tmp/EfficentNetB0_flowers_model_e_128.h5\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0465 - sparse_categorical_accuracy: 0.9863 - val_loss: 5.9699 - val_sparse_categorical_accuracy: 0.1775\n",
      "\n",
      "Epoch 00129: saving model to tmp/EfficentNetB0_flowers_model_e_129.h5\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0886 - sparse_categorical_accuracy: 0.9738 - val_loss: 6.2412 - val_sparse_categorical_accuracy: 0.1480\n",
      "\n",
      "Epoch 00130: saving model to tmp/EfficentNetB0_flowers_model_e_130.h5\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0605 - sparse_categorical_accuracy: 0.9788 - val_loss: 6.3666 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00131: saving model to tmp/EfficentNetB0_flowers_model_e_131.h5\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9847 - val_loss: 6.6537 - val_sparse_categorical_accuracy: 0.1441\n",
      "\n",
      "Epoch 00132: saving model to tmp/EfficentNetB0_flowers_model_e_132.h5\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9893 - val_loss: 6.4072 - val_sparse_categorical_accuracy: 0.1647\n",
      "\n",
      "Epoch 00133: saving model to tmp/EfficentNetB0_flowers_model_e_133.h5\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0447 - sparse_categorical_accuracy: 0.9914 - val_loss: 6.4278 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00134: saving model to tmp/EfficentNetB0_flowers_model_e_134.h5\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9967 - val_loss: 6.1818 - val_sparse_categorical_accuracy: 0.1608\n",
      "\n",
      "Epoch 00135: saving model to tmp/EfficentNetB0_flowers_model_e_135.h5\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9909 - val_loss: 6.1572 - val_sparse_categorical_accuracy: 0.1549\n",
      "\n",
      "Epoch 00136: saving model to tmp/EfficentNetB0_flowers_model_e_136.h5\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0629 - sparse_categorical_accuracy: 0.9777 - val_loss: 6.4228 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00137: saving model to tmp/EfficentNetB0_flowers_model_e_137.h5\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9903 - val_loss: 6.0963 - val_sparse_categorical_accuracy: 0.1569\n",
      "\n",
      "Epoch 00138: saving model to tmp/EfficentNetB0_flowers_model_e_138.h5\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9892 - val_loss: 6.0957 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00139: saving model to tmp/EfficentNetB0_flowers_model_e_139.h5\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0547 - sparse_categorical_accuracy: 0.9853 - val_loss: 6.4539 - val_sparse_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00140: saving model to tmp/EfficentNetB0_flowers_model_e_140.h5\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9771 - val_loss: 6.4078 - val_sparse_categorical_accuracy: 0.1598\n",
      "\n",
      "Epoch 00141: saving model to tmp/EfficentNetB0_flowers_model_e_141.h5\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 17s 416ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9911 - val_loss: 6.6809 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00142: saving model to tmp/EfficentNetB0_flowers_model_e_142.h5\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 18s 428ms/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9818 - val_loss: 6.5136 - val_sparse_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00143: saving model to tmp/EfficentNetB0_flowers_model_e_143.h5\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9806 - val_loss: 5.9617 - val_sparse_categorical_accuracy: 0.1657\n",
      "\n",
      "Epoch 00144: saving model to tmp/EfficentNetB0_flowers_model_e_144.h5\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9884 - val_loss: 6.0271 - val_sparse_categorical_accuracy: 0.1657\n",
      "\n",
      "Epoch 00145: saving model to tmp/EfficentNetB0_flowers_model_e_145.h5\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9890 - val_loss: 6.3643 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00146: saving model to tmp/EfficentNetB0_flowers_model_e_146.h5\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0454 - sparse_categorical_accuracy: 0.9847 - val_loss: 6.5144 - val_sparse_categorical_accuracy: 0.1422\n",
      "\n",
      "Epoch 00147: saving model to tmp/EfficentNetB0_flowers_model_e_147.h5\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0712 - sparse_categorical_accuracy: 0.9767 - val_loss: 6.3081 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00148: saving model to tmp/EfficentNetB0_flowers_model_e_148.h5\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0635 - sparse_categorical_accuracy: 0.9827 - val_loss: 6.7774 - val_sparse_categorical_accuracy: 0.1539\n",
      "\n",
      "Epoch 00149: saving model to tmp/EfficentNetB0_flowers_model_e_149.h5\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9956 - val_loss: 6.5822 - val_sparse_categorical_accuracy: 0.1696\n",
      "\n",
      "Epoch 00150: saving model to tmp/EfficentNetB0_flowers_model_e_150.h5\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0495 - sparse_categorical_accuracy: 0.9910 - val_loss: 6.3541 - val_sparse_categorical_accuracy: 0.1735\n",
      "\n",
      "Epoch 00151: saving model to tmp/EfficentNetB0_flowers_model_e_151.h5\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0411 - sparse_categorical_accuracy: 0.9921 - val_loss: 6.1984 - val_sparse_categorical_accuracy: 0.1676\n",
      "\n",
      "Epoch 00152: saving model to tmp/EfficentNetB0_flowers_model_e_152.h5\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0469 - sparse_categorical_accuracy: 0.9805 - val_loss: 6.4944 - val_sparse_categorical_accuracy: 0.1480\n",
      "\n",
      "Epoch 00153: saving model to tmp/EfficentNetB0_flowers_model_e_153.h5\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9858 - val_loss: 6.4882 - val_sparse_categorical_accuracy: 0.1725\n",
      "\n",
      "Epoch 00154: saving model to tmp/EfficentNetB0_flowers_model_e_154.h5\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0722 - sparse_categorical_accuracy: 0.9724 - val_loss: 6.6775 - val_sparse_categorical_accuracy: 0.1314\n",
      "\n",
      "Epoch 00155: saving model to tmp/EfficentNetB0_flowers_model_e_155.h5\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9795 - val_loss: 6.7754 - val_sparse_categorical_accuracy: 0.1353\n",
      "\n",
      "Epoch 00156: saving model to tmp/EfficentNetB0_flowers_model_e_156.h5\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0400 - sparse_categorical_accuracy: 0.9912 - val_loss: 6.5554 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00157: saving model to tmp/EfficentNetB0_flowers_model_e_157.h5\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 18s 413ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9880 - val_loss: 6.3398 - val_sparse_categorical_accuracy: 0.1451\n",
      "\n",
      "Epoch 00158: saving model to tmp/EfficentNetB0_flowers_model_e_158.h5\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9904 - val_loss: 6.2782 - val_sparse_categorical_accuracy: 0.1451\n",
      "\n",
      "Epoch 00159: saving model to tmp/EfficentNetB0_flowers_model_e_159.h5\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9918 - val_loss: 6.1359 - val_sparse_categorical_accuracy: 0.1627\n",
      "\n",
      "Epoch 00160: saving model to tmp/EfficentNetB0_flowers_model_e_160.h5\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9897 - val_loss: 6.5915 - val_sparse_categorical_accuracy: 0.1255\n",
      "\n",
      "Epoch 00161: saving model to tmp/EfficentNetB0_flowers_model_e_161.h5\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0401 - sparse_categorical_accuracy: 0.9879 - val_loss: 6.3534 - val_sparse_categorical_accuracy: 0.1598\n",
      "\n",
      "Epoch 00162: saving model to tmp/EfficentNetB0_flowers_model_e_162.h5\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9906 - val_loss: 6.6440 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00163: saving model to tmp/EfficentNetB0_flowers_model_e_163.h5\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0559 - sparse_categorical_accuracy: 0.9781 - val_loss: 6.6948 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00164: saving model to tmp/EfficentNetB0_flowers_model_e_164.h5\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0677 - sparse_categorical_accuracy: 0.9796 - val_loss: 6.7537 - val_sparse_categorical_accuracy: 0.1167\n",
      "\n",
      "Epoch 00165: saving model to tmp/EfficentNetB0_flowers_model_e_165.h5\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 18s 431ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9855 - val_loss: 6.7849 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00166: saving model to tmp/EfficentNetB0_flowers_model_e_166.h5\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0520 - sparse_categorical_accuracy: 0.9837 - val_loss: 6.7214 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00167: saving model to tmp/EfficentNetB0_flowers_model_e_167.h5\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0638 - sparse_categorical_accuracy: 0.9843 - val_loss: 6.5903 - val_sparse_categorical_accuracy: 0.1578\n",
      "\n",
      "Epoch 00168: saving model to tmp/EfficentNetB0_flowers_model_e_168.h5\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9888 - val_loss: 6.6737 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00169: saving model to tmp/EfficentNetB0_flowers_model_e_169.h5\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9925 - val_loss: 6.3227 - val_sparse_categorical_accuracy: 0.1686\n",
      "\n",
      "Epoch 00170: saving model to tmp/EfficentNetB0_flowers_model_e_170.h5\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9896 - val_loss: 6.4968 - val_sparse_categorical_accuracy: 0.1588\n",
      "\n",
      "Epoch 00171: saving model to tmp/EfficentNetB0_flowers_model_e_171.h5\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0564 - sparse_categorical_accuracy: 0.9857 - val_loss: 6.6341 - val_sparse_categorical_accuracy: 0.1627\n",
      "\n",
      "Epoch 00172: saving model to tmp/EfficentNetB0_flowers_model_e_172.h5\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0573 - sparse_categorical_accuracy: 0.9859 - val_loss: 6.8520 - val_sparse_categorical_accuracy: 0.1451\n",
      "\n",
      "Epoch 00173: saving model to tmp/EfficentNetB0_flowers_model_e_173.h5\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9698 - val_loss: 7.3175 - val_sparse_categorical_accuracy: 0.1176\n",
      "\n",
      "Epoch 00174: saving model to tmp/EfficentNetB0_flowers_model_e_174.h5\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0663 - sparse_categorical_accuracy: 0.9820 - val_loss: 7.4251 - val_sparse_categorical_accuracy: 0.1157\n",
      "\n",
      "Epoch 00175: saving model to tmp/EfficentNetB0_flowers_model_e_175.h5\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0339 - sparse_categorical_accuracy: 0.9909 - val_loss: 6.6173 - val_sparse_categorical_accuracy: 0.1559\n",
      "\n",
      "Epoch 00176: saving model to tmp/EfficentNetB0_flowers_model_e_176.h5\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 18s 427ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9855 - val_loss: 6.3479 - val_sparse_categorical_accuracy: 0.1549\n",
      "\n",
      "Epoch 00177: saving model to tmp/EfficentNetB0_flowers_model_e_177.h5\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0343 - sparse_categorical_accuracy: 0.9912 - val_loss: 6.7504 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00178: saving model to tmp/EfficentNetB0_flowers_model_e_178.h5\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0351 - sparse_categorical_accuracy: 0.9892 - val_loss: 6.7453 - val_sparse_categorical_accuracy: 0.1441\n",
      "\n",
      "Epoch 00179: saving model to tmp/EfficentNetB0_flowers_model_e_179.h5\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 18s 426ms/step - loss: 0.0594 - sparse_categorical_accuracy: 0.9806 - val_loss: 6.5691 - val_sparse_categorical_accuracy: 0.1627\n",
      "\n",
      "Epoch 00180: saving model to tmp/EfficentNetB0_flowers_model_e_180.h5\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9872 - val_loss: 6.7658 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00181: saving model to tmp/EfficentNetB0_flowers_model_e_181.h5\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0555 - sparse_categorical_accuracy: 0.9841 - val_loss: 6.3998 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00182: saving model to tmp/EfficentNetB0_flowers_model_e_182.h5\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9959 - val_loss: 6.2882 - val_sparse_categorical_accuracy: 0.1765\n",
      "\n",
      "Epoch 00183: saving model to tmp/EfficentNetB0_flowers_model_e_183.h5\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0137 - sparse_categorical_accuracy: 0.9964 - val_loss: 6.1866 - val_sparse_categorical_accuracy: 0.1814\n",
      "\n",
      "Epoch 00184: saving model to tmp/EfficentNetB0_flowers_model_e_184.h5\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0408 - sparse_categorical_accuracy: 0.9909 - val_loss: 6.2196 - val_sparse_categorical_accuracy: 0.1804\n",
      "\n",
      "Epoch 00185: saving model to tmp/EfficentNetB0_flowers_model_e_185.h5\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9936 - val_loss: 6.4934 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00186: saving model to tmp/EfficentNetB0_flowers_model_e_186.h5\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0452 - sparse_categorical_accuracy: 0.9910 - val_loss: 6.5244 - val_sparse_categorical_accuracy: 0.1725\n",
      "\n",
      "Epoch 00187: saving model to tmp/EfficentNetB0_flowers_model_e_187.h5\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.9914 - val_loss: 6.4443 - val_sparse_categorical_accuracy: 0.1696\n",
      "\n",
      "Epoch 00188: saving model to tmp/EfficentNetB0_flowers_model_e_188.h5\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9930 - val_loss: 6.6682 - val_sparse_categorical_accuracy: 0.1539\n",
      "\n",
      "Epoch 00189: saving model to tmp/EfficentNetB0_flowers_model_e_189.h5\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9936 - val_loss: 6.4103 - val_sparse_categorical_accuracy: 0.1627\n",
      "\n",
      "Epoch 00190: saving model to tmp/EfficentNetB0_flowers_model_e_190.h5\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9949 - val_loss: 7.1141 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00191: saving model to tmp/EfficentNetB0_flowers_model_e_191.h5\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0641 - sparse_categorical_accuracy: 0.9819 - val_loss: 7.2713 - val_sparse_categorical_accuracy: 0.1265\n",
      "\n",
      "Epoch 00192: saving model to tmp/EfficentNetB0_flowers_model_e_192.h5\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0602 - sparse_categorical_accuracy: 0.9906 - val_loss: 7.3327 - val_sparse_categorical_accuracy: 0.1353\n",
      "\n",
      "Epoch 00193: saving model to tmp/EfficentNetB0_flowers_model_e_193.h5\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9852 - val_loss: 7.0495 - val_sparse_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00194: saving model to tmp/EfficentNetB0_flowers_model_e_194.h5\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0384 - sparse_categorical_accuracy: 0.9885 - val_loss: 6.6104 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00195: saving model to tmp/EfficentNetB0_flowers_model_e_195.h5\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 18s 411ms/step - loss: 0.0353 - sparse_categorical_accuracy: 0.9901 - val_loss: 7.2698 - val_sparse_categorical_accuracy: 0.1333\n",
      "\n",
      "Epoch 00196: saving model to tmp/EfficentNetB0_flowers_model_e_196.h5\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 17s 411ms/step - loss: 0.0821 - sparse_categorical_accuracy: 0.9805 - val_loss: 6.9301 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00197: saving model to tmp/EfficentNetB0_flowers_model_e_197.h5\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9856 - val_loss: 6.5505 - val_sparse_categorical_accuracy: 0.1598\n",
      "\n",
      "Epoch 00198: saving model to tmp/EfficentNetB0_flowers_model_e_198.h5\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0137 - sparse_categorical_accuracy: 0.9986 - val_loss: 6.6253 - val_sparse_categorical_accuracy: 0.1598\n",
      "\n",
      "Epoch 00199: saving model to tmp/EfficentNetB0_flowers_model_e_199.h5\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9933 - val_loss: 7.0187 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00200: saving model to tmp/EfficentNetB0_flowers_model_e_200.h5\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 17s 414ms/step - loss: 0.0304 - sparse_categorical_accuracy: 0.9932 - val_loss: 6.6893 - val_sparse_categorical_accuracy: 0.1510\n",
      "\n",
      "Epoch 00201: saving model to tmp/EfficentNetB0_flowers_model_e_201.h5\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0444 - sparse_categorical_accuracy: 0.9930 - val_loss: 6.4866 - val_sparse_categorical_accuracy: 0.1569\n",
      "\n",
      "Epoch 00202: saving model to tmp/EfficentNetB0_flowers_model_e_202.h5\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9881 - val_loss: 6.1214 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00203: saving model to tmp/EfficentNetB0_flowers_model_e_203.h5\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9957 - val_loss: 6.2832 - val_sparse_categorical_accuracy: 0.1725\n",
      "\n",
      "Epoch 00204: saving model to tmp/EfficentNetB0_flowers_model_e_204.h5\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0410 - sparse_categorical_accuracy: 0.9930 - val_loss: 6.2954 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00205: saving model to tmp/EfficentNetB0_flowers_model_e_205.h5\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9941 - val_loss: 6.5161 - val_sparse_categorical_accuracy: 0.1578\n",
      "\n",
      "Epoch 00206: saving model to tmp/EfficentNetB0_flowers_model_e_206.h5\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 17s 413ms/step - loss: 0.0296 - sparse_categorical_accuracy: 0.9926 - val_loss: 6.4959 - val_sparse_categorical_accuracy: 0.1657\n",
      "\n",
      "Epoch 00207: saving model to tmp/EfficentNetB0_flowers_model_e_207.h5\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0405 - sparse_categorical_accuracy: 0.9905 - val_loss: 6.8849 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00208: saving model to tmp/EfficentNetB0_flowers_model_e_208.h5\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9810 - val_loss: 6.8896 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00209: saving model to tmp/EfficentNetB0_flowers_model_e_209.h5\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.0351 - sparse_categorical_accuracy: 0.9900 - val_loss: 6.3692 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00210: saving model to tmp/EfficentNetB0_flowers_model_e_210.h5\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9892 - val_loss: 6.9184 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00211: saving model to tmp/EfficentNetB0_flowers_model_e_211.h5\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0459 - sparse_categorical_accuracy: 0.9898 - val_loss: 6.9444 - val_sparse_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00212: saving model to tmp/EfficentNetB0_flowers_model_e_212.h5\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9883 - val_loss: 6.4883 - val_sparse_categorical_accuracy: 0.1627\n",
      "\n",
      "Epoch 00213: saving model to tmp/EfficentNetB0_flowers_model_e_213.h5\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0360 - sparse_categorical_accuracy: 0.9889 - val_loss: 6.6403 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00214: saving model to tmp/EfficentNetB0_flowers_model_e_214.h5\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0448 - sparse_categorical_accuracy: 0.9892 - val_loss: 6.4143 - val_sparse_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00215: saving model to tmp/EfficentNetB0_flowers_model_e_215.h5\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0410 - sparse_categorical_accuracy: 0.9861 - val_loss: 6.8575 - val_sparse_categorical_accuracy: 0.1422\n",
      "\n",
      "Epoch 00216: saving model to tmp/EfficentNetB0_flowers_model_e_216.h5\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9914 - val_loss: 6.5239 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00217: saving model to tmp/EfficentNetB0_flowers_model_e_217.h5\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0307 - sparse_categorical_accuracy: 0.9941 - val_loss: 6.6649 - val_sparse_categorical_accuracy: 0.1441\n",
      "\n",
      "Epoch 00218: saving model to tmp/EfficentNetB0_flowers_model_e_218.h5\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.9875 - val_loss: 6.6475 - val_sparse_categorical_accuracy: 0.1559\n",
      "\n",
      "Epoch 00219: saving model to tmp/EfficentNetB0_flowers_model_e_219.h5\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 18s 409ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9892 - val_loss: 7.6727 - val_sparse_categorical_accuracy: 0.1157\n",
      "\n",
      "Epoch 00220: saving model to tmp/EfficentNetB0_flowers_model_e_220.h5\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9949 - val_loss: 6.6333 - val_sparse_categorical_accuracy: 0.1480\n",
      "\n",
      "Epoch 00221: saving model to tmp/EfficentNetB0_flowers_model_e_221.h5\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9919 - val_loss: 6.5695 - val_sparse_categorical_accuracy: 0.1559\n",
      "\n",
      "Epoch 00222: saving model to tmp/EfficentNetB0_flowers_model_e_222.h5\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 17s 412ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9911 - val_loss: 6.6718 - val_sparse_categorical_accuracy: 0.1510\n",
      "\n",
      "Epoch 00223: saving model to tmp/EfficentNetB0_flowers_model_e_223.h5\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9915 - val_loss: 6.6070 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00224: saving model to tmp/EfficentNetB0_flowers_model_e_224.h5\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 18s 410ms/step - loss: 0.0387 - sparse_categorical_accuracy: 0.9872 - val_loss: 6.9078 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00225: saving model to tmp/EfficentNetB0_flowers_model_e_225.h5\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0307 - sparse_categorical_accuracy: 0.9928 - val_loss: 7.3666 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00226: saving model to tmp/EfficentNetB0_flowers_model_e_226.h5\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9844 - val_loss: 7.5052 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00227: saving model to tmp/EfficentNetB0_flowers_model_e_227.h5\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0272 - sparse_categorical_accuracy: 0.9933 - val_loss: 7.0724 - val_sparse_categorical_accuracy: 0.1353\n",
      "\n",
      "Epoch 00228: saving model to tmp/EfficentNetB0_flowers_model_e_228.h5\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0486 - sparse_categorical_accuracy: 0.9859 - val_loss: 6.8271 - val_sparse_categorical_accuracy: 0.1510\n",
      "\n",
      "Epoch 00229: saving model to tmp/EfficentNetB0_flowers_model_e_229.h5\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9968 - val_loss: 6.9541 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00230: saving model to tmp/EfficentNetB0_flowers_model_e_230.h5\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0591 - sparse_categorical_accuracy: 0.9848 - val_loss: 7.0002 - val_sparse_categorical_accuracy: 0.1441\n",
      "\n",
      "Epoch 00231: saving model to tmp/EfficentNetB0_flowers_model_e_231.h5\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9912 - val_loss: 6.8877 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00232: saving model to tmp/EfficentNetB0_flowers_model_e_232.h5\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9910 - val_loss: 6.8487 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00233: saving model to tmp/EfficentNetB0_flowers_model_e_233.h5\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9916 - val_loss: 6.8025 - val_sparse_categorical_accuracy: 0.1294\n",
      "\n",
      "Epoch 00234: saving model to tmp/EfficentNetB0_flowers_model_e_234.h5\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9935 - val_loss: 6.8752 - val_sparse_categorical_accuracy: 0.1343\n",
      "\n",
      "Epoch 00235: saving model to tmp/EfficentNetB0_flowers_model_e_235.h5\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0253 - sparse_categorical_accuracy: 0.9925 - val_loss: 6.4772 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00236: saving model to tmp/EfficentNetB0_flowers_model_e_236.h5\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9938 - val_loss: 6.5509 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00237: saving model to tmp/EfficentNetB0_flowers_model_e_237.h5\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9933 - val_loss: 6.7829 - val_sparse_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00238: saving model to tmp/EfficentNetB0_flowers_model_e_238.h5\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9923 - val_loss: 6.7349 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00239: saving model to tmp/EfficentNetB0_flowers_model_e_239.h5\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9908 - val_loss: 6.8856 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00240: saving model to tmp/EfficentNetB0_flowers_model_e_240.h5\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9873 - val_loss: 6.8079 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00241: saving model to tmp/EfficentNetB0_flowers_model_e_241.h5\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9953 - val_loss: 6.5864 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00242: saving model to tmp/EfficentNetB0_flowers_model_e_242.h5\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 17s 416ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9973 - val_loss: 6.5218 - val_sparse_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00243: saving model to tmp/EfficentNetB0_flowers_model_e_243.h5\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9973 - val_loss: 6.3570 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00244: saving model to tmp/EfficentNetB0_flowers_model_e_244.h5\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9982 - val_loss: 6.2381 - val_sparse_categorical_accuracy: 0.1725\n",
      "\n",
      "Epoch 00245: saving model to tmp/EfficentNetB0_flowers_model_e_245.h5\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0239 - sparse_categorical_accuracy: 0.9933 - val_loss: 6.3884 - val_sparse_categorical_accuracy: 0.1608\n",
      "\n",
      "Epoch 00246: saving model to tmp/EfficentNetB0_flowers_model_e_246.h5\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9927 - val_loss: 6.8759 - val_sparse_categorical_accuracy: 0.1461\n",
      "\n",
      "Epoch 00247: saving model to tmp/EfficentNetB0_flowers_model_e_247.h5\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9929 - val_loss: 6.4741 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00248: saving model to tmp/EfficentNetB0_flowers_model_e_248.h5\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 18s 412ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9942 - val_loss: 6.9143 - val_sparse_categorical_accuracy: 0.1539\n",
      "\n",
      "Epoch 00249: saving model to tmp/EfficentNetB0_flowers_model_e_249.h5\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0257 - sparse_categorical_accuracy: 0.9931 - val_loss: 6.6527 - val_sparse_categorical_accuracy: 0.1480\n",
      "\n",
      "Epoch 00250: saving model to tmp/EfficentNetB0_flowers_model_e_250.h5\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0401 - sparse_categorical_accuracy: 0.9879 - val_loss: 7.0372 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00251: saving model to tmp/EfficentNetB0_flowers_model_e_251.h5\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9864 - val_loss: 7.1047 - val_sparse_categorical_accuracy: 0.1392\n",
      "\n",
      "Epoch 00252: saving model to tmp/EfficentNetB0_flowers_model_e_252.h5\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.9887 - val_loss: 7.3647 - val_sparse_categorical_accuracy: 0.1304\n",
      "\n",
      "Epoch 00253: saving model to tmp/EfficentNetB0_flowers_model_e_253.h5\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0550 - sparse_categorical_accuracy: 0.9875 - val_loss: 7.0591 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00254: saving model to tmp/EfficentNetB0_flowers_model_e_254.h5\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9908 - val_loss: 6.6417 - val_sparse_categorical_accuracy: 0.1510\n",
      "\n",
      "Epoch 00255: saving model to tmp/EfficentNetB0_flowers_model_e_255.h5\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0665 - sparse_categorical_accuracy: 0.9864 - val_loss: 7.3326 - val_sparse_categorical_accuracy: 0.1343\n",
      "\n",
      "Epoch 00256: saving model to tmp/EfficentNetB0_flowers_model_e_256.h5\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 18s 425ms/step - loss: 0.0364 - sparse_categorical_accuracy: 0.9917 - val_loss: 6.7097 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00257: saving model to tmp/EfficentNetB0_flowers_model_e_257.h5\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9973 - val_loss: 7.3424 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00258: saving model to tmp/EfficentNetB0_flowers_model_e_258.h5\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9911 - val_loss: 6.7977 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00259: saving model to tmp/EfficentNetB0_flowers_model_e_259.h5\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9982 - val_loss: 6.4588 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00260: saving model to tmp/EfficentNetB0_flowers_model_e_260.h5\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 18s 422ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9953 - val_loss: 6.7815 - val_sparse_categorical_accuracy: 0.1637\n",
      "\n",
      "Epoch 00261: saving model to tmp/EfficentNetB0_flowers_model_e_261.h5\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0276 - sparse_categorical_accuracy: 0.9888 - val_loss: 6.3875 - val_sparse_categorical_accuracy: 0.1598\n",
      "\n",
      "Epoch 00262: saving model to tmp/EfficentNetB0_flowers_model_e_262.h5\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9943 - val_loss: 6.3616 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00263: saving model to tmp/EfficentNetB0_flowers_model_e_263.h5\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9971 - val_loss: 6.5832 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00264: saving model to tmp/EfficentNetB0_flowers_model_e_264.h5\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0286 - sparse_categorical_accuracy: 0.9932 - val_loss: 6.1484 - val_sparse_categorical_accuracy: 0.1745\n",
      "\n",
      "Epoch 00265: saving model to tmp/EfficentNetB0_flowers_model_e_265.h5\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9986 - val_loss: 6.1496 - val_sparse_categorical_accuracy: 0.1784\n",
      "\n",
      "Epoch 00266: saving model to tmp/EfficentNetB0_flowers_model_e_266.h5\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9937 - val_loss: 6.3200 - val_sparse_categorical_accuracy: 0.1814\n",
      "\n",
      "Epoch 00267: saving model to tmp/EfficentNetB0_flowers_model_e_267.h5\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9890 - val_loss: 6.9401 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00268: saving model to tmp/EfficentNetB0_flowers_model_e_268.h5\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 18s 424ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9986 - val_loss: 6.4825 - val_sparse_categorical_accuracy: 0.1471\n",
      "\n",
      "Epoch 00269: saving model to tmp/EfficentNetB0_flowers_model_e_269.h5\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9948 - val_loss: 6.4060 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00270: saving model to tmp/EfficentNetB0_flowers_model_e_270.h5\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9914 - val_loss: 6.5985 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00271: saving model to tmp/EfficentNetB0_flowers_model_e_271.h5\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 18s 426ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9931 - val_loss: 6.8126 - val_sparse_categorical_accuracy: 0.1402\n",
      "\n",
      "Epoch 00272: saving model to tmp/EfficentNetB0_flowers_model_e_272.h5\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9898 - val_loss: 6.5265 - val_sparse_categorical_accuracy: 0.1686\n",
      "\n",
      "Epoch 00273: saving model to tmp/EfficentNetB0_flowers_model_e_273.h5\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9919 - val_loss: 6.5993 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00274: saving model to tmp/EfficentNetB0_flowers_model_e_274.h5\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9945 - val_loss: 6.6509 - val_sparse_categorical_accuracy: 0.1520\n",
      "\n",
      "Epoch 00275: saving model to tmp/EfficentNetB0_flowers_model_e_275.h5\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9887 - val_loss: 6.5872 - val_sparse_categorical_accuracy: 0.1618\n",
      "\n",
      "Epoch 00276: saving model to tmp/EfficentNetB0_flowers_model_e_276.h5\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0072 - sparse_categorical_accuracy: 0.9990 - val_loss: 6.7250 - val_sparse_categorical_accuracy: 0.1725\n",
      "\n",
      "Epoch 00277: saving model to tmp/EfficentNetB0_flowers_model_e_277.h5\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9953 - val_loss: 6.8110 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00278: saving model to tmp/EfficentNetB0_flowers_model_e_278.h5\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9958 - val_loss: 6.6126 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00279: saving model to tmp/EfficentNetB0_flowers_model_e_279.h5\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 18s 421ms/step - loss: 0.0291 - sparse_categorical_accuracy: 0.9939 - val_loss: 6.6162 - val_sparse_categorical_accuracy: 0.1539\n",
      "\n",
      "Epoch 00280: saving model to tmp/EfficentNetB0_flowers_model_e_280.h5\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 17s 410ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9900 - val_loss: 6.7338 - val_sparse_categorical_accuracy: 0.1559\n",
      "\n",
      "Epoch 00281: saving model to tmp/EfficentNetB0_flowers_model_e_281.h5\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9982 - val_loss: 6.4441 - val_sparse_categorical_accuracy: 0.1676\n",
      "\n",
      "Epoch 00282: saving model to tmp/EfficentNetB0_flowers_model_e_282.h5\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9958 - val_loss: 6.4843 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00283: saving model to tmp/EfficentNetB0_flowers_model_e_283.h5\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 18s 414ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9913 - val_loss: 7.2794 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00284: saving model to tmp/EfficentNetB0_flowers_model_e_284.h5\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9917 - val_loss: 6.8204 - val_sparse_categorical_accuracy: 0.1578\n",
      "\n",
      "Epoch 00285: saving model to tmp/EfficentNetB0_flowers_model_e_285.h5\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9972 - val_loss: 6.4865 - val_sparse_categorical_accuracy: 0.1735\n",
      "\n",
      "Epoch 00286: saving model to tmp/EfficentNetB0_flowers_model_e_286.h5\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0306 - sparse_categorical_accuracy: 0.9914 - val_loss: 6.9613 - val_sparse_categorical_accuracy: 0.1716\n",
      "\n",
      "Epoch 00287: saving model to tmp/EfficentNetB0_flowers_model_e_287.h5\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9952 - val_loss: 6.9404 - val_sparse_categorical_accuracy: 0.1755\n",
      "\n",
      "Epoch 00288: saving model to tmp/EfficentNetB0_flowers_model_e_288.h5\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9888 - val_loss: 7.0819 - val_sparse_categorical_accuracy: 0.1647\n",
      "\n",
      "Epoch 00289: saving model to tmp/EfficentNetB0_flowers_model_e_289.h5\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9910 - val_loss: 8.3684 - val_sparse_categorical_accuracy: 0.1373\n",
      "\n",
      "Epoch 00290: saving model to tmp/EfficentNetB0_flowers_model_e_290.h5\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 18s 411ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.9904 - val_loss: 7.5777 - val_sparse_categorical_accuracy: 0.1324\n",
      "\n",
      "Epoch 00291: saving model to tmp/EfficentNetB0_flowers_model_e_291.h5\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9886 - val_loss: 7.6652 - val_sparse_categorical_accuracy: 0.1431\n",
      "\n",
      "Epoch 00292: saving model to tmp/EfficentNetB0_flowers_model_e_292.h5\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 18s 423ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9866 - val_loss: 7.4754 - val_sparse_categorical_accuracy: 0.1578\n",
      "\n",
      "Epoch 00293: saving model to tmp/EfficentNetB0_flowers_model_e_293.h5\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 18s 420ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9838 - val_loss: 7.3782 - val_sparse_categorical_accuracy: 0.1324\n",
      "\n",
      "Epoch 00294: saving model to tmp/EfficentNetB0_flowers_model_e_294.h5\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 18s 418ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9962 - val_loss: 7.7713 - val_sparse_categorical_accuracy: 0.1343\n",
      "\n",
      "Epoch 00295: saving model to tmp/EfficentNetB0_flowers_model_e_295.h5\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 18s 419ms/step - loss: 0.0202 - sparse_categorical_accuracy: 0.9960 - val_loss: 7.4162 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00296: saving model to tmp/EfficentNetB0_flowers_model_e_296.h5\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9908 - val_loss: 7.2198 - val_sparse_categorical_accuracy: 0.1363\n",
      "\n",
      "Epoch 00297: saving model to tmp/EfficentNetB0_flowers_model_e_297.h5\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 18s 416ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9904 - val_loss: 7.2305 - val_sparse_categorical_accuracy: 0.1382\n",
      "\n",
      "Epoch 00298: saving model to tmp/EfficentNetB0_flowers_model_e_298.h5\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 18s 417ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9941 - val_loss: 6.9138 - val_sparse_categorical_accuracy: 0.1539\n",
      "\n",
      "Epoch 00299: saving model to tmp/EfficentNetB0_flowers_model_e_299.h5\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 18s 415ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9982 - val_loss: 6.8714 - val_sparse_categorical_accuracy: 0.1647\n",
      "\n",
      "Epoch 00300: saving model to tmp/EfficentNetB0_flowers_model_e_300.h5\n",
      "Saved to: EfficentNetB0_flowers_model.h5\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB0(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_flowers(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = './flowers_models/EfficentNetB4_flowers_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 45s 927ms/step - loss: 4.8518 - sparse_categorical_accuracy: 0.0060 - val_loss: 4.6327 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00001: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_01.h5\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 32s 864ms/step - loss: 4.6755 - sparse_categorical_accuracy: 0.0201 - val_loss: 4.6627 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00002: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_02.h5\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 4.7108 - sparse_categorical_accuracy: 0.0232 - val_loss: 4.7506 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00003: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_03.h5\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 32s 846ms/step - loss: 4.5762 - sparse_categorical_accuracy: 0.0293 - val_loss: 4.8663 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00004: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_04.h5\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 4.5655 - sparse_categorical_accuracy: 0.0208 - val_loss: 4.8707 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00005: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_05.h5\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 4.4890 - sparse_categorical_accuracy: 0.0453 - val_loss: 5.3437 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00006: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_06.h5\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 4.2285 - sparse_categorical_accuracy: 0.0393 - val_loss: 5.0570 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00007: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_07.h5\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 4.2029 - sparse_categorical_accuracy: 0.0410 - val_loss: 5.3717 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00008: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_08.h5\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 4.0167 - sparse_categorical_accuracy: 0.0642 - val_loss: 5.4358 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00009: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_09.h5\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 3.8954 - sparse_categorical_accuracy: 0.0844 - val_loss: 5.4731 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00010: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_10.h5\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 3.6315 - sparse_categorical_accuracy: 0.1122 - val_loss: 5.9403 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00011: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_11.h5\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 3.6074 - sparse_categorical_accuracy: 0.1002 - val_loss: 6.8652 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00012: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_12.h5\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 3.3977 - sparse_categorical_accuracy: 0.1566 - val_loss: 7.1343 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00013: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_13.h5\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 3.3244 - sparse_categorical_accuracy: 0.1588 - val_loss: 7.0443 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00014: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_14.h5\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 3.0921 - sparse_categorical_accuracy: 0.2091 - val_loss: 6.7827 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00015: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_15.h5\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 2.9564 - sparse_categorical_accuracy: 0.2003 - val_loss: 6.5342 - val_sparse_categorical_accuracy: 0.0176\n",
      "\n",
      "Epoch 00016: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_16.h5\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 31s 845ms/step - loss: 2.7464 - sparse_categorical_accuracy: 0.2667 - val_loss: 8.0014 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00017: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_17.h5\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 2.5474 - sparse_categorical_accuracy: 0.3161 - val_loss: 6.9247 - val_sparse_categorical_accuracy: 0.0098\n",
      "\n",
      "Epoch 00018: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_18.h5\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 2.3861 - sparse_categorical_accuracy: 0.3708 - val_loss: 6.7333 - val_sparse_categorical_accuracy: 0.0167\n",
      "\n",
      "Epoch 00019: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_19.h5\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 2.1131 - sparse_categorical_accuracy: 0.4222 - val_loss: 6.0241 - val_sparse_categorical_accuracy: 0.0137\n",
      "\n",
      "Epoch 00020: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_20.h5\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 2.1159 - sparse_categorical_accuracy: 0.4237 - val_loss: 6.3610 - val_sparse_categorical_accuracy: 0.0206\n",
      "\n",
      "Epoch 00021: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_21.h5\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 1.8311 - sparse_categorical_accuracy: 0.5160 - val_loss: 5.6744 - val_sparse_categorical_accuracy: 0.0333\n",
      "\n",
      "Epoch 00022: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_22.h5\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 1.5732 - sparse_categorical_accuracy: 0.5593 - val_loss: 5.2355 - val_sparse_categorical_accuracy: 0.0549\n",
      "\n",
      "Epoch 00023: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_23.h5\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 1.5595 - sparse_categorical_accuracy: 0.5592 - val_loss: 5.3849 - val_sparse_categorical_accuracy: 0.0637\n",
      "\n",
      "Epoch 00024: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_24.h5\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 1.3427 - sparse_categorical_accuracy: 0.6404 - val_loss: 4.9873 - val_sparse_categorical_accuracy: 0.1049\n",
      "\n",
      "Epoch 00025: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_25.h5\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 31s 842ms/step - loss: 1.3062 - sparse_categorical_accuracy: 0.6561 - val_loss: 4.4240 - val_sparse_categorical_accuracy: 0.1333\n",
      "\n",
      "Epoch 00026: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_26.h5\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 1.2215 - sparse_categorical_accuracy: 0.6809 - val_loss: 4.3530 - val_sparse_categorical_accuracy: 0.1529\n",
      "\n",
      "Epoch 00027: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_27.h5\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 1.0314 - sparse_categorical_accuracy: 0.7158 - val_loss: 4.3235 - val_sparse_categorical_accuracy: 0.1559\n",
      "\n",
      "Epoch 00028: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_28.h5\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.8710 - sparse_categorical_accuracy: 0.7809 - val_loss: 4.5227 - val_sparse_categorical_accuracy: 0.1667\n",
      "\n",
      "Epoch 00029: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_29.h5\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.7784 - sparse_categorical_accuracy: 0.7993 - val_loss: 4.5761 - val_sparse_categorical_accuracy: 0.1588\n",
      "\n",
      "Epoch 00030: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_30.h5\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.7090 - sparse_categorical_accuracy: 0.8304 - val_loss: 4.7326 - val_sparse_categorical_accuracy: 0.1657\n",
      "\n",
      "Epoch 00031: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_31.h5\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.7658 - sparse_categorical_accuracy: 0.7966 - val_loss: 4.7708 - val_sparse_categorical_accuracy: 0.1608\n",
      "\n",
      "Epoch 00032: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_32.h5\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.5703 - sparse_categorical_accuracy: 0.8607 - val_loss: 4.2790 - val_sparse_categorical_accuracy: 0.1922\n",
      "\n",
      "Epoch 00033: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_33.h5\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.4724 - sparse_categorical_accuracy: 0.8877 - val_loss: 4.5893 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00034: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_34.h5\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.5419 - sparse_categorical_accuracy: 0.8619 - val_loss: 4.4671 - val_sparse_categorical_accuracy: 0.1990\n",
      "\n",
      "Epoch 00035: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_35.h5\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.5058 - sparse_categorical_accuracy: 0.8728 - val_loss: 4.9129 - val_sparse_categorical_accuracy: 0.1549\n",
      "\n",
      "Epoch 00036: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_36.h5\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.5304 - sparse_categorical_accuracy: 0.8624 - val_loss: 4.7951 - val_sparse_categorical_accuracy: 0.1657\n",
      "\n",
      "Epoch 00037: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_37.h5\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.4653 - sparse_categorical_accuracy: 0.8814 - val_loss: 4.6629 - val_sparse_categorical_accuracy: 0.1686\n",
      "\n",
      "Epoch 00038: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_38.h5\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.4007 - sparse_categorical_accuracy: 0.9086 - val_loss: 4.6206 - val_sparse_categorical_accuracy: 0.1882\n",
      "\n",
      "Epoch 00039: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_39.h5\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.3774 - sparse_categorical_accuracy: 0.9063 - val_loss: 4.5984 - val_sparse_categorical_accuracy: 0.1971\n",
      "\n",
      "Epoch 00040: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_40.h5\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.3551 - sparse_categorical_accuracy: 0.9230 - val_loss: 4.9718 - val_sparse_categorical_accuracy: 0.1775\n",
      "\n",
      "Epoch 00041: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_41.h5\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.3312 - sparse_categorical_accuracy: 0.9194 - val_loss: 4.5859 - val_sparse_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00042: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_42.h5\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.3496 - sparse_categorical_accuracy: 0.9077 - val_loss: 4.9190 - val_sparse_categorical_accuracy: 0.1784\n",
      "\n",
      "Epoch 00043: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_43.h5\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 31s 843ms/step - loss: 0.3303 - sparse_categorical_accuracy: 0.9227 - val_loss: 4.9858 - val_sparse_categorical_accuracy: 0.1706\n",
      "\n",
      "Epoch 00044: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_44.h5\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 32s 860ms/step - loss: 0.3113 - sparse_categorical_accuracy: 0.9288 - val_loss: 4.5729 - val_sparse_categorical_accuracy: 0.1902\n",
      "\n",
      "Epoch 00045: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_45.h5\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 31s 843ms/step - loss: 0.3297 - sparse_categorical_accuracy: 0.9142 - val_loss: 4.9116 - val_sparse_categorical_accuracy: 0.1676\n",
      "\n",
      "Epoch 00046: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_46.h5\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.2817 - sparse_categorical_accuracy: 0.9251 - val_loss: 4.8139 - val_sparse_categorical_accuracy: 0.1814\n",
      "\n",
      "Epoch 00047: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_47.h5\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.2407 - sparse_categorical_accuracy: 0.9348 - val_loss: 4.7924 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00048: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_48.h5\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.2658 - sparse_categorical_accuracy: 0.9490 - val_loss: 4.8549 - val_sparse_categorical_accuracy: 0.1873\n",
      "\n",
      "Epoch 00049: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_49.h5\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.2340 - sparse_categorical_accuracy: 0.9393 - val_loss: 4.8039 - val_sparse_categorical_accuracy: 0.1863\n",
      "\n",
      "Epoch 00050: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_50.h5\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.3014 - sparse_categorical_accuracy: 0.9249 - val_loss: 4.8878 - val_sparse_categorical_accuracy: 0.1882\n",
      "\n",
      "Epoch 00051: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_51.h5\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.2101 - sparse_categorical_accuracy: 0.9591 - val_loss: 4.7897 - val_sparse_categorical_accuracy: 0.2157\n",
      "\n",
      "Epoch 00052: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_52.h5\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 32s 847ms/step - loss: 0.2306 - sparse_categorical_accuracy: 0.9312 - val_loss: 4.8637 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00053: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_53.h5\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.2101 - sparse_categorical_accuracy: 0.9356 - val_loss: 4.7332 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00054: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_54.h5\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.2352 - sparse_categorical_accuracy: 0.9535 - val_loss: 5.1020 - val_sparse_categorical_accuracy: 0.1735\n",
      "\n",
      "Epoch 00055: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_55.h5\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9536 - val_loss: 5.0071 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00056: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_56.h5\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 31s 851ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9419 - val_loss: 5.1805 - val_sparse_categorical_accuracy: 0.1833\n",
      "\n",
      "Epoch 00057: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_57.h5\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.1624 - sparse_categorical_accuracy: 0.9650 - val_loss: 4.8119 - val_sparse_categorical_accuracy: 0.2157\n",
      "\n",
      "Epoch 00058: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_58.h5\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.1758 - sparse_categorical_accuracy: 0.9483 - val_loss: 4.8197 - val_sparse_categorical_accuracy: 0.2206\n",
      "\n",
      "Epoch 00059: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_59.h5\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.1897 - sparse_categorical_accuracy: 0.9542 - val_loss: 4.7700 - val_sparse_categorical_accuracy: 0.2010\n",
      "\n",
      "Epoch 00060: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_60.h5\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.2471 - sparse_categorical_accuracy: 0.9446 - val_loss: 5.2649 - val_sparse_categorical_accuracy: 0.1775\n",
      "\n",
      "Epoch 00061: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_61.h5\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.1746 - sparse_categorical_accuracy: 0.9596 - val_loss: 4.9654 - val_sparse_categorical_accuracy: 0.1922\n",
      "\n",
      "Epoch 00062: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_62.h5\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.2211 - sparse_categorical_accuracy: 0.9377 - val_loss: 5.1980 - val_sparse_categorical_accuracy: 0.1843\n",
      "\n",
      "Epoch 00063: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_63.h5\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.1604 - sparse_categorical_accuracy: 0.9613 - val_loss: 5.0054 - val_sparse_categorical_accuracy: 0.1794\n",
      "\n",
      "Epoch 00064: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_64.h5\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.1877 - sparse_categorical_accuracy: 0.9487 - val_loss: 4.8907 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00065: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_65.h5\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.2196 - sparse_categorical_accuracy: 0.9518 - val_loss: 6.3274 - val_sparse_categorical_accuracy: 0.1275\n",
      "\n",
      "Epoch 00066: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_66.h5\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.2088 - sparse_categorical_accuracy: 0.9457 - val_loss: 5.2982 - val_sparse_categorical_accuracy: 0.1980\n",
      "\n",
      "Epoch 00067: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_67.h5\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.1718 - sparse_categorical_accuracy: 0.9622 - val_loss: 5.4118 - val_sparse_categorical_accuracy: 0.1902\n",
      "\n",
      "Epoch 00068: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_68.h5\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.1350 - sparse_categorical_accuracy: 0.9685 - val_loss: 4.9315 - val_sparse_categorical_accuracy: 0.2039\n",
      "\n",
      "Epoch 00069: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_69.h5\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 31s 845ms/step - loss: 0.1481 - sparse_categorical_accuracy: 0.9652 - val_loss: 4.7820 - val_sparse_categorical_accuracy: 0.2049\n",
      "\n",
      "Epoch 00070: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_70.h5\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.1751 - sparse_categorical_accuracy: 0.9494 - val_loss: 4.6512 - val_sparse_categorical_accuracy: 0.2382\n",
      "\n",
      "Epoch 00071: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_71.h5\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.1487 - sparse_categorical_accuracy: 0.9634 - val_loss: 4.9475 - val_sparse_categorical_accuracy: 0.1902\n",
      "\n",
      "Epoch 00072: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_72.h5\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.1855 - sparse_categorical_accuracy: 0.9605 - val_loss: 4.9427 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00073: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_73.h5\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.1853 - sparse_categorical_accuracy: 0.9437 - val_loss: 4.8184 - val_sparse_categorical_accuracy: 0.2216\n",
      "\n",
      "Epoch 00074: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_74.h5\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.1175 - sparse_categorical_accuracy: 0.9709 - val_loss: 4.8331 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00075: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_75.h5\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.1157 - sparse_categorical_accuracy: 0.9712 - val_loss: 4.7694 - val_sparse_categorical_accuracy: 0.2206\n",
      "\n",
      "Epoch 00076: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_76.h5\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0991 - sparse_categorical_accuracy: 0.9712 - val_loss: 4.8545 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00077: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_77.h5\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.1059 - sparse_categorical_accuracy: 0.9690 - val_loss: 4.8537 - val_sparse_categorical_accuracy: 0.2314\n",
      "\n",
      "Epoch 00078: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_78.h5\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9746 - val_loss: 5.2495 - val_sparse_categorical_accuracy: 0.1873\n",
      "\n",
      "Epoch 00079: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_79.h5\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.1125 - sparse_categorical_accuracy: 0.9783 - val_loss: 5.0431 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00080: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_80.h5\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0894 - sparse_categorical_accuracy: 0.9786 - val_loss: 4.8305 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00081: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_81.h5\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 31s 851ms/step - loss: 0.1055 - sparse_categorical_accuracy: 0.9657 - val_loss: 4.9598 - val_sparse_categorical_accuracy: 0.2039\n",
      "\n",
      "Epoch 00082: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_82.h5\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.1460 - sparse_categorical_accuracy: 0.9603 - val_loss: 5.1755 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00083: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_83.h5\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9887 - val_loss: 4.9951 - val_sparse_categorical_accuracy: 0.2304\n",
      "\n",
      "Epoch 00084: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_84.h5\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.1573 - sparse_categorical_accuracy: 0.9632 - val_loss: 5.0244 - val_sparse_categorical_accuracy: 0.2314\n",
      "\n",
      "Epoch 00085: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_85.h5\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 32s 859ms/step - loss: 0.0655 - sparse_categorical_accuracy: 0.9869 - val_loss: 5.4780 - val_sparse_categorical_accuracy: 0.2049\n",
      "\n",
      "Epoch 00086: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_86.h5\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0977 - sparse_categorical_accuracy: 0.9838 - val_loss: 5.2932 - val_sparse_categorical_accuracy: 0.2029\n",
      "\n",
      "Epoch 00087: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_87.h5\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0913 - sparse_categorical_accuracy: 0.9732 - val_loss: 5.0541 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00088: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_88.h5\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.1178 - sparse_categorical_accuracy: 0.9631 - val_loss: 5.4892 - val_sparse_categorical_accuracy: 0.1775\n",
      "\n",
      "Epoch 00089: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_89.h5\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.0840 - sparse_categorical_accuracy: 0.9791 - val_loss: 5.3099 - val_sparse_categorical_accuracy: 0.2049\n",
      "\n",
      "Epoch 00090: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_90.h5\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.1253 - sparse_categorical_accuracy: 0.9592 - val_loss: 5.6679 - val_sparse_categorical_accuracy: 0.2029\n",
      "\n",
      "Epoch 00091: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_91.h5\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.1034 - sparse_categorical_accuracy: 0.9769 - val_loss: 5.3010 - val_sparse_categorical_accuracy: 0.2088\n",
      "\n",
      "Epoch 00092: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_92.h5\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.1091 - sparse_categorical_accuracy: 0.9666 - val_loss: 5.2970 - val_sparse_categorical_accuracy: 0.1990\n",
      "\n",
      "Epoch 00093: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_93.h5\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 32s 863ms/step - loss: 0.1009 - sparse_categorical_accuracy: 0.9750 - val_loss: 5.0847 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00094: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_94.h5\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0954 - sparse_categorical_accuracy: 0.9707 - val_loss: 4.9015 - val_sparse_categorical_accuracy: 0.2216\n",
      "\n",
      "Epoch 00095: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_95.h5\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.1148 - sparse_categorical_accuracy: 0.9738 - val_loss: 5.5189 - val_sparse_categorical_accuracy: 0.1745\n",
      "\n",
      "Epoch 00096: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_96.h5\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9659 - val_loss: 5.0655 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00097: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_97.h5\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0948 - sparse_categorical_accuracy: 0.9742 - val_loss: 5.0975 - val_sparse_categorical_accuracy: 0.2039\n",
      "\n",
      "Epoch 00098: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_98.h5\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.1062 - sparse_categorical_accuracy: 0.9742 - val_loss: 5.0557 - val_sparse_categorical_accuracy: 0.2049\n",
      "\n",
      "Epoch 00099: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_99.h5\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.1415 - sparse_categorical_accuracy: 0.9615 - val_loss: 5.1949 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00100: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_100.h5\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9752 - val_loss: 4.9854 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00101: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_101.h5\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 31s 851ms/step - loss: 0.1356 - sparse_categorical_accuracy: 0.9621 - val_loss: 5.0972 - val_sparse_categorical_accuracy: 0.2441\n",
      "\n",
      "Epoch 00102: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_102.h5\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0846 - sparse_categorical_accuracy: 0.9794 - val_loss: 5.0719 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00103: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_103.h5\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0716 - sparse_categorical_accuracy: 0.9807 - val_loss: 4.9707 - val_sparse_categorical_accuracy: 0.2245\n",
      "\n",
      "Epoch 00104: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_104.h5\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0730 - sparse_categorical_accuracy: 0.9809 - val_loss: 5.3223 - val_sparse_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00105: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_105.h5\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.0754 - sparse_categorical_accuracy: 0.9765 - val_loss: 5.2098 - val_sparse_categorical_accuracy: 0.2353\n",
      "\n",
      "Epoch 00106: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_106.h5\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0991 - sparse_categorical_accuracy: 0.9755 - val_loss: 5.3925 - val_sparse_categorical_accuracy: 0.2088\n",
      "\n",
      "Epoch 00107: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_107.h5\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0576 - sparse_categorical_accuracy: 0.9865 - val_loss: 5.4595 - val_sparse_categorical_accuracy: 0.2088\n",
      "\n",
      "Epoch 00108: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_108.h5\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.0872 - sparse_categorical_accuracy: 0.9783 - val_loss: 5.4648 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00109: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_109.h5\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9874 - val_loss: 5.3718 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00110: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_110.h5\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0846 - sparse_categorical_accuracy: 0.9817 - val_loss: 5.3197 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00111: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_111.h5\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0741 - sparse_categorical_accuracy: 0.9780 - val_loss: 5.0779 - val_sparse_categorical_accuracy: 0.2147\n",
      "\n",
      "Epoch 00112: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_112.h5\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9829 - val_loss: 4.9832 - val_sparse_categorical_accuracy: 0.2343\n",
      "\n",
      "Epoch 00113: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_113.h5\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0789 - sparse_categorical_accuracy: 0.9763 - val_loss: 5.2612 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00114: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_114.h5\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0660 - sparse_categorical_accuracy: 0.9851 - val_loss: 5.2681 - val_sparse_categorical_accuracy: 0.2020\n",
      "\n",
      "Epoch 00115: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_115.h5\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0796 - sparse_categorical_accuracy: 0.9803 - val_loss: 5.3509 - val_sparse_categorical_accuracy: 0.2020\n",
      "\n",
      "Epoch 00116: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_116.h5\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0680 - sparse_categorical_accuracy: 0.9802 - val_loss: 5.8763 - val_sparse_categorical_accuracy: 0.1676\n",
      "\n",
      "Epoch 00117: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_117.h5\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9821 - val_loss: 5.5794 - val_sparse_categorical_accuracy: 0.1990\n",
      "\n",
      "Epoch 00118: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_118.h5\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9691 - val_loss: 5.1416 - val_sparse_categorical_accuracy: 0.2206\n",
      "\n",
      "Epoch 00119: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_119.h5\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 32s 860ms/step - loss: 0.0382 - sparse_categorical_accuracy: 0.9951 - val_loss: 5.1546 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00120: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_120.h5\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 32s 861ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9883 - val_loss: 5.2443 - val_sparse_categorical_accuracy: 0.2039\n",
      "\n",
      "Epoch 00121: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_121.h5\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 31s 845ms/step - loss: 0.0649 - sparse_categorical_accuracy: 0.9791 - val_loss: 5.4595 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00122: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_122.h5\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 31s 854ms/step - loss: 0.0708 - sparse_categorical_accuracy: 0.9833 - val_loss: 5.3831 - val_sparse_categorical_accuracy: 0.2245\n",
      "\n",
      "Epoch 00123: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_123.h5\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0649 - sparse_categorical_accuracy: 0.9838 - val_loss: 5.5014 - val_sparse_categorical_accuracy: 0.2255\n",
      "\n",
      "Epoch 00124: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_124.h5\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 31s 843ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9911 - val_loss: 5.6432 - val_sparse_categorical_accuracy: 0.1863\n",
      "\n",
      "Epoch 00125: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_125.h5\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0573 - sparse_categorical_accuracy: 0.9860 - val_loss: 5.3326 - val_sparse_categorical_accuracy: 0.2157\n",
      "\n",
      "Epoch 00126: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_126.h5\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0748 - sparse_categorical_accuracy: 0.9784 - val_loss: 5.5244 - val_sparse_categorical_accuracy: 0.2176\n",
      "\n",
      "Epoch 00127: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_127.h5\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0513 - sparse_categorical_accuracy: 0.9853 - val_loss: 5.8301 - val_sparse_categorical_accuracy: 0.1784\n",
      "\n",
      "Epoch 00128: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_128.h5\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.1010 - sparse_categorical_accuracy: 0.9701 - val_loss: 5.6840 - val_sparse_categorical_accuracy: 0.1775\n",
      "\n",
      "Epoch 00129: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_129.h5\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0850 - sparse_categorical_accuracy: 0.9733 - val_loss: 6.1021 - val_sparse_categorical_accuracy: 0.1971\n",
      "\n",
      "Epoch 00130: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_130.h5\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0835 - sparse_categorical_accuracy: 0.9740 - val_loss: 6.8891 - val_sparse_categorical_accuracy: 0.1490\n",
      "\n",
      "Epoch 00131: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_131.h5\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.1188 - sparse_categorical_accuracy: 0.9677 - val_loss: 6.1835 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00132: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_132.h5\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.1085 - sparse_categorical_accuracy: 0.9729 - val_loss: 5.9155 - val_sparse_categorical_accuracy: 0.1735\n",
      "\n",
      "Epoch 00133: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_133.h5\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9779 - val_loss: 5.6535 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00134: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_134.h5\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.0548 - sparse_categorical_accuracy: 0.9869 - val_loss: 5.5214 - val_sparse_categorical_accuracy: 0.1990\n",
      "\n",
      "Epoch 00135: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_135.h5\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0626 - sparse_categorical_accuracy: 0.9764 - val_loss: 5.6302 - val_sparse_categorical_accuracy: 0.2225\n",
      "\n",
      "Epoch 00136: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_136.h5\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0495 - sparse_categorical_accuracy: 0.9853 - val_loss: 5.2135 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00137: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_137.h5\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9826 - val_loss: 5.6061 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00138: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_138.h5\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 31s 838ms/step - loss: 0.0776 - sparse_categorical_accuracy: 0.9737 - val_loss: 5.1493 - val_sparse_categorical_accuracy: 0.2441\n",
      "\n",
      "Epoch 00139: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_139.h5\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.1093 - sparse_categorical_accuracy: 0.9687 - val_loss: 5.8075 - val_sparse_categorical_accuracy: 0.1686\n",
      "\n",
      "Epoch 00140: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_140.h5\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0587 - sparse_categorical_accuracy: 0.9839 - val_loss: 5.1023 - val_sparse_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00141: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_141.h5\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0536 - sparse_categorical_accuracy: 0.9842 - val_loss: 5.1659 - val_sparse_categorical_accuracy: 0.2324\n",
      "\n",
      "Epoch 00142: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_142.h5\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0671 - sparse_categorical_accuracy: 0.9797 - val_loss: 5.5718 - val_sparse_categorical_accuracy: 0.2176\n",
      "\n",
      "Epoch 00143: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_143.h5\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.0696 - sparse_categorical_accuracy: 0.9790 - val_loss: 5.9793 - val_sparse_categorical_accuracy: 0.1892\n",
      "\n",
      "Epoch 00144: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_144.h5\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.1287 - sparse_categorical_accuracy: 0.9568 - val_loss: 6.3815 - val_sparse_categorical_accuracy: 0.1735\n",
      "\n",
      "Epoch 00145: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_145.h5\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.1317 - sparse_categorical_accuracy: 0.9593 - val_loss: 5.9388 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00146: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_146.h5\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 31s 842ms/step - loss: 0.0780 - sparse_categorical_accuracy: 0.9773 - val_loss: 5.9448 - val_sparse_categorical_accuracy: 0.2088\n",
      "\n",
      "Epoch 00147: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_147.h5\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0778 - sparse_categorical_accuracy: 0.9782 - val_loss: 5.6488 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00148: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_148.h5\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0792 - sparse_categorical_accuracy: 0.9834 - val_loss: 5.6689 - val_sparse_categorical_accuracy: 0.2363\n",
      "\n",
      "Epoch 00149: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_149.h5\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0738 - sparse_categorical_accuracy: 0.9733 - val_loss: 5.6631 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00150: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_150.h5\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0891 - sparse_categorical_accuracy: 0.9819 - val_loss: 5.2873 - val_sparse_categorical_accuracy: 0.2118\n",
      "\n",
      "Epoch 00151: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_151.h5\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0832 - sparse_categorical_accuracy: 0.9738 - val_loss: 5.4999 - val_sparse_categorical_accuracy: 0.2206\n",
      "\n",
      "Epoch 00152: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_152.h5\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0411 - sparse_categorical_accuracy: 0.9897 - val_loss: 5.8559 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00153: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_153.h5\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0589 - sparse_categorical_accuracy: 0.9825 - val_loss: 6.4251 - val_sparse_categorical_accuracy: 0.1412\n",
      "\n",
      "Epoch 00154: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_154.h5\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9835 - val_loss: 5.4176 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00155: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_155.h5\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0594 - sparse_categorical_accuracy: 0.9784 - val_loss: 5.5128 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00156: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_156.h5\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9906 - val_loss: 5.2352 - val_sparse_categorical_accuracy: 0.2265\n",
      "\n",
      "Epoch 00157: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_157.h5\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9835 - val_loss: 5.1736 - val_sparse_categorical_accuracy: 0.2324\n",
      "\n",
      "Epoch 00158: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_158.h5\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9901 - val_loss: 5.1662 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00159: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_159.h5\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0527 - sparse_categorical_accuracy: 0.9855 - val_loss: 5.1125 - val_sparse_categorical_accuracy: 0.2451\n",
      "\n",
      "Epoch 00160: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_160.h5\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0845 - sparse_categorical_accuracy: 0.9789 - val_loss: 5.5801 - val_sparse_categorical_accuracy: 0.2020\n",
      "\n",
      "Epoch 00161: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_161.h5\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0598 - sparse_categorical_accuracy: 0.9808 - val_loss: 5.4640 - val_sparse_categorical_accuracy: 0.2176\n",
      "\n",
      "Epoch 00162: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_162.h5\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0358 - sparse_categorical_accuracy: 0.9894 - val_loss: 5.7103 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00163: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_163.h5\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0337 - sparse_categorical_accuracy: 0.9912 - val_loss: 5.3686 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00164: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_164.h5\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.9905 - val_loss: 5.3684 - val_sparse_categorical_accuracy: 0.2245\n",
      "\n",
      "Epoch 00165: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_165.h5\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0475 - sparse_categorical_accuracy: 0.9871 - val_loss: 5.2951 - val_sparse_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00166: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_166.h5\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9876 - val_loss: 5.2959 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00167: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_167.h5\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9939 - val_loss: 5.2189 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00168: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_168.h5\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.0221 - sparse_categorical_accuracy: 0.9917 - val_loss: 5.0435 - val_sparse_categorical_accuracy: 0.2549\n",
      "\n",
      "Epoch 00169: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_169.h5\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9887 - val_loss: 5.2550 - val_sparse_categorical_accuracy: 0.2412\n",
      "\n",
      "Epoch 00170: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_170.h5\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9928 - val_loss: 5.4413 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00171: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_171.h5\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.0332 - sparse_categorical_accuracy: 0.9912 - val_loss: 5.4438 - val_sparse_categorical_accuracy: 0.2176\n",
      "\n",
      "Epoch 00172: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_172.h5\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0893 - sparse_categorical_accuracy: 0.9804 - val_loss: 5.5040 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00173: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_173.h5\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 31s 841ms/step - loss: 0.0635 - sparse_categorical_accuracy: 0.9813 - val_loss: 5.9516 - val_sparse_categorical_accuracy: 0.2147\n",
      "\n",
      "Epoch 00174: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_174.h5\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0617 - sparse_categorical_accuracy: 0.9835 - val_loss: 5.6186 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00175: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_175.h5\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9871 - val_loss: 6.1927 - val_sparse_categorical_accuracy: 0.1833\n",
      "\n",
      "Epoch 00176: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_176.h5\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 31s 845ms/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9835 - val_loss: 5.5803 - val_sparse_categorical_accuracy: 0.2216\n",
      "\n",
      "Epoch 00177: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_177.h5\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0639 - sparse_categorical_accuracy: 0.9801 - val_loss: 5.5280 - val_sparse_categorical_accuracy: 0.2206\n",
      "\n",
      "Epoch 00178: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_178.h5\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0561 - sparse_categorical_accuracy: 0.9823 - val_loss: 5.6856 - val_sparse_categorical_accuracy: 0.1990\n",
      "\n",
      "Epoch 00179: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_179.h5\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9811 - val_loss: 5.5484 - val_sparse_categorical_accuracy: 0.2049\n",
      "\n",
      "Epoch 00180: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_180.h5\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9852 - val_loss: 5.5084 - val_sparse_categorical_accuracy: 0.2245\n",
      "\n",
      "Epoch 00181: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_181.h5\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 31s 841ms/step - loss: 0.0280 - sparse_categorical_accuracy: 0.9929 - val_loss: 6.0188 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00182: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_182.h5\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9901 - val_loss: 6.0549 - val_sparse_categorical_accuracy: 0.1843\n",
      "\n",
      "Epoch 00183: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_183.h5\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9887 - val_loss: 5.4574 - val_sparse_categorical_accuracy: 0.2431\n",
      "\n",
      "Epoch 00184: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_184.h5\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9858 - val_loss: 5.9603 - val_sparse_categorical_accuracy: 0.2010\n",
      "\n",
      "Epoch 00185: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_185.h5\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 32s 845ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9773 - val_loss: 5.5588 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00186: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_186.h5\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0561 - sparse_categorical_accuracy: 0.9874 - val_loss: 5.7372 - val_sparse_categorical_accuracy: 0.2029\n",
      "\n",
      "Epoch 00187: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_187.h5\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9876 - val_loss: 5.7506 - val_sparse_categorical_accuracy: 0.2265\n",
      "\n",
      "Epoch 00188: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_188.h5\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0401 - sparse_categorical_accuracy: 0.9890 - val_loss: 5.6847 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00189: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_189.h5\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0465 - sparse_categorical_accuracy: 0.9902 - val_loss: 5.1487 - val_sparse_categorical_accuracy: 0.2490\n",
      "\n",
      "Epoch 00190: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_190.h5\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9920 - val_loss: 5.4037 - val_sparse_categorical_accuracy: 0.2363\n",
      "\n",
      "Epoch 00191: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_191.h5\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0320 - sparse_categorical_accuracy: 0.9968 - val_loss: 5.0683 - val_sparse_categorical_accuracy: 0.2539\n",
      "\n",
      "Epoch 00192: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_192.h5\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9905 - val_loss: 5.5386 - val_sparse_categorical_accuracy: 0.2284\n",
      "\n",
      "Epoch 00193: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_193.h5\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.0559 - sparse_categorical_accuracy: 0.9864 - val_loss: 5.6766 - val_sparse_categorical_accuracy: 0.2157\n",
      "\n",
      "Epoch 00194: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_194.h5\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9931 - val_loss: 5.3468 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00195: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_195.h5\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0498 - sparse_categorical_accuracy: 0.9820 - val_loss: 5.6646 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00196: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_196.h5\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.9887 - val_loss: 5.2825 - val_sparse_categorical_accuracy: 0.2412\n",
      "\n",
      "Epoch 00197: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_197.h5\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9879 - val_loss: 5.5840 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00198: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_198.h5\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 31s 843ms/step - loss: 0.0376 - sparse_categorical_accuracy: 0.9862 - val_loss: 6.1002 - val_sparse_categorical_accuracy: 0.1892\n",
      "\n",
      "Epoch 00199: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_199.h5\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0356 - sparse_categorical_accuracy: 0.9889 - val_loss: 5.8973 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00200: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_200.h5\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9863 - val_loss: 6.1024 - val_sparse_categorical_accuracy: 0.1814\n",
      "\n",
      "Epoch 00201: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_201.h5\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0638 - sparse_categorical_accuracy: 0.9818 - val_loss: 5.8233 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00202: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_202.h5\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9913 - val_loss: 5.5962 - val_sparse_categorical_accuracy: 0.2245\n",
      "\n",
      "Epoch 00203: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_203.h5\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9919 - val_loss: 5.9486 - val_sparse_categorical_accuracy: 0.1912\n",
      "\n",
      "Epoch 00204: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_204.h5\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9845 - val_loss: 5.5323 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00205: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_205.h5\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9921 - val_loss: 5.3071 - val_sparse_categorical_accuracy: 0.2461\n",
      "\n",
      "Epoch 00206: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_206.h5\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9893 - val_loss: 5.6242 - val_sparse_categorical_accuracy: 0.2078\n",
      "\n",
      "Epoch 00207: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_207.h5\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9887 - val_loss: 5.6202 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00208: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_208.h5\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0668 - sparse_categorical_accuracy: 0.9806 - val_loss: 5.4271 - val_sparse_categorical_accuracy: 0.2225\n",
      "\n",
      "Epoch 00209: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_209.h5\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0319 - sparse_categorical_accuracy: 0.9922 - val_loss: 5.4610 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00210: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_210.h5\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0631 - sparse_categorical_accuracy: 0.9793 - val_loss: 5.7766 - val_sparse_categorical_accuracy: 0.1833\n",
      "\n",
      "Epoch 00211: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_211.h5\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.0571 - sparse_categorical_accuracy: 0.9823 - val_loss: 5.5804 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00212: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_212.h5\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.0408 - sparse_categorical_accuracy: 0.9868 - val_loss: 5.5260 - val_sparse_categorical_accuracy: 0.2265\n",
      "\n",
      "Epoch 00213: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_213.h5\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9873 - val_loss: 5.6977 - val_sparse_categorical_accuracy: 0.2088\n",
      "\n",
      "Epoch 00214: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_214.h5\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0451 - sparse_categorical_accuracy: 0.9873 - val_loss: 5.3528 - val_sparse_categorical_accuracy: 0.2284\n",
      "\n",
      "Epoch 00215: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_215.h5\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.9874 - val_loss: 5.2597 - val_sparse_categorical_accuracy: 0.2304\n",
      "\n",
      "Epoch 00216: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_216.h5\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 31s 845ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9849 - val_loss: 5.5347 - val_sparse_categorical_accuracy: 0.2343\n",
      "\n",
      "Epoch 00217: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_217.h5\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0240 - sparse_categorical_accuracy: 0.9958 - val_loss: 5.3367 - val_sparse_categorical_accuracy: 0.2363\n",
      "\n",
      "Epoch 00218: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_218.h5\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9913 - val_loss: 5.4614 - val_sparse_categorical_accuracy: 0.2343\n",
      "\n",
      "Epoch 00219: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_219.h5\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9938 - val_loss: 5.7804 - val_sparse_categorical_accuracy: 0.2186\n",
      "\n",
      "Epoch 00220: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_220.h5\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0310 - sparse_categorical_accuracy: 0.9912 - val_loss: 5.6800 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00221: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_221.h5\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0550 - sparse_categorical_accuracy: 0.9847 - val_loss: 5.9919 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00222: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_222.h5\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0455 - sparse_categorical_accuracy: 0.9908 - val_loss: 5.9287 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00223: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_223.h5\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0609 - sparse_categorical_accuracy: 0.9808 - val_loss: 6.1087 - val_sparse_categorical_accuracy: 0.1971\n",
      "\n",
      "Epoch 00224: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_224.h5\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0279 - sparse_categorical_accuracy: 0.9940 - val_loss: 5.6698 - val_sparse_categorical_accuracy: 0.2451\n",
      "\n",
      "Epoch 00225: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_225.h5\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 32s 865ms/step - loss: 0.0518 - sparse_categorical_accuracy: 0.9846 - val_loss: 5.6290 - val_sparse_categorical_accuracy: 0.2373\n",
      "\n",
      "Epoch 00226: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_226.h5\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0275 - sparse_categorical_accuracy: 0.9923 - val_loss: 6.0416 - val_sparse_categorical_accuracy: 0.2284\n",
      "\n",
      "Epoch 00227: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_227.h5\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.1049 - sparse_categorical_accuracy: 0.9716 - val_loss: 5.8044 - val_sparse_categorical_accuracy: 0.2216\n",
      "\n",
      "Epoch 00228: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_228.h5\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0311 - sparse_categorical_accuracy: 0.9917 - val_loss: 5.6268 - val_sparse_categorical_accuracy: 0.2088\n",
      "\n",
      "Epoch 00229: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_229.h5\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 31s 854ms/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9841 - val_loss: 5.5411 - val_sparse_categorical_accuracy: 0.2196\n",
      "\n",
      "Epoch 00230: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_230.h5\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0314 - sparse_categorical_accuracy: 0.9911 - val_loss: 5.6793 - val_sparse_categorical_accuracy: 0.2059\n",
      "\n",
      "Epoch 00231: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_231.h5\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0325 - sparse_categorical_accuracy: 0.9890 - val_loss: 5.8271 - val_sparse_categorical_accuracy: 0.2039\n",
      "\n",
      "Epoch 00232: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_232.h5\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9823 - val_loss: 5.5267 - val_sparse_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00233: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_233.h5\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0325 - sparse_categorical_accuracy: 0.9869 - val_loss: 5.6410 - val_sparse_categorical_accuracy: 0.2471\n",
      "\n",
      "Epoch 00234: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_234.h5\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9952 - val_loss: 5.9981 - val_sparse_categorical_accuracy: 0.2157\n",
      "\n",
      "Epoch 00235: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_235.h5\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0451 - sparse_categorical_accuracy: 0.9948 - val_loss: 5.7661 - val_sparse_categorical_accuracy: 0.2431\n",
      "\n",
      "Epoch 00236: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_236.h5\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9986 - val_loss: 5.7722 - val_sparse_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00237: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_237.h5\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9936 - val_loss: 5.7679 - val_sparse_categorical_accuracy: 0.2324\n",
      "\n",
      "Epoch 00238: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_238.h5\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 31s 851ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9879 - val_loss: 5.4647 - val_sparse_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00239: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_239.h5\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 32s 859ms/step - loss: 0.0139 - sparse_categorical_accuracy: 0.9976 - val_loss: 5.5307 - val_sparse_categorical_accuracy: 0.2471\n",
      "\n",
      "Epoch 00240: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_240.h5\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9952 - val_loss: 5.4843 - val_sparse_categorical_accuracy: 0.2284\n",
      "\n",
      "Epoch 00241: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_241.h5\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9892 - val_loss: 5.5361 - val_sparse_categorical_accuracy: 0.2314\n",
      "\n",
      "Epoch 00242: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_242.h5\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0351 - sparse_categorical_accuracy: 0.9926 - val_loss: 5.9706 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00243: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_243.h5\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0483 - sparse_categorical_accuracy: 0.9871 - val_loss: 6.3137 - val_sparse_categorical_accuracy: 0.1765\n",
      "\n",
      "Epoch 00244: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_244.h5\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 32s 859ms/step - loss: 0.0520 - sparse_categorical_accuracy: 0.9861 - val_loss: 5.8687 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00245: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_245.h5\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0321 - sparse_categorical_accuracy: 0.9903 - val_loss: 5.5299 - val_sparse_categorical_accuracy: 0.2304\n",
      "\n",
      "Epoch 00246: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_246.h5\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9943 - val_loss: 5.6093 - val_sparse_categorical_accuracy: 0.2461\n",
      "\n",
      "Epoch 00247: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_247.h5\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9947 - val_loss: 5.5891 - val_sparse_categorical_accuracy: 0.2363\n",
      "\n",
      "Epoch 00248: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_248.h5\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.9962 - val_loss: 5.8850 - val_sparse_categorical_accuracy: 0.2265\n",
      "\n",
      "Epoch 00249: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_249.h5\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 32s 863ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9983 - val_loss: 5.5069 - val_sparse_categorical_accuracy: 0.2461\n",
      "\n",
      "Epoch 00250: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_250.h5\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0294 - sparse_categorical_accuracy: 0.9941 - val_loss: 5.5079 - val_sparse_categorical_accuracy: 0.2431\n",
      "\n",
      "Epoch 00251: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_251.h5\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9891 - val_loss: 5.6344 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00252: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_252.h5\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9945 - val_loss: 5.7879 - val_sparse_categorical_accuracy: 0.1931\n",
      "\n",
      "Epoch 00253: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_253.h5\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0624 - sparse_categorical_accuracy: 0.9811 - val_loss: 6.5124 - val_sparse_categorical_accuracy: 0.1765\n",
      "\n",
      "Epoch 00254: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_254.h5\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9911 - val_loss: 6.1546 - val_sparse_categorical_accuracy: 0.1784\n",
      "\n",
      "Epoch 00255: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_255.h5\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.0475 - sparse_categorical_accuracy: 0.9839 - val_loss: 6.2432 - val_sparse_categorical_accuracy: 0.2010\n",
      "\n",
      "Epoch 00256: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_256.h5\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 31s 847ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9874 - val_loss: 5.8294 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00257: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_257.h5\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0326 - sparse_categorical_accuracy: 0.9909 - val_loss: 5.9660 - val_sparse_categorical_accuracy: 0.2373\n",
      "\n",
      "Epoch 00258: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_258.h5\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.0711 - sparse_categorical_accuracy: 0.9842 - val_loss: 6.0795 - val_sparse_categorical_accuracy: 0.2108\n",
      "\n",
      "Epoch 00259: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_259.h5\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9952 - val_loss: 5.8302 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00260: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_260.h5\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.9866 - val_loss: 6.0566 - val_sparse_categorical_accuracy: 0.2020\n",
      "\n",
      "Epoch 00261: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_261.h5\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.9861 - val_loss: 5.9979 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00262: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_262.h5\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0485 - sparse_categorical_accuracy: 0.9858 - val_loss: 5.6829 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00263: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_263.h5\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9837 - val_loss: 5.8276 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00264: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_264.h5\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9908 - val_loss: 5.8659 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00265: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_265.h5\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 31s 849ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9877 - val_loss: 6.0305 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00266: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_266.h5\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9884 - val_loss: 5.5952 - val_sparse_categorical_accuracy: 0.2294\n",
      "\n",
      "Epoch 00267: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_267.h5\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 31s 844ms/step - loss: 0.0802 - sparse_categorical_accuracy: 0.9761 - val_loss: 6.0941 - val_sparse_categorical_accuracy: 0.2069\n",
      "\n",
      "Epoch 00268: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_268.h5\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 31s 851ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9909 - val_loss: 6.0147 - val_sparse_categorical_accuracy: 0.2176\n",
      "\n",
      "Epoch 00269: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_269.h5\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 32s 849ms/step - loss: 0.0381 - sparse_categorical_accuracy: 0.9901 - val_loss: 5.7354 - val_sparse_categorical_accuracy: 0.2324\n",
      "\n",
      "Epoch 00270: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_270.h5\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9938 - val_loss: 5.8109 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00271: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_271.h5\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 31s 850ms/step - loss: 0.0296 - sparse_categorical_accuracy: 0.9926 - val_loss: 5.7332 - val_sparse_categorical_accuracy: 0.2167\n",
      "\n",
      "Epoch 00272: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_272.h5\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9877 - val_loss: 5.8777 - val_sparse_categorical_accuracy: 0.2118\n",
      "\n",
      "Epoch 00273: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_273.h5\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9823 - val_loss: 5.7642 - val_sparse_categorical_accuracy: 0.2127\n",
      "\n",
      "Epoch 00274: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_274.h5\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 31s 846ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9923 - val_loss: 5.5558 - val_sparse_categorical_accuracy: 0.2324\n",
      "\n",
      "Epoch 00275: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_275.h5\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0287 - sparse_categorical_accuracy: 0.9955 - val_loss: 5.5932 - val_sparse_categorical_accuracy: 0.2382\n",
      "\n",
      "Epoch 00276: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_276.h5\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 31s 840ms/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9928 - val_loss: 5.5281 - val_sparse_categorical_accuracy: 0.2392\n",
      "\n",
      "Epoch 00277: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_277.h5\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9998 - val_loss: 5.4492 - val_sparse_categorical_accuracy: 0.2471\n",
      "\n",
      "Epoch 00278: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_278.h5\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 32s 859ms/step - loss: 0.0173 - sparse_categorical_accuracy: 0.9962 - val_loss: 5.5245 - val_sparse_categorical_accuracy: 0.2549\n",
      "\n",
      "Epoch 00279: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_279.h5\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 31s 848ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9966 - val_loss: 5.4244 - val_sparse_categorical_accuracy: 0.2353\n",
      "\n",
      "Epoch 00280: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_280.h5\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 31s 843ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9942 - val_loss: 5.5311 - val_sparse_categorical_accuracy: 0.2373\n",
      "\n",
      "Epoch 00281: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_281.h5\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 32s 861ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9958 - val_loss: 5.8880 - val_sparse_categorical_accuracy: 0.2255\n",
      "\n",
      "Epoch 00282: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_282.h5\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9922 - val_loss: 6.3894 - val_sparse_categorical_accuracy: 0.2147\n",
      "\n",
      "Epoch 00283: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_283.h5\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9975 - val_loss: 6.2338 - val_sparse_categorical_accuracy: 0.2049\n",
      "\n",
      "Epoch 00284: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_284.h5\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 32s 854ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9963 - val_loss: 5.9334 - val_sparse_categorical_accuracy: 0.2137\n",
      "\n",
      "Epoch 00285: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_285.h5\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 32s 848ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9970 - val_loss: 5.8452 - val_sparse_categorical_accuracy: 0.2382\n",
      "\n",
      "Epoch 00286: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_286.h5\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 31s 840ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9978 - val_loss: 5.9371 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00287: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_287.h5\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 32s 857ms/step - loss: 0.0309 - sparse_categorical_accuracy: 0.9925 - val_loss: 5.9674 - val_sparse_categorical_accuracy: 0.2255\n",
      "\n",
      "Epoch 00288: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_288.h5\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 31s 851ms/step - loss: 0.0253 - sparse_categorical_accuracy: 0.9950 - val_loss: 6.0072 - val_sparse_categorical_accuracy: 0.2196\n",
      "\n",
      "Epoch 00289: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_289.h5\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 31s 843ms/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9911 - val_loss: 5.9270 - val_sparse_categorical_accuracy: 0.2235\n",
      "\n",
      "Epoch 00290: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_290.h5\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 31s 852ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9951 - val_loss: 5.8157 - val_sparse_categorical_accuracy: 0.2275\n",
      "\n",
      "Epoch 00291: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_291.h5\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9936 - val_loss: 5.9821 - val_sparse_categorical_accuracy: 0.2392\n",
      "\n",
      "Epoch 00292: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_292.h5\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9980 - val_loss: 5.8109 - val_sparse_categorical_accuracy: 0.2392\n",
      "\n",
      "Epoch 00293: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_293.h5\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 32s 858ms/step - loss: 0.0046 - sparse_categorical_accuracy: 0.9997 - val_loss: 5.6556 - val_sparse_categorical_accuracy: 0.2539\n",
      "\n",
      "Epoch 00294: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_294.h5\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 32s 852ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9975 - val_loss: 5.8022 - val_sparse_categorical_accuracy: 0.2422\n",
      "\n",
      "Epoch 00295: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_295.h5\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 32s 850ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9931 - val_loss: 5.7486 - val_sparse_categorical_accuracy: 0.2588\n",
      "\n",
      "Epoch 00296: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_296.h5\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 32s 851ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9942 - val_loss: 5.6406 - val_sparse_categorical_accuracy: 0.2539\n",
      "\n",
      "Epoch 00297: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_297.h5\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 32s 855ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9963 - val_loss: 5.6395 - val_sparse_categorical_accuracy: 0.2422\n",
      "\n",
      "Epoch 00298: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_298.h5\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 32s 856ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9988 - val_loss: 5.5922 - val_sparse_categorical_accuracy: 0.2480\n",
      "\n",
      "Epoch 00299: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_299.h5\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 32s 853ms/step - loss: 0.0073 - sparse_categorical_accuracy: 0.9995 - val_loss: 5.6185 - val_sparse_categorical_accuracy: 0.2431\n",
      "\n",
      "Epoch 00300: saving model to tmp/./flowers_models/EfficentNetB4_flowers_model.h5_e_300.h5\n",
      "Saved to: ./flowers_models/EfficentNetB4_flowers_model.h5.h5\n"
     ]
    }
   ],
   "source": [
    "base_model = EfficientNetB4(include_top=False, weights=None, input_shape=INPUT_SHAPE)\n",
    "\n",
    "model = wrap_model_for_beans(base_model=base_model, num_classes=NUM_CLASSES)\n",
    "\n",
    "train_model(model=model, ds_train=ds_train, ds_validation=ds_validation, model_name=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
